---
layout: post
title: "论文阅读笔记之——《Vision-language-action models: Concepts, progress, applications and challenges》"
date:   2025-09-15
tags: [Deep Learning]
comments: true
author: kwanwaipang
toc: true
---


<!-- * 目录
{:toc} -->

之前博客对VLN及基于VLN的相关survey进行了调研学习。
VLA，Vision-Language-Action其跟VLN有很多共通之处，甚至本质上讲，两者只是数据集、任务类型不一样。或者说VLN是VLA的更一步。但是从网络发展、底层逻辑都是非常相似的。更有甚者，有些基于VLN的工作则是直接把VLN任务分为：VLA+locomotion policy相结合。

本博文对VLA综述论文`《Vision-language-action models: Concepts, progress, applications and challenges》`进行阅读整理。本博文仅共本人学习记录用~

* [PDF](https://arxiv.org/pdf/2505.04769?)
* [Paper List](https://github.com/KwanWaiPang/Awesome-VLN)
* [Blog for VLN](https://kwanwaipang.github.io/VLN/)
* [Blog for CLIP](https://kwanwaipang.github.io/CLIP/)
* [论文阅读笔记之《Vision-and-language navigation today and tomorrow: A survey in the era of foundation models》](https://kwanwaipang.github.io/VLNsurvery2024/)

<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# 引言

论文系统综述视觉-语言-动作（VLA）模型，从概念基础、发展进展、应用场景到挑战展开分析。
梳理 VLA 模型从跨模态学习架构到融合视觉语言模型（VLMs）、动作规划器和分层控制器的通用智能体的演化历程，涵盖 80 多个近三年发布的 VLA 模型，详述其架构创新、参数高效训练、实时推理加速等关键进展，探讨在类人机器人、自动驾驶、医疗与工业机器人、精准农业、增强现实导航等领域的应用，分析实时控制、多模态动作表示、系统可扩展性等核心挑战，并提出智能体 AI 自适应、跨实体泛化等针对性解决方案，最后展望 VLA 与 VLMs、智能体 AI 融合的未来路线图。

在视觉-语言-动作（Vision-Language-Action，VLA）模型出现之前，机器人技术和人工智能主要分布在彼此割裂的几个子领域：视觉系统能够“看”并识别图像；语言系统能够理解和生成文本；动作系统则能够控制物体运动。
如下图1所示。
传统的CV可以通过CNN来识别物体或者进行分类，但是并不能理解语言、也没有将视觉转换为action的能力。
而语言模型，特别是基于LLM的虽然可以革新了文本的理解以及生成，但是他们仍然只能处理语言文本，而不能感知或者推理物理世界。
与此同时，机器人中基于action的系统一般都是依赖于传统控制策略（hand-crafted policies）或者强化学习来实现例如目标抓取等等，但是这需要艰苦的工程设计。

<div align="center">
  <img src="../images/微信截图_20250915104813.png" width="60%" />
<figcaption>  
</figcaption>
</div>

近年来，视觉语言模型（VLM， 如CLIP）等已经实现了通过融合视觉与语言的多模态感知，但是仍然无法基于输入的多模态信息生成或执行连贯的动作（coherent action）。
而VLA则是把视觉、语言、动作三者结合到一起。这也是具身智能的一个关键问题:如果系统无法同时感知、理解并采取行动，真正的智能自主行为将无从谈起.(`without systems that could jointly perceive, understand, and act, intelligent autonomous behavior remained an elusive goal`)

VLA 模型构想于 2021-2022 年，并在 Google DeepMind 的 Robotic Transformer 2（RT-2）等研究工作中得到率先实践，提出了一种将感知、推理与控制统一于单一架构的变革性方法。
VLA 模型整合了视觉输入、语言理解与运动控制能力，使具身智能体能够感知环境、理解复杂指令并动态地执行相应动作。

早期的 VLA 方法通过在视觉-语言模型中引入action tokens——即用于表示机器人动作指令的数值或符号形式（numerical or symbolic representations），实现了这种三模态融合。
模型可借助配对的图像、语言与轨迹数据进行训练，大幅提升了机器人对未见物体的泛化能力、对新颖语言指令的解释能力，以及在非结构化环境中的多步推理能力。
进一步地，VLA还可以借助大规模互联网数据集（g internet-scale datasets），这些数据集整合了图像、文本与行为信息，VLA 模型不仅能够识别与描述环境，还可以进行语境推理并在复杂、动态的场景中执行合适的动作。


# VLA 模型的核心概念






# VLA技术进展与训练效率





# VLA存在的挑战