---
layout: post
title: "论文阅读笔记之——《Vision-and-language navigation today and tomorrow: A survey in the era of foundation models》"
date:   2025-09-04
tags: [Deep Learning]
comments: true
author: kwanwaipang
toc: true
---


<!-- * 目录
{:toc} -->

之前博客对VLN做了个初步的调研，但是还是有点囫囵吞枣，本博文对2024年在Transactions on Machine Learning Research发表的VLN综述做个深入的阅读，希望能对其有更深入的理解~

* Paper list in: [Awesome-VLN](https://github.com/KwanWaiPang/Awesome-VLN)
and [Blog](https://kwanwaipang.github.io/VLN/)
* [PDF](https://openreview.net/pdf?id=yiqeh2ZYUh)
* [github](https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models)

基于本文的总结思路如下：

<div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/VLN进展2025-09-05-16-45.png" width="100%" />
<figcaption>  
</figcaption>
</div>

<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# 引言

本文致力于对利用基础模型（foundation models）应对VLN任务的survey，也就是大模型赋能下的VLN。

今年来，基础模型比如BERT，large language models (LLMs)，和vision language models (VLMs)在多模态理解（multimodal comprehension）、推理和跨领域泛化方面表现出了出色的能力。
这些模型都是通过大量的数据训练，比如文本、图像、声音、视频，并且可以进一步泛化到不同的具身智能AI task。而这些基础模型也进一步可以用到VLN领域。这也是本文的motivation，关注基础模型在VLN领域的应用，而并非像其他survey那样关注benchmark或者传统的方法。

此外，作者还引入了基于LAW（论文：Language models, agent models, and world models: The law for machine reasoning and planning）框架的系统框架，用于组织和理解VLN中的多模态推理和规划任务，强调了基础模型在构建世界模型和智能体模型中的作用。
作者，从world model、human model和VLN agent三个角度对VLN的挑战进行了分类，并提供了相应的解决方案。

如下图所示，VLN中的`world model`就是agent所维护的，可以用于理解周围环境及其action可以改变世界状态，的一个抽象。
`human model`就是人类partner给的指令

<div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/微信截图_20250904150407.png" width="80%" />
<figcaption>  
</figcaption>
</div>

因此，对于VLN任务，可以分为三个方面：
1. 学习world model来更好的表征视觉环境，并且可以泛化到没见过的环境
2. 学习human model进而根据的人类的指令，有效地理解意图
3. 学习VLN agent，利用world model和human model来理解语言、交流、推理和规划，使其能够按照指示在环境中导航

下图分析了基于基础模型时代的挑战、解决方案及未来发展方向。

<div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/微信截图_20250904152133.png" width="100%" />
<figcaption>  
</figcaption>
</div>

# 背景

VLN的定义：VLN被定义为一个需要智能体遵循人类指令、探索三维环境并在各种形式的歧义下进行情境交流的多模态和协作任务。这个任务要求智能体能够理解和执行自然语言指令，同时处理视觉信息。
按照语言指示，agent的需要在一系列离散视图（discrete views）或较低级别的action和控制上生成轨迹以到达目标位置（一般认为到目标3米内）。
此外，在导航过程中agent可能还需要跟人类做信息的交互（请求帮助或进行自由形式的语言交流）。并且还有一部分需要做操作或者目标检测等任务。

VLN包含了大量的benchmarks以及任务，如下表1所示：

<div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/微信截图_20250904153903.png" width="100%" />
<figcaption> 
注释：VLN-CE就是VLN in continuous environments 

</figcaption>
</div>

从LAW的角度来分析，VLN的基准目前包含以下几个方面:
1. world: 导航的场景，室内、室外、空中场景。仿真环境抑或真实环境。
2. human：人类指令的类型，包括交互轮数（单个或多个）、通信格式（自由对话、受限对话或多个指令）和语言粒度（动作导向或目标导向，甚至是需求导向）
3. VLN agent：包括了其类型（如，家用机器人、自动驾驶车辆或自动飞行器）、动作空间（基于图形、离散或连续）和附加任务（操作或物体检测）。
4. 数据收集：包括文本收集（text collection）方法（人工生成或模板化）和路线演示（人工执行或planner-generated）。


至于Foundation models则是有很多种，都是通过大型数据集训练的，在大量下游任务中都展示了较好的泛化能力。
1. 纯文本的基础模型，比如BERT， GPT-3等在文本生成、翻译、理解等都展示出色的性能
2. vision-language (VL) foundation models，比如LXMERT，CLIP，GPT-4则是展示了强大的多模态学习能力（integrating both visual and textual data）

# world model:学习及表征视觉环境

world model帮助VLN agent了解其周围环境，预测其行为将如何改变世界状态，并使其感知和行为与语言指令保持一致。它存在两点挑战：
1. 编码历史的视觉观测量，相当于就是让模型有记忆
2. 对于没见过的环境的泛化能力

## History and Memory

对于VLN任务而言，其历史的观测量不仅要包括过去的视觉观测，还应该包括过去执行的actions，来为当前要执行的action做决策。
在采用基础模型之前，LSTM采用隐含层来作为隐式的memory以此支持agent做导航决策。学者们也研究了不同的attention机制或者auxiliary tasks（辅助任务）来提升历史与指令之间的对齐。

### History Encoding
通过基础模型将历史信息编码的做法有以下几类：
<!-- * 基于编码的指令以及导航历史的multi-modal Transformer用于decision-making，模型则是通过in-domain instruction-trajectory数据进行预训练的 -->
* 将导航历史编码到定期更新的状态 tokens中
* 将历史导航信息通过multi-modal Transformer编码成序列
* 而基于LLM的方法有倾向于将视觉环境转换为文字描述。同时导航历史也编码这些图片的描述。

### Graph-based History

也有一部分方法是通过图信息来增强导航历史建模的。
* 利用Transformer encoder来捕获环境的几何线索
* 除了拓扑图以外，也有用grid map，语义地图、local metrics map以及 local neighborhood map来编码导航过程中的历史信息。


## 不同环境下的泛化能力

VLN中的一个难点则是从有限的环境中学习信息并且泛化到新的以及没见过的环境

### 预训练的视觉表征

* 早期大部分获取视觉表征都是通过ImageNet预训练的ResNet
* 在大模型时代，则是用CLIP的visual encoder来代替ResNet。由于CLIP通过图像-文本对的对比损失进行预训练，自然更好地对齐图像和指令。
* 更进一步的有采用从video数据中学习视觉表征，这类工作认为从视频中学到的时间信息是非常重要的。

### 环境的数据增广
通过大量的数据训练也可以增强模型的泛化能力

* 在基础模型之前，一般都是通过自动收集新的环境来fine-tune 基于LSTM的agent
* 通过自动生成的合成数据来做数据增广
* 不同环境的数据拼接（mix up），改变环境的外观、风格或者提取高频feature，甚至还有根据当前的观测来合成将来的环境。
* 而直接调用预训练好的LLM模型可以大大增强模型的能力。当然in-domain的pre-trained会更有效



# human model
除了学习以及对世界进行建模，VLN agent还需要human model来根据具体情况理解人类提供的自然语言指令，以完成导航任务。
对于human model也有两个挑战点：解决不同视觉环境中接地指令（grounded instructions）的歧义和泛化问题。

## 指令的歧义

在单论指令任务中，agent需要跟从初始指令，而没有进一步的与人交互来做澄清，因此容易陷入歧义。
此外，指令可能包含在当前视角下不可见的landmarks或从多个视图中可见的无法区分的landmarks。
这些问题在基础模型之前基本没有得到解决。虽然也有通过聚合多条指令，从不同角度描述同一轨迹，但仍然依赖于人类标注。
而来自于基础模型的综合感知语境以及commonsense knowledge可以很好的解决指令的歧义

### 感知语境与常识

大尺度跨模态预训练模型（比如CLIP）可以较好的匹配视觉语义与文本，这使得VLN agent可以利用视觉对象的信息及其在当前感知中的状态来解决歧义（特别是单轮导航场景）。
此外，LLM的常识推理能力可用于澄清或纠正指令中模糊的landmarks，并将指令分解为可操作的items。

### 主动寻求信息

虽然模糊的指令可以根据视觉感知和情境来解决，但另一种更直接的方法是向沟通partner寻求帮助。而这类型的工作由分为三个关键挑战：
1. 决定什么时候请求帮助
2. 生成信息寻求的问题，比如下一个行动、目标、方向
3. 开发一个提供查询信息的oracle，也可以是真人，规则与模板，或者神经网络
LLM和VLM可以在这个框架中扮演两个角色，要么作为信息寻求模型（解决when以及what to ask），要么作为人类助手或信息提供模型的代理。


## 指令的理解与泛化

导航数据的规模和多样性有限，影响了智能体理解和遵循各种语言表达的能力，特别是在未见导航环境中。
虽然，语言风格本身在见过或没见过的场景都有较好的泛化能力，但是如何将指令与看不见的环境联系起来可能是一项艰巨的任务。

### 预训练的文本表征

在基础大模型处理之前，大部分的工作都是依赖于LSTM来做文本编码。
而基础模型出来后（比如PRESS，BERT），基于预训练的语言模型来fine-tune获取的文本表达可以有更好的泛化能力；
接下来就是多模态的Transformers（比如VLN-BERT，pre-training on large-scale text-image pairs form web）可以更好的获取视觉-语言表征

### 指令合成
通过指令合成可以提升agent的泛化能力。比如通过使用人类注释的指令轨迹对训练离线说话者（指令生成器），然后由它来生成基于全景图与轨迹的指令（训练离线指令生成器）。


# VLN agent

 VLN agent则是负责具身推理和规划。
 world与human模型是负责增强视觉和语言理解能力，而agent则是负责学习推理以及规划的


## 对齐和推理

VLN需要对空间以及时间维度的指令、环境都要理解，特别是agent需要考虑之前的action，识别当前要执行的子指令，将文本与视觉环境对齐来执行action。

### 显示的语义理解

之前的工作通过通过显式的语义建模来增强智能体的对齐(grounding)能力,包括：在视觉和语言模态中对运动和地标进行建模，利用指令中的句法信息以及空间关系。
然而，使用基础模型的研究较少探索在VLN智能体中进行显式对齐。


### 预训练VLN基础模型
除了显式的语义建模外，之前的研究还通过辅助推理任务来增强智能体的对齐能力（grounding ability）。
但对于基于大模型的VLN agent较少有这方面的研究，因为预训练的模型已经让agent有较好的空间及时间维度的语义先验。


## 规划
动态规划使VLN智能体能够适应环境变化并实时改进导航策略。
除了基于图的规划器（graph-based planners）利用全局图信息来增强局部动作空间外，基础模型特别是大语言模型（LLMs）的兴起也带来了基于LLM的规划器进入VLN领域。


### 基于图的规划器

最近的研究强调了通过全局图信息来增强导航智能体的规划能力：
* 通过从访问节点的图前沿（graph frontiers of visited nodes）获取全局action step来增强局部导航动作空间，以实现更好的全局规划。
* 通过区域选择和选择节点来进行高层次的规划，以增强导航决策。
* 通过为图前沿（graph-frontier）增加网格级动作来丰富全局和局部动作空间，以实现更准确的动作预测。
* 采用分层规划方法，通过从预测的局部可导航图中选择局部路点来替代低层次的规划。
* 通过在局部地图内对对齐指令来实现轨迹规划
* 构建全局拓扑图或网格地图，以促进基于地图的全局规划
* 使用视频预测模型或神经辐射表示模型预测多个未来路点，以根据候选路点的长期效果计划最佳动作。

### 基于LLM的规划器
利用LLMs的常识知识生成基于文本的规划。
* 创建详细的计划，由子目标组成，并根据检测到的对象动态调整这些计划
* 专注于将导航任务分解为详细的文本指令，从静态和动态角度生成分步计划。
* 使用思维链推理生成缺失的动作，与交互对象一起使用。
*  将导航指令分解为代码格式的序列化、目标相关函数，并使用代码编写的大语言模型来指导这些目标的执行。
*  构建一个3D场景图作为输入到LLMs，以生成可行且上下文适当的高层次计划。


## 基于基础模型的VLN智能体

随着基础模型的出现，VLN智能体的架构经历了显著的变化。
最初由Anderson et al. (2018) 概念化的VLN智能体是在Seq2Seq框架内构建的，使用LSTM和注意力机制来模拟视觉和语言模态之间的交互。
随着基础模型的出现，智能体的后端从LSTM过渡到Transformer，最近则转向这些大规模预训练系统。

### 采用VLMs作为agent
每一轮输入语言、视觉和historical tokens。它通过对这些跨模态token进行self-attention来捕捉文本-视觉对应关系，然后用于推断action的概率。

### 采用LLMs作为agent

由于LLMs具有强大的推理能力和世界的语义抽象能力，并且在未知的大规模环境中表现出强大的泛化能力，最近的研究开始直接使用LLMs作为智能体来完成导航任务。通常，视觉观察被转换为文本描述并与指令一起输入到LLM中，然后执行动作预测。



# paper list

最后附上对本文对应的github的[paper list](https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models)的重塑

## World Model
A world model helps the VLN agent to understand their surrounding environments, predict how their actions would change the world state, and align their perception and actions with language instructions.

| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:|:-----------:|:----------:|:----:|
| 2025 | ACL | [MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation](https://arxiv.org/pdf/2502.13451) | - | - |
| 2024 | AAAI | [VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation](https://arxiv.org/abs/2402.03561) | - | - |
| 2024 | CVPR | [Volumetric Environment Representation for Vision-Language Navigation](https://arxiv.org/pdf/2403.14158) | [![Github stars](https://img.shields.io/github/stars/DefaultRui/VLN-VER.svg)](https://github.com/DefaultRui/VLN-VER) | - |
| 2023 | IJCAI | [Vision Language Navigation with Knowledge-driven Environmental Dreamer](https://www.ijcai.org/proceedings/2023/0204.pdf) | - | - |
| 2023 | NeurIPS | [Frequency-enhanced Data Augmentation for Vision-and-Language Navigation](https://openreview.net/pdf?id=eKFrXWb0sT) | [![Github stars](https://img.shields.io/github/stars/hekj/FDA.svg)](https://github.com/hekj/FDA) | - |
| 2023 | NeurIPS | [Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation](https://proceedings.neurips.cc/paper_files/paper/2023/file/0d9e08f247ca7fbbfd5e50b7ff9cf357-Paper-Conference.pdf) | [![Github stars](https://img.shields.io/github/stars/hekj/FDA.svg)](https://github.com/hekj/FDA) | - |
| 2023 | NeurIPS | [Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation](https://arxiv.org/abs/2305.19195) | [![Github stars](https://img.shields.io/github/stars/jialuli-luka/PanoGen.svg)](https://github.com/jialuli-luka/PanoGen) | - |
| 2023 | AAAI | [Simple and Effective Synthesis of Indoor 3D Scenes](https://arxiv.org/pdf/2204.02960) | [![Github stars](https://img.shields.io/github/stars/google-research/se3ds.svg)](https://github.com/google-research/se3ds) | - |
| 2023 | ICCV | [Learning Navigational Visual Representations with Semantic Map Supervision](https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.pdf) | - | - |
| 2023 | ICCV | [Learning vision-and-language navigation from youtube videos](https://arxiv.org/abs/2307.11984) | [![Github stars](https://img.shields.io/github/stars/JeremyLinky/YouTube-VLN.svg)](https://github.com/JeremyLinky/YouTube-VLN) | - |
| 2023 | ICCV | [GridMM: Grid Memory Map for Vision-and-Language Navigation](https://arxiv.org/abs/2307.12907) | [![Github stars](https://img.shields.io/github/stars/MrZihan/GridMM.svg)](https://github.com/MrZihan/GridMM) | - |
| 2023 | ICCV | [BEVBert: Multimodal Map Pre-training for Language-guided Navigation](https://arxiv.org/abs/2212.04385) | [![Github stars](https://img.shields.io/github/stars/MarSaKi/VLN-BEVBert.svg)](https://github.com/MarSaKi/VLN-BEVBert) | - |
| 2023 | ICCV | [Scaling Data Generation in Vision-and-Language Navigation](https://arxiv.org/abs/2307.15644) | [![Github stars](https://img.shields.io/github/stars/wz0919/ScaleVLN.svg)](https://github.com/wz0919/ScaleVLN) | - |
| 2023 | CVPR | [A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning](https://arxiv.org/abs/2210.03112) | [![Github stars](https://img.shields.io/github/stars/clin1223/MTVM.svg)](https://github.com/clin1223/MTVM) | - |
| 2022 | CVPR | [EnvEdit: Environment Editing for Vision-and-Language Navigation](https://arxiv.org/abs/2203.15685) | [![Github stars](https://img.shields.io/github/stars/jialuli-luka/VLN-SIG.svg)](https://github.com/jialuli-luka/VLN-SIG) | - |
| 2022 | ECCV | [Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960375.pdf) | [![Github stars](https://img.shields.io/github/stars/jialuli-luka/VLN-SIG.svg)](https://github.com/jialuli-luka/VLN-SIG) | - |
| 2022 | ICLR | [How Much Can CLIP Benefit Vision-and-Language Tasks?](https://arxiv.org/abs/2107.06383) | [![Github stars](https://img.shields.io/github/stars/clip-vil/CLIP-ViL.svg)](https://github.com/clip-vil/CLIP-ViL) | - |
| 2022 | CVPR | [Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation](https://arxiv.org/abs/2202.11742) | [![Github stars](https://img.shields.io/github/stars/cshizhe/VLN-DUET.svg)](https://github.com/cshizhe/VLN-DUET) | - |
| 2021 | NeurIPS | [History Aware Multimodal Transformer for Vision-and-Language Navigation](https://arxiv.org/abs/2110.13309) | [![Github stars](https://img.shields.io/github/stars/cshizhe/vln_hamt.svg)](https://cshizhe.github.io/projects/vln_hamt.html) | - |
| 2021 | ICCV | [Pathdreamer: A World Model for Indoor Navigation](https://arxiv.org/abs/2105.08756) | - | - |
| 2021 | ICCV | [Episodic Transformer for Vision-and-Language Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Pashevich_Episodic_Transformer_for_Vision-and-Language_Navigation_ICCV_2021_paper.pdf) | - | - |
| 2021 | ICCV | [Airbert: In-domain Pretraining for Vision-and-Language Navigation](https://arxiv.org/abs/2108.09105) | [![Github stars](https://img.shields.io/github/stars/airbert-vln/airbert-vln.github.io.svg)](https://airbert-vln.github.io/) | - |
| 2021 | ICCV | [Vision-Language Navigation with Random Environmental Mixup](https://arxiv.org/abs/2106.07876) | [![Github stars](https://img.shields.io/github/stars/LCFractal/VLNREM.svg)](https://github.com/LCFractal/VLNREM) | - |

## Human Model
The human model comprehends human-provided natural language instructions per situation to complete navigation tasks.

| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:|:-----------:|:----------:|:----:|
| 2025 | CVPR | [Scene Map-based Prompt Tuning for Navigation Instruction Generation](https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_Scene_Map-based_Prompt_Tuning_for_Navigation_Instruction_Generation_CVPR_2025_paper.pdf) | - | - |
| 2025 | ACL | [NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM](https://arxiv.org/pdf/2502.11142) | [![Github stars](https://img.shields.io/github/stars/MrZihan/NavRAG.svg)](https://github.com/MrZihan/NavRAG) | - |
| 2025 | ICLR | [Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel](https://arxiv.org/abs/2412.08467) | [![Github stars](https://img.shields.io/github/stars/wz0919/VLN-SRDF.svg)](https://github.com/wz0919/VLN-SRDF) | - |
| 2024 | ECCV | [Navigation Instruction Generation with BEV Perception and Large Language Models](https://arxiv.org/pdf/2407.15087) | [![Github stars](https://img.shields.io/github/stars/FanScy/BEVInstructor.svg)](https://github.com/FanScy/BEVInstructor) | - |
| 2024 | ECCV | [Controllable Navigation Instruction Generation with Chain of Thought Prompting](https://arxiv.org/pdf/2407.07433) | [![Github stars](https://img.shields.io/github/stars/refkxh/C-Instructor.svg)](https://github.com/refkxh/C-Instructor) | - |
| 2024 | ACL | [Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation](https://aclanthology.org/2024.acl-long.734.pdf) | [![Github stars](https://img.shields.io/github/stars/gmuraleekrishna/SAS.svg)](https://github.com/gmuraleekrishna/SAS) | - |
| 2024 | TPAMI | [Correctable Landmark Discovery via Large Models for Vision-Language Navigation](https://arxiv.org/abs/2405.18721) | [![Github stars](https://img.shields.io/github/stars/expectorlin/CONSOLE.svg)](https://github.com/expectorlin/CONSOLE) | - |
| 2024 | EACL | [NavHint: Vision and Language Navigation Agent with a Hint Generator](https://arxiv.org/pdf/2402.02559) | [![Github stars](https://img.shields.io/github/stars/HLR/NavHint.svg)](https://github.com/HLR/NavHint) | - |
| 2023 | TPAMI | [Learning to Follow and Generate Instructions for Language-Capable Navigation](https://ieeexplore.ieee.org/document/10359152) | - | - |
| 2023 | CVPR | [A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning](https://arxiv.org/pdf/2210.03112) | [Dataset](https://github.com/google-research-datasets/RxR/tree/main/marky-mT5) | - |
| 2023 | CVPR | [Lana: A Language-Capable Navigator for Instruction Following and Generation](https://arxiv.org/abs/2303.08409) | [![Github stars](https://img.shields.io/github/stars/wxh1996/LANA-VLN.svg)](https://github.com/wxh1996/LANA-VLN) | - |
| 2023 | CVPR | [KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_KERM_Knowledge_Enhanced_Reasoning_for_Vision-and-Language_Navigation_CVPR_2023_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/xiangyangli-cn/KERM.svg)](https://github.com/xiangyangli-cn/KERM) | - |
| 2023 | MM | [PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation](https://arxiv.org/pdf/2305.11918) | - | - |
| 2023 | - | [CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation](https://arxiv.org/abs/2103.00852) | - | - |
| 2023 | ACL | [VLN-Trans: Translator for the Vision and Language Navigation Agent](https://arxiv.org/pdf/2302.09230) | [![Github stars](https://img.shields.io/github/stars/HLR/VLN-trans.svg)](https://github.com/HLR/VLN-trans) | - |
| 2022 | ACL | [Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration](https://arxiv.org/pdf/2203.04006) | [![Github stars](https://img.shields.io/github/stars/liangcici/Probes-VLN.svg)](https://github.com/liangcici/Probes-VLN) | - |
| 2022 | CVPR | [Less is More: Generating Grounded Navigation Instructions from Landmarks](https://arxiv.org/pdf/2004.14973) | [![Github stars](https://img.shields.io/github/stars/google-research-datasets/RxR.svg)](https://github.com/google-research-datasets/RxR/tree/main/marky-mT5) | - |
| 2021 | EACL | [On the Evaluation of Vision-and-Language Navigation Instructions](https://arxiv.org/abs/2101.10504) | - | - |
| - | - | [Do As I Can, Not As I Say:Grounding Language in Robotic Affordances](https://say-can.github.io/assets/palm_saycan.pdf) | [![Github stars](https://img.shields.io/github/stars/say-can/say-can.github.io.svg)](https://say-can.github.io/) | - |

## VLN Agent

| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:|:-----------:|:----------:|:----:|
| 2025 | ICCV | [SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts](https://arxiv.org/pdf/2412.05552) | [![Github stars](https://img.shields.io/github/stars/GengzeZhou/SAME.svg)](https://github.com/GengzeZhou/SAME) | - |
| 2023 | AAAI | [Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation](https://arxiv.org/pdf/2302.06072) | - | - |
| 2023 | ICCV | [Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation](https://arxiv.org/abs/2308.12587) | [![Github stars](https://img.shields.io/github/stars/CSir1996/VLN-GELA.svg)](https://github.com/CSir1996/VLN-GELA) | - |
| 2023 | CVPR | [Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/chengaopro/AZHP.svg)](https://github.com/chengaopro/AZHP) | - |
| 2023 | ICCV | [Bird's-Eye-View Scene Graph for Vision-Language Navigation](https://arxiv.org/abs/2308.04758) | - | - |
| 2023 | EMNLP Findings | [Masked Path Modeling for Vision-and-Language Navigation](https://arxiv.org/abs/2305.14268) | - | - |
| 2023 | CVPR | [Improving Vision-and-Language Navigation by Generating Future-View Image Semantics](https://arxiv.org/pdf/2304.04907) | [![Github stars](https://img.shields.io/github/stars/jialuli-luka/VLN-SIG.svg)](https://github.com/jialuli-luka/VLN-SIG) | - |
| 2023 | TPAMI | [HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation](https://ieeexplore.ieee.org/document/10006384) | - | - |
| 2022 | MM | [Target-Driven Structured Transformer Planner for Vision-Language Navigation](https://arxiv.org/pdf/2207.11201) | [![Github stars](https://img.shields.io/github/stars/YushengZhao/TD-STP.svg)](https://github.com/YushengZhao/TD-STP) | - |
| 2022 | CVPR | [HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation](https://ieeexplore.ieee.org/document/9880046) | [![Github stars](https://img.shields.io/github/stars/YanyuanQiao/HOP-VLN.svg)](https://github.com/YanyuanQiao/HOP-VLN) | - |
| 2022 | COLING | [LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation](https://aclanthology.org/2022.coling-1.505.pdf) | [![Github stars](https://img.shields.io/github/stars/HLR/LOViS.svg)](https://github.com/HLR/LOViS) | - |
| 2021 | CVPR | [Scene-Intuitive Agent for Remote Embodied Visual Grounding](https://arxiv.org/pdf/2103.12944) | - | - |
| 2021 | NeurIPS | [SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation](https://arxiv.org/abs/2110.14143) | - | - |
| 2021 | ICCV | [The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_The_Road_To_Know-Where_An_Object-and-Room_Informed_Sequential_BERT_for_ICCV_2021_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/YuankaiQi/ORIST.svg)](https://github.com/YuankaiQi/ORIST) | - |
| 2021 | CVPR | [VLN BERT: A Recurrent Vision-and-Language BERT for Navigation](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_VLN_BERT_A_Recurrent_Vision-and-Language_BERT_for_Navigation_CVPR_2021_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/YicongHong/Recurrent-VLN-BERT.svg)](https://github.com/YicongHong/Recurrent-VLN-BERT) | - |
| 2020 | CVPR | [Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training](https://arxiv.org/abs/2002.10638) | [![Github stars](https://img.shields.io/github/stars/weituo12321/PREVALENT.svg)](https://github.com/weituo12321/PREVALENT) | - |

### VLN-CE Agent

| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:|:-----------:|:----------:|:----:|
| 2025 | ICCV | [NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/pdf/2506.23468) | [![Github stars](https://img.shields.io/github/stars/Feliciaxyao/NavMorph.svg)](https://github.com/Feliciaxyao/NavMorph) | - |
| 2025 | AAAI | [Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation](https://arxiv.org/abs/2407.05890) | - | - |
| 2024 | CVPR | [Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation](https://arxiv.org/pdf/2404.01943) | [![Github stars](https://img.shields.io/github/stars/MrZihan/HNR-VLN.svg)](https://github.com/MrZihan/HNR-VLN) | - |
| 2024 | PAMI | [ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2304.03047v2) | [![Github stars](https://img.shields.io/github/stars/MarSaKi/ETPNav.svg)](https://github.com/MarSaKi/ETPNav) | - |
| 2024 | MM | [Narrowing the Gap between Vision and Action in Navigation](https://www.arxiv.org/abs/2408.10388) | - | - |
| 2023 | ICCV | [BEVBert: Multimodal Map Pre-training for Language-guided Navigation](https://arxiv.org/pdf/2212.04385) | [![Github stars](https://img.shields.io/github/stars/MarSaKi/VLN-BEVBert.svg)](https://github.com/MarSaKi/VLN-BEVBert) | - |
| 2022 | CVPR | [Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation](https://arxiv.org/abs/2203.02764) | [![Github stars](https://img.shields.io/github/stars/YicongHong/Discrete-Continuous-VLN.svg)](https://github.com/YicongHong/Discrete-Continuous-VLN) | - |
| 2020 | ECCV | [Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/abs/2004.02857) | [![Github stars](https://img.shields.io/github/stars/jacobkrantz/VLN-CE.svg)](https://github.com/jacobkrantz/VLN-CE) | - |

### LLM/VLM-based VLN Agent

#### Zero-shot

| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:|:-----------:|:----------:|:----:|
| 2024 | ECCV | [LLM as Copilot for Coarse-grained Vision-and-Language Navigation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00833.pdf) | - | - |
| 2024 | ICRA | [Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions](https://ieeexplore.ieee.org/abstract/document/10611565) | [![Github stars](https://img.shields.io/github/stars/LYX0501/DiscussNav.svg)](https://github.com/LYX0501/DiscussNav) | - |
| 2024 | ACL | [MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation](https://arxiv.org/abs/2401.07314) | [![Github stars](https://img.shields.io/github/stars/chen-judge/MapGPT.svg)](https://chen-judge.github.io/MapGPT/) | - |
| 2024 | - | [MC-GPT: Empowering Vision-and-LanguageNavigation with Memory Map and Reasoning Chains](https://arxiv.org/pdf/2405.10620) | - | - |
| 2024 | - | [InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment](https://arxiv.org/pdf/2406.04882) | [![Github stars](https://img.shields.io/github/stars/LYX0501/InstructNav.svg)](https://github.com/LYX0501/InstructNav) | - |
| 2024 | AAAI | [NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models](https://arxiv.org/abs/2305.16986) | [![Github stars](https://img.shields.io/github/stars/GengzeZhou/NavGPT.svg)](https://github.com/GengzeZhou/NavGPT) | - |
| 2023 | ICCV | [March in Chat: Interactive Prompting for Remote Embodied Referring Expression](https://openaccess.thecvf.com//content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/YanyuanQiao/MiC.svg)](https://github.com/YanyuanQiao/MiC) | - |
| 2023 | - | [Vision and Language Navigation in the Real World via Online Visual Language Mapping](https://arxiv.org/pdf/2310.10822) | - | - |
| 2023 | NeurIPS Workshop | [A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models](https://peihaochen.github.io/files/publications/A2Nav.pdf) | - | - |
| 2022 | - | [CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/pdf/2211.16649) | - | - |

#### Fine-tuning

| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:|:-----------:|:----------:|:----:|
| 2025 | Arxiv | [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/pdf/2506.01551) | [![Github stars](https://img.shields.io/github/stars/expectorlin/EvolveNav.svg)](https://github.com/expectorlin/EvolveNav) | - |
| 2024 | NACCL Findings | [LangNav: Language as a Perceptual Representation for Navigation](https://aclanthology.org/2024.findings-naacl.60.pdf) | [![Github stars](https://img.shields.io/github/stars/pbw-Berwin/LangNav.svg)](https://github.com/pbw-Berwin/LangNav) | - |
| 2024 | - | [NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning](https://arxiv.org/abs/2403.07376) | [![Github stars](https://img.shields.io/github/stars/expectorlin/NavCoT.svg)](https://github.com/expectorlin/NavCoT) | - |
| 2024 | CVPR | [Towards Learning a Generalist Model for Embodied Navigation](https://arxiv.org/abs/2312.02010) | [![Github stars](https://img.shields.io/github/stars/LaVi-Lab/NaviLLM.svg)](https://github.com/LaVi-Lab/NaviLLM) | - |
| 2024 | ECCV | [NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models](https://www.arxiv.org/abs/2407.12366) | [![Github stars](https://img.shields.io/github/stars/GengzeZhou/NavGPT-2.svg)](https://github.com/GengzeZhou/NavGPT-2) | - |
| 2024 | RSS | [NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation](https://arxiv.org/pdf/2402.15852) | [![Github stars](https://img.shields.io/github/stars/GengzeZhou/NavGPT-2.svg)](https://github.com/GengzeZhou/NavGPT-2) | - |

## Behavior Analysis of the VLN Agent

| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:|:-----------:|:----------:|:----:|
| 2025 | CVPR | [Do Visual Imaginations Improve Vision-and-Language Navigation Agents?](https://arxiv.org/pdf/2503.16394) | - | - |
| 2024 | EMNLP Findings | [Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation](https://arxiv.org/pdf/2409.17313) | [![Github stars](https://img.shields.io/github/stars/zehao-wang/navnuances.svg)](https://github.com/zehao-wang/navnuances) | - |
| 2023 | CVPR | [Behavioral Analysis of Vision-and-Language Navigation Agents](https://yoark.github.io/assets/pdf/vln-behave/vln-behave.pdf) | [![Github stars](https://img.shields.io/github/stars/Yoark/vln-behave.svg)](https://github.com/Yoark/vln-behave) | - |
| 2022 | NACCL | [Diagnosing Vision-and-Language Navigation: What Really Matters](https://aclanthology.org/2022.naacl-main.438.pdf) | [![Github stars](https://img.shields.io/github/stars/VegB/Diagnose_VLN.svg)](https://github.com/VegB/Diagnose_VLN) | - |

