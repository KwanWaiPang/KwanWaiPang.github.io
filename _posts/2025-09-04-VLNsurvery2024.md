---
layout: post
title: "论文阅读笔记之《Vision-and-language navigation today and tomorrow: A survey in the era of foundation models》"
date:   2025-09-04
tags: [Deep Learning]
comments: true
author: kwanwaipang
toc: true
---


<!-- * 目录
{:toc} -->

之前博客对VLN做了个初步的调研，但是还是有点囫囵吞枣，本博文对2024年在Transactions on Machine Learning Research发表的VLN综述做个深入的阅读，希望能对其有更深入的理解~

* Paper list in: [Awesome-VLN](https://github.com/KwanWaiPang/Awesome-VLN)
and [Blog](https://kwanwaipang.github.io/VLN/)

<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# 引言

本文致力于对利用基础模型（foundation models）应对VLN任务的survey，也就是大模型赋能下的VLN。

今年来，基础模型比如BERT，large language models (LLMs)，和vision language models (VLMs)在多模态理解（multimodal comprehension）、推理和跨领域泛化方面表现出了出色的能力。
这些模型都是通过大量的数据训练，比如文本、图像、声音、视频，并且可以进一步泛化到不同的具身智能AI task。而这些基础模型也进一步可以用到VLN领域。这也是本文的motivation，关注基础模型在VLN领域的应用，而并非像其他survey那样关注benchmark或者传统的方法。

此外，作者还引入了基于LAW（论文：Language models, agent models, and world models: The law for machine reasoning and planning）框架的系统框架，用于组织和理解VLN中的多模态推理和规划任务，强调了基础模型在构建世界模型和智能体模型中的作用。
作者，从world model、human model和VLN agent三个角度对VLN的挑战进行了分类，并提供了相应的解决方案。

如下图所示，VLN中的`world model`就是agent所维护的，可以用于理解周围环境及其action可以改变世界状态，的一个抽象。
`human model`就是人类partner给的指令

<div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/微信截图_20250904150407.png" width="80%" />
<figcaption>  
</figcaption>
</div>

因此，对于VLN任务，可以分为三个方面：
1. 学习world model来更好的表征视觉环境，并且可以泛化到没见过的环境
2. 学习human model进而根据的人类的指令，有效地理解意图
3. 学习VLN agent，利用world model和human model来理解语言、交流、推理和规划，使其能够按照指示在环境中导航

下图分析了基于基础模型时代的挑战、解决方案及未来发展方向。

<div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/微信截图_20250904152133.png" width="100%" />
<figcaption>  
</figcaption>
</div>

# 背景

VLN的定义：VLN被定义为一个需要智能体遵循人类指令、探索三维环境并在各种形式的歧义下进行情境交流的多模态和协作任务。这个任务要求智能体能够理解和执行自然语言指令，同时处理视觉信息。
按照语言指示，agent的需要在一系列离散视图（discrete views）或较低级别的action和控制上生成轨迹以到达目标位置（一般认为到目标3米内）。
此外，在导航过程中agent可能还需要跟人类做信息的交互（请求帮助或进行自由形式的语言交流）。并且还有一部分需要做操作或者目标检测等任务。

VLN包含了大量的benchmarks以及任务，如下表1所示：

<div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/微信截图_20250904153903.png" width="100%" />
<figcaption> 
注释：VLN-CE就是VLN in continuous environments 

</figcaption>
</div>

从LAW的角度来分析，VLN的基准目前包含以下几个方面:
1. world: 导航的场景，室内、室外、空中场景。仿真环境抑或真实环境。
2. human：人类指令的类型，包括交互轮数（单个或多个）、通信格式（自由对话、受限对话或多个指令）和语言粒度（动作导向或目标导向，甚至是需求导向）
3. VLN agent：包括了其类型（如，家用机器人、自动驾驶车辆或自动飞行器）、动作空间（基于图形、离散或连续）和附加任务（操作或物体检测）。
4. 数据收集：包括文本收集（text collection）方法（人工生成或模板化）和路线演示（人工执行或planner-generated）。


至于Foundation models则是有很多种，都是通过大型数据集训练的，在大量下游任务中都展示了较好的泛化能力。
1. 纯文本的基础模型，比如BERT， GPT-3等在文本生成、翻译、理解等都展示出色的性能
2. vision-language (VL) foundation models，比如LXMERT，CLIP，GPT-4则是展示了强大的多模态学习能力（integrating both visual and textual data）

# world model:学习及表征视觉环境





# human model


# VLN agent