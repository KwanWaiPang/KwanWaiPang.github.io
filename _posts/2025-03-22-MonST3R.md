---
layout: post
title: "论文学习及实验笔记之——《MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion》"
date:   2025-03-22
tags: [Deep Learning,SLAM]
comments: true
author: kwanwaipang
toc: true
---


<!-- * 目录
{:toc} -->


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# 引言
DUSt3R具有非常强大的性能，但是还是针对静态场景的。而在ICLR2025上新出的Motion DUSt3R (MonST3R)则是从动态场景中直接估计每个时间下的几何信息。进而实现对动态场景的建模~

为此，写下本博文记录阅读及测试过程，本博文仅供本人学习记录用~

相关的资料：
* [paper](https://arxiv.org/pdf/2410.03825)
* [Github](https://github.com/Junyi42/monst3r)
* 本博客采用的代码及注释（如有）均在[Github Link](https://github.com/KwanWaiPang/monst3r)
* Survey for Transformer-based SLAM：[Paper List](https://github.com/KwanWaiPang/Awesome-Transformer-based-SLAM) and [Blog](https://kwanwaipang.github.io/Transformer_SLAM/)
* 博客：[What is Transformer? Form NLP to CV](https://kwanwaipang.github.io/Transformer/)


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->

# 理论解读
本文关键的insight则是将每个timestep下的pointmap估算出来（这样可以直接用DUSt3R的框架），进而实现动态场景的建模。
但是由于动态场景的数据并不好获取，因此，作者采用的是 fine-tuning 的形式，那么原始DUSt3R的网络权重也可以复用。
此外，动态场景也并不多，但是作者还是找到了几个small-scale的数据集可以让网络finetune一下即可使用。

如下图所示，本文所指的从dynamic场景中估算的pointmap，则是根据物体的移动把动态物体的点云也在上面显示出来。而静态部分的场景则需要实现Multi-frame alignment~
<div align="center">
  <img src="../images/微信截图_20250322165035.png" width="80%" />
<figcaption>  
</figcaption>
</div>

下图则是展示了MonST3R与DUSt3R在动态场景下的对比效果。可以看到首先由于训练数据中缺乏动态场景，因此DUSt3R是没有办法align运动物体的pointmap的。同时由于运动物体的存在也会影响到静态背景的align。当然作者在论文也提到，这一缺陷是因为数据domain 的mismatch，所以在实际MonST3R中还是需要对DUSt3R的权重进行fine-tune

<div align="center">
  <img src="../images/微信截图_20250322165433.png" width="80%" />
<figcaption>  
</figcaption>
</div>

采用的模型的结构跟DUSt3R是一样的。主要不一样的点只是对于每个pointmap，MonST3R都是refer to时间的一个点。而采用的带有动态物体的数据集则是如下四个：

<div align="center">
  <img src="../images/微信截图_20250322171126.png" width="80%" />
  <img src="../images/微信截图_20250322170607.png" width="80%" />
<figcaption>  
</figcaption>
</div>
对于finetune过程中，首先只finetune perediction head以及decoder，而网络的encoder是固定不变的。

此外，额外加了模块来估算光流（off-the-shelf method），这一可以很好的把动态与静态物体分离出来。而对于多个视角下的全局优化都是采用DUSt3R的全局alignment的模块来做的，只是分别构建三个loss（针对光流、相机姿态的平衡、以及全局对齐）进而实现动态场景下全局pointmap的构建。

作者用TUM-dymanic数据集来进行测试。可以看到，通过采用运动物体的GT mask，可以将DUSt3R处理数据的运动物体部分给mask掉，但这会导致位姿估计的退化（这应该是由于黑色的区域跟原本DUSt3R的数据分布不一样导致的~）。
<div align="center">
  <img src="../images/微信截图_20250322170433.png" width="80%" />
<figcaption>  
</figcaption>
</div>


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->

# 实验测试


