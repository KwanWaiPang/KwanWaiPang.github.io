---
layout: post
title: "Paper Survey之——Awesome Visual-Language-Navigation (VLN)"
date:   2025-08-24
tags: [Deep Learning]
comments: true
author: kwanwaipang
toc: true
---


<!-- * 目录
{:toc} -->


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# 引言

本博文对VLN进行调研，并对一些经典的工作进行阅读。


本博文仅供本人学习记录用~

* Keep update the paper list in: [Awesome-VLN](https://github.com/KwanWaiPang/Awesome-VLN)


# 基本概述

VLN其实是个视觉与语言的互动。

任务起源于2018年，最开始起源是一个名为‘bring me a spoon’的任务。
作者认为认为命令一个5岁左右的孩子去拿一个勺子是一个很简单的任务，而通过语言指令去指导机器人去拿一个勺子是非常困难的。

对于一个VLN，有三个重要的因素：
1. 人类（oracle）发布语言指令；
2. agent或机器人，执行者；
3. environment（环境），相当于需要工作的空间.但是考虑到真实场景训练成本比较高昂，所以一般都是采用模拟器，比如R2R就是采用Matterport 3D数据集作为仿真的室内环境。这些模拟器有的是通过相机拍的一些真实场景然后做渲染，有的则是通过合成的方式来生成虚拟的3d环境.

<div align="center">
  <img src="../images/WX20250824-184006.png" width="80%" />
<figcaption>  
</figcaption>
</div>

下面图片展示了VLN的Research Timeline，展示的从2018年～2023年的研究概况。
早期可能更多是网络结构如何更好的表征数据，其次就是扩展数据集，然后近期就是大模型的使用。

<div align="center">
  <img src="../images/2025-vln-research-timeline.png" width="100%" />
<figcaption>  
 Refer from “Thinking-VLN”
</figcaption>
</div>

# 主流的数据集

## Room-to-Room (R2R)
会给出相对详细的指令，并且轨迹是离散的，可移动的点。


## Room-Across-Room (RxR)
在R2R的基础上诞生了RxR，有两个关键点：
1. 指令的标记更细，有跟细粒度的指令信息
2. 多语言，在英语的基础上增加了两张语言（印度语）


## REVERIE
相比起之前的指令式则是更加high-level的，只会告诉agent想要什么物体，但不会告诉如何具体走到物体跟前。因此让任务更难。

此外，找到目的后，还需要定位物体，因此需要对每个物体都有bounding box。这样机器人不仅可以到达想要到的地方，还可以识别物体，下一步可能就是拿物体或者其他作业需求。


## CVDN
Vision-and-Dialog Navigation，这个数据集中，给的不再是单一的指令或者需求，给出的是人与机器人的对话。比如人告诉机器人去哪里，期间机器人可能有一些困惑就会问人类，也就是存在中间交流谈话的过程。
这样机器人可能就可以通过交流对话过程中，找到目的地或者找到物体。

<div align="center">
  <img src="../images/WX20250824-190113@2x.png" width="60%" />
<figcaption>  
</figcaption>
</div>


# 评估指标
首先是成功率（SR），就是希望机器人走到目的地（如范围内3米）来判断是否成功到达。
其次是导航误差（NE），就是到达目的地的误差。

但如果仅仅根据上述两个指标，机器人可能不会计较花的时间或者走的路径长路。因此还会有SPL（success weighted by path length）
其余的指标基本是基于这三者的一些改进。






agent 跟环境真实的交互而并非仅仅局限于仿真。




# 参考材料
* [《视觉语言导航》特邀讲师：吴琦副教授（澳大利亚阿德莱德大学）](https://www.bilibili.com/video/BV13g41157yL/?spm_id_from=333.1387.top_right_bar_window_history.content.click&vd_source=a88e426798937812a8ffc1a9be5a3cb7)
* [吴琦：AI研究一路走到“黑”， 从VQA到VLN](https://cloud.tencent.com/developer/article/1806234)
* [Thinking-VLN](https://github.com/YicongHong/Thinking-VLN)
* [“青源Talk”第43期 | 视觉语言导航](https://www.bilibili.com/video/BV1UV411g7UN/?spm_id_from=333.337.search-card.all.click&vd_source=a88e426798937812a8ffc1a9be5a3cb7)
