---
layout: post
title: "Paper Survey之——Awesome Visual-Language-Navigation (VLN)"
date:   2025-08-24
tags: [Deep Learning]
comments: true
author: kwanwaipang
toc: true
---


<!-- * 目录
{:toc} -->


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# 引言

本博文对VLN进行调研，并对一些经典的工作进行阅读。


本博文仅供本人学习记录用~

* Keep update the paper list in: [Awesome-VLN](https://github.com/KwanWaiPang/Awesome-VLN)


# 基本概述

VLN其实是个视觉与语言的互动。
下图直观的看到VLN在具身智能领域的位置。图来源于[A survey of embodied ai: From simulators to research tasks](https://arxiv.org/pdf/2103.04918)是关于embodied AI的survey，涵盖了各种仿真软件、数据集以及研究方向。可以看到VLN是在给定物体导航以及具有先验信息的导航之间的位置。

<div align="center">
  <img src="../images/WX20250824-193706.png" width="100%" />
<figcaption>  
</figcaption>
</div>

任务起源于2018年，最开始起源是一个名为‘bring me a spoon’的任务。
作者认为命令一个5岁左右的孩子去拿一个勺子是一个很简单的任务，但是如果想通过语言指令去指导机器人去拿一个勺子是非常困难的。
那么作者很快就锁定了用Matterport 3D来作为室内的环境，并选取了90个不同的室内建筑，包括住宅，办公室等。
而关于标注的语言指令（instruction），则是一个room-to-room的任务，就是只导航到目的地房间，并且给出比较详细的instruction，能更好的反应出vision和language的结合。
于是作者就开始搭建simulator，用AMT标定数据，有了第一个VLN的simulator叫Matterport 3D simulator以及第一个VLN的任务和数据，room2room （R2R）。

这个任务的定义是：给定一个natural language instruction，放置在simulator中初始位置的一个agent，需要通过理解instruction，并观察视觉环境，按照instruction给定的路线，移动到目的地。
而从此以后，VLN作为一个重要的任务正式诞生了。


对于一个VLN，有三个重要的因素：
1. 人类（oracle）发布语言指令；
2. agent或机器人，执行者；
3. environment（环境），相当于需要工作的空间.但是考虑到真实场景训练成本比较高昂，所以一般都是采用模拟器，比如R2R就是采用Matterport 3D数据集作为仿真的室内环境。这些模拟器有的是通过相机拍的一些真实场景然后做渲染，有的则是通过合成的方式来生成虚拟的3d环境.

<div align="center">
  <img src="../images/WX20250824-184006.png" width="80%" />
<figcaption>  
</figcaption>
</div>

与经典的VQA任务相比，VLN其实就是增加了主动视觉（active vision）的观测，在每一步的action的过程中，视觉的输入也总是在变化的。要根据行为来决定下一刻的行动。

<div align="center">
  <img src="../images/WX20250824-201611.png" width="80%" />
<figcaption>  
</figcaption>
</div>

下面图片展示了VLN的Research Timeline，展示的从2018年～2023年的研究概况。
早期可能更多是网络结构如何更好的表征数据，其次就是扩展数据集，然后近期就是大模型的使用。

<div align="center">
  <img src="../images/2025-vln-research-timeline.png" width="100%" />
<figcaption>  
 Refer from “Thinking-VLN”
</figcaption>
</div>


# 主流的数据集

## Room-to-Room (R2R)
会给出相对详细的指令，并且轨迹是离散的，可移动的点。

这个工作首先最关键的其实是[Matterport3D](https://niessner.github.io/Matterport/) simulator（全景图）

<div align="center">
  <img src="../images/WX20250824-200027@2x.png" width="80%" />
<figcaption>  
</figcaption>
</div>

而对于每个建筑物，就构建一个导航图（navigation graph），最初提出的时候把导航任务设置为离散的节点。（早期研究集中在离散导航）
而通过导航图可以确定可运动的轨迹。

<div align="center">
  <img src="../images/WX20250824-200714.png" width="80%" />
<figcaption>  
</figcaption>
</div>

同时对于给定的起点与终点的路径，每个可导航的节点都有对应的全景图。而所谓的语言指令描述则是从一个点到另外一个点的过程。
那么机器人，基于指令以及当前节点的全景图就要决定下一个时刻的action应该是什么，该往哪里走，最终要到终点位置。

<div align="center">
  <img src="../images/WX20250824-200930@2x.png" width="80%" />
<figcaption>  
</figcaption>
</div>


<div align="center">
  <img src="../images/WX20250824-201234@2x.png" width="80%" />
<figcaption>  
</figcaption>
</div>


## Room-Across-Room (RxR)
在R2R的基础上诞生了RxR，有两个关键点：
1. 指令的标记更细,路径也更长了，有更细粒度的指令信息
2. 多语言，在英语的基础上增加了两张语言（印度语）


## REVERIE
相比起之前的指令式则是更加high-level的，只会告诉agent想要什么物体，但不会告诉如何具体走到物体跟前。因此让任务更难。
并且物体是位于远处的，也就是起点无法被观测到的，也就是需要机器人有一定的推理能力去找到目标物体。（注意，此时仍然是仿真虚拟环境，无法跟场景进行交互，因此找到即可而不用交互）

此外，找到目的后，还需要定位物体，因此需要对每个物体都有bounding box。这样机器人不仅可以到达想要到的地方，还可以识别物体，下一步可能就是拿物体或者其他作业需求。


## CVDN
Vision-and-Dialog Navigation，这个数据集中，给的不再是单一的指令或者需求，给出的是人与机器人的对话。比如人告诉机器人去哪里，期间机器人可能有一些困惑就会问人类，也就是存在中间交流谈话的过程。
这样机器人可能就可以通过交流对话过程中，找到目的地或者找到物体。

<div align="center">
  <img src="../images/WX20250824-190113@2x.png" width="60%" />
<figcaption>  
</figcaption>
</div>


# 评估指标
首先是成功率（SR），就是希望机器人走到目的地（如范围内3米）来判断是否成功到达。
其次是导航误差（NE），就是到达目的地的误差。

但如果仅仅根据上述两个指标，机器人可能不会计较花的时间或者走的路径长路。因此还会有SPL（success weighted by path length）
其余的指标基本是基于这三者的一些改进。






agent 跟环境真实的交互而并非仅仅局限于仿真。


# 经典论文阅读

## Navila: Legged robot vision-language-action model for navigation
* [PDF](https://arxiv.org/pdf/2412.04453)
* [website](https://navila-bot.github.io/)
* [github](https://github.com/yang-zj1026/legged-loco)

现有的视觉语言导航（Vision-and-Language Navigation, VLN）系统通常依赖于预计算地图或使用深度传感器和单目RGB相机构建几何地图，但这些方法在复杂和杂乱的环境中表现有限。
本文主要解决腿式机器人（如四足机器狗或类人机器人）的视觉语言导航问题。


研究难点：
* 如何将人类语言指令转换为低级别的腿部关节动作；
* 在不同机器人之间迁移VLN模型；
* 现有的VLN系统在处理连续环境和低级运动控制方面也存在挑战


NaVILA结合了视觉-语言-动作模型（VLA）与运动控制的两级系统，以提高腿式机器人的导航能力。使用视觉语言模型（VILA）处理单视图图像，生成自然语言形式的中间动作指令。
本质上应该算是VLA，只不过进一步到导航层面，故此为VLN


VILA由三个主要组件组成：视觉编码器、投影器和大型语言模型（LLM）。
* 视觉编码器将输入图像转换为视觉标记序列
* 这些标记通过多层感知机（MLP）投影器映射到语义空间。
* 这些投影后的标记与文本标记一起被发送到LLM进行自回归生成。
VILA采用三阶段训练过程，包括连接器的预训练、连接器和LLM的联合预训练以及使用指令调整数据的微调。

对于视觉-语言导航任务中，不同时间步的图像具有不同的作用。当前时间步的图像用于立即决策，而之前的帧作为记忆库帮助智能体追踪整体进度。

为了更好地处理这两种表示，论文使用导航任务提示方法，通过区分当前观察和历史帧，并使用文本线索来构建导航任务提示。


# 参考材料
* [《视觉语言导航》特邀讲师：吴琦副教授（澳大利亚阿德莱德大学）](https://www.bilibili.com/video/BV13g41157yL/?spm_id_from=333.1387.top_right_bar_window_history.content.click&vd_source=a88e426798937812a8ffc1a9be5a3cb7)
* [吴琦：AI研究一路走到“黑”， 从VQA到VLN](https://cloud.tencent.com/developer/article/1806234)
* [Thinking-VLN](https://github.com/YicongHong/Thinking-VLN)
* [“青源Talk”第43期 | 视觉语言导航](https://www.bilibili.com/video/BV1UV411g7UN/?spm_id_from=333.337.search-card.all.click&vd_source=a88e426798937812a8ffc1a9be5a3cb7)
