---
layout: post
title: "Paper Survey之——Awesome Visual-Language-Navigation (VLN)"
date:   2025-08-24
tags: [Deep Learning]
comments: true
author: kwanwaipang
toc: true
---


<!-- * 目录
{:toc} -->


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# 引言

视觉语言导航（Vision-Language Navigation, VLN）是一个多学科交叉的研究领域，涵盖了自然语言处理、计算机视觉、多模态信息融合以及机器人导航等多个学科。
在该领域，研究人员致力于开发能够理解自然语言指令，并在复杂环境中实现自主导航的智能体。

本博文对VLN进行调研，并对一些经典的工作进行阅读。

本博文仅供本人学习记录用~

* Keep update the paper list in: [Awesome-VLN](https://github.com/KwanWaiPang/Awesome-VLN)


# 基本概述

VLN其实就是视觉与语言的互动。
下图直观的看到VLN在具身智能领域的位置。图来源于[A survey of embodied ai: From simulators to research tasks](https://arxiv.org/pdf/2103.04918)是关于embodied AI的survey，涵盖了各种仿真软件、数据集以及研究方向。可以看到VLN是在给定物体导航以及具有先验信息的导航之间的位置。

<div align="center">
  <img src="../images/WX20250824-193706.png" width="100%" />
<figcaption>  
</figcaption>
</div>

任务起源于2018年，最开始起源是一个名为‘bring me a spoon’的任务。
作者认为命令一个5岁左右的孩子去拿一个勺子是一个很简单的任务，但是如果想通过语言指令去指导机器人去拿一个勺子是非常困难的。
那么作者很快就锁定了用Matterport 3D来作为室内的环境，并选取了90个不同的室内建筑，包括住宅，办公室等。
而关于标注的语言指令（instruction），则是一个room-to-room的任务，就是只导航到目的地房间，并且给出比较详细的instruction，能更好的反应出vision和language的结合。
于是作者就开始搭建simulator，用AMT标定数据，有了第一个VLN的simulator叫Matterport 3D simulator以及第一个VLN的任务和数据，room2room （R2R）。

这个任务的定义是：给定一个natural language instruction，放置在simulator中初始位置的一个agent，需要通过理解instruction，并观察视觉环境，按照instruction给定的路线，移动到目的地。
而从此以后，VLN作为一个重要的任务正式诞生了。

视觉语言导航任务通常依赖于指令以及由环境模拟器（如Matterport3D、Habitat等）构建的交互式环境。
智能体的任务是按照自然语言指令要求，在环境中导航到目标位置。 指令被表示为一个单词序列。
而模拟器为智能体提供了数据交互接口，能够依据智能体的状态（例如坐标和朝向）以及其执行的操作，生成动态的感知信息。
因此，对于一个VLN，有三个重要的因素：
1. 人类（oracle）发布语言指令；
2. agent或机器人，执行者；
3. environment（环境），相当于需要工作的空间.但是考虑到真实场景训练成本比较高昂，所以一般都是采用模拟器，比如R2R就是采用Matterport 3D数据集作为仿真的室内环境。这些模拟器有的是通过相机拍的一些真实场景然后做渲染，有的则是通过合成的方式来生成虚拟的3d环境.

<div align="center">
  <img src="../images/WX20250824-184006.png" width="80%" />
<figcaption>  
</figcaption>
</div>

与经典的VQA任务相比，VLN其实就是增加了主动视觉（active vision）的观测，在每一步的action的过程中，视觉的输入也总是在变化的。要根据行为来决定下一刻的行动。

<div align="center">
  <img src="../images/WX20250824-201611.png" width="80%" />
<figcaption>  
VQA vs VLN
</figcaption>
</div>

下面图片展示了VLN的Research Timeline，展示的从2018年～2023年的研究概况。
早期可能更多是网络结构如何更好的表征数据，其次就是扩展数据集，然后近期就是大模型的使用。

<div align="center">
  <img src="../images/2025-vln-research-timeline.png" width="100%" />
<figcaption>  
 Refer from “Thinking-VLN”
</figcaption>
</div>

# 任务类型
从任务类型来看，视觉语言导航任务涵盖了指令导向（如R2R和R4R）、目标导向（如REVERIE和SOON）。需求导向（如DDN），所有这些任务都要求智能体能够利用语言指令和动态视觉观察来做出实时决策。
* 指令导向：指令导向的视觉语言导航任务侧重于智能体严格遵循给定的语言指令进行导航。这种任务要求智能体能够理解复杂的自然语言指令，并将其转化为导航动作。例如，一个指令可能是“往前走到海报附近然后右拐进办公室”，智能体需要理解并执行这些动作以到达指定位置。
* 目标导向：目标导向的视觉语言导航任务要求智能体根据给定的目标进行导航。在这种任务中，智能体需要理解目标的语义信息，并在环境中搜索与目标相匹配的物体。例如，智能体可能会收到指令“找到沙发”，然后需要在环境中识别沙发并导航到那里。
* 需求导向：需求导向的视觉语言导航任务是一种更高级的形式，它要求智能体根据用户的抽象需求进行导航。与前两种任务不同，需求导向导航不依赖于特定的物体或目标，而是需要智能体理解用户的需求并找到满足这些需求的物体或位置。例如，如果用户说“我饿了”，智能体需要找到食物或厨房等可以满足用户需求的地方。
  
依据用户与智能体之间的交互轮数，任务可被细分为单轮指令任务和多轮对话式导航任务。
* 单轮指令任务：在单轮指令任务中，智能体接收到一个自然语言指令，并且需要在没有进一步交互的情况下执行该指令。这种任务要求智能体能够理解指令的含义，并将其转化为导航动作。例如，智能体可能会接收到指令“走出浴室，左转，通过左侧的门离开房间”，然后智能体必须理解并执行这些动作以到达目的地。
* 对话式导航任务：对话式导航任务则涉及到更复杂的交互，智能体可以在导航过程中与用户进行多次对话。在这种任务中，智能体可能无法仅凭初始指令就完全理解用户的意图，需要通过提问来获取更多信息，或者在不确定时请求用户澄清。例如，如果智能体对指令中的某个地标有疑问，它可以询问用户以获得更明确的指导。

# 场景类型
根据应用场景不同，可以将视觉语言导航分为室内、室外、空中三种场景。
* 室内场景：室内视觉语言导航主要关注于家庭或办公环境内的导航。智能体需要理解自然语言指令，并在室内环境中找到正确的路径。室内环境通常较为复杂，包含多个房间和各种家具，因此对智能体的空间理解能力要求较高。例如，Room-to-Room数据集 是专为室内VLN设计的，它提供了大量的自然语言指令和相应的导航路径。
* 室外场景：室外视觉语言导航涉及到更开放的环境，如街道、公园等。在这种场景下，智能体不仅需要理解指令，还需要处理更复杂的空间关系和可能的遮挡物。室外环境的动态性，如行人和车辆的移动，也会增加导航的难度。
* 室外场景：室外视觉语言导航涉及到更开放的环境，如街道、公园等。在这种场景下，智能体不仅需要理解指令，还需要处理更复杂的空间关系和可能的遮挡物。室外环境的动态性，如行人和车辆的移动，也会增加导航的难度。




# 主流的数据集与模拟器

## 模拟器


## Room-to-Room (R2R)
会给出相对详细的指令，并且轨迹是离散的，可移动的点。

这个工作首先最关键的其实是[Matterport3D](https://niessner.github.io/Matterport/) simulator（全景图）

<div align="center">
  <img src="../images/WX20250824-200027@2x.png" width="80%" />
<figcaption>  
</figcaption>
</div>

而对于每个建筑物，就构建一个导航图（navigation graph），最初提出的时候把导航任务设置为离散的节点。（早期研究集中在离散导航）
而通过导航图可以确定可运动的轨迹。

<div align="center">
  <img src="../images/WX20250824-200714.png" width="80%" />
<figcaption>  
</figcaption>
</div>

同时对于给定的起点与终点的路径，每个可导航的节点都有对应的全景图。而所谓的语言指令描述则是从一个点到另外一个点的过程。
那么机器人，基于指令以及当前节点的全景图就要决定下一个时刻的action应该是什么，该往哪里走，最终要到终点位置。

<div align="center">
  <img src="../images/WX20250824-200930@2x.png" width="80%" />
<figcaption>  
</figcaption>
</div>


<div align="center">
  <img src="../images/WX20250824-201234@2x.png" width="80%" />
<figcaption>  
</figcaption>
</div>


## Room-Across-Room (RxR)
在R2R的基础上诞生了RxR，有两个关键点：
1. 指令的标记更细,路径也更长了，有更细粒度的指令信息
2. 多语言，在英语的基础上增加了两张语言（印度语）


## REVERIE
相比起之前的指令式则是更加high-level的，只会告诉agent想要什么物体，但不会告诉如何具体走到物体跟前。因此让任务更难。
并且物体是位于远处的，也就是起点无法被观测到的，也就是需要机器人有一定的推理能力去找到目标物体。（注意，此时仍然是仿真虚拟环境，无法跟场景进行交互，因此找到即可而不用交互）

此外，找到目的后，还需要定位物体，因此需要对每个物体都有bounding box。这样机器人不仅可以到达想要到的地方，还可以识别物体，下一步可能就是拿物体或者其他作业需求。


## CVDN
Vision-and-Dialog Navigation，这个数据集中，给的不再是单一的指令或者需求，给出的是人与机器人的对话。比如人告诉机器人去哪里，期间机器人可能有一些困惑就会问人类，也就是存在中间交流谈话的过程。
这样机器人可能就可以通过交流对话过程中，找到目的地或者找到物体。

<div align="center">
  <img src="../images/WX20250824-190113@2x.png" width="60%" />
<figcaption>  
</figcaption>
</div>


# 评估指标
首先是成功率（SR），就是希望机器人走到目的地（如范围内3米）来判断是否成功到达。
其次是导航误差（NE），就是到达目的地的误差。

但如果仅仅根据上述两个指标，机器人可能不会计较花的时间或者走的路径长路。因此还会有SPL（success weighted by path length）
其余的指标基本是基于这三者的一些改进。






agent 跟环境真实的交互而并非仅仅局限于仿真。


# 经典论文阅读

## Navila: Legged robot vision-language-action model for navigation
* [PDF](https://arxiv.org/pdf/2412.04453)
* [website](https://navila-bot.github.io/)
* [github](https://github.com/yang-zj1026/legged-loco)

现有的视觉语言导航（Vision-and-Language Navigation, VLN）系统通常依赖于预计算地图或使用深度传感器和单目RGB相机构建几何地图，但这些方法在复杂和杂乱的环境中表现有限。
本文主要解决腿式机器人（如四足机器狗或类人机器人）的视觉语言导航问题。


研究难点：
* 如何将人类语言指令转换为低级别的腿部关节动作；
* 在不同机器人之间迁移VLN模型；
* 现有的VLN系统在处理连续环境和低级运动控制方面也存在挑战


NaVILA结合了视觉-语言-动作模型（VLA）与运动控制的两级系统，以提高腿式机器人的导航能力。使用视觉语言模型（VILA）处理单视图图像，生成自然语言形式的中间动作指令。
本质上应该算是VLA，只不过进一步到导航层面，故此为VLN


VILA由三个主要组件组成：视觉编码器、投影器和大型语言模型（LLM）。
* 视觉编码器将输入图像转换为视觉标记序列
* 这些标记通过多层感知机（MLP）投影器映射到语义空间。
* 这些投影后的标记与文本标记一起被发送到LLM进行自回归生成。
VILA采用三阶段训练过程，包括连接器的预训练、连接器和LLM的联合预训练以及使用指令调整数据的微调。

对于视觉-语言导航任务中，不同时间步的图像具有不同的作用。当前时间步的图像用于立即决策，而之前的帧作为记忆库帮助智能体追踪整体进度。

为了更好地处理这两种表示，论文使用导航任务提示方法，通过区分当前观察和历史帧，并使用文本线索来构建导航任务提示。


# 参考材料
* [《视觉语言导航》特邀讲师：吴琦副教授（澳大利亚阿德莱德大学）](https://www.bilibili.com/video/BV13g41157yL/?spm_id_from=333.1387.top_right_bar_window_history.content.click&vd_source=a88e426798937812a8ffc1a9be5a3cb7)
* [吴琦：AI研究一路走到“黑”， 从VQA到VLN](https://cloud.tencent.com/developer/article/1806234)
* [Thinking-VLN](https://github.com/YicongHong/Thinking-VLN)
* [“青源Talk”第43期 | 视觉语言导航](https://www.bilibili.com/video/BV1UV411g7UN/?spm_id_from=333.337.search-card.all.click&vd_source=a88e426798937812a8ffc1a9be5a3cb7)
* [一些VLN论文阅读记录](https://blog.csdn.net/weixin_45347379?type=blog)
