<article>
  <!-- æ’å…¥åšå®¢å†…å®¹-->
    <h1>1. Abstract</h1>
    <p>
      In this report, we will provide a brief overview of our sensor synchronization process.
      This primarily involves synchronizing event cameras, synchronizing different types of sensors, and synchronizing onboard computers across multiple platforms.
    </p>
    <p>
    ðŸ˜€For the details of Event Cameras calibration, please refer to:
    <ul>
      <li> Achieving image reconstruction from event streams, and then using the Kalibr toolbox for camera-and-IMU calibration: <a href="https://arclab-hku.github.io/ecmd/calibration/" target="_blank">Link</a></li>
      <li>Using dv-gui for camera-and-IMU calibration: <a href="https://blog.csdn.net/gwplovekimi/article/details/121637241?spm=1001.2014.3001.5501" target="_blank">Link</a></li>
      <li>Assuming that the event camera is consistent with the image camera in pixel coordination, use the Kalibr toolbox to calibrate the image sensor: <a href="https://blog.csdn.net/gwplovekimi/article/details/120948986" target="_blank">Link</a></li>
    </ul> 
    </p>
    <h1>2. The Synchronization of Event Cameras</h1>
    <p>
      The event camera has an additional synchronization interface, called as "sync connectors", as shown in the figure below. (Using DAVIS346 as example reference to <a href="https://inivation.com/wp-content/uploads/2019/08/DAVIS346.pdf">Link</a>,
      same as DVXplorer <a href="https://inivation.com/wp-content/uploads/2023/03/DVXplorer.pdf">Link</a>).
    </p>
  
    <figure style="text-align: center;">
      <img style="width: 60%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/syns_connectors.png" alt="Image description">
      <figcaption>Fig. 1. Sync connector pinouts on DAVIS 346</figcaption>
    </figure>
  
    <p>
      The synchronization connectors are HiRose HR10A-7R-4P (male, SYNC OUTPUT) and HR10A-7R-4S (female, SYNC INPUT) connectors. Cables should use the matching connectors HR10A-7P-4S (female) and HR10A-7P-4P (male).
      Please note that to keep full electrical isolation between different cameras, the cable should not be shielded, or if it is, the shield should not connect one end of the cable to the other.
      Input signals can be 3.3V or 5V, depending on the VDD_IN supplied externally, output signals are 5V, as is VDD_OUT.
      If you chain cameras together for synchronization, the clock and VDD will be 5V, for example.
    </p>
    <!--        <br>-->
    <p>
      Therefore, utilizing this sync connector, we connected four event cameras together (two DAVIS346 and two DVXplorer) as shown in the diagram below.
      Additionally, we replaced the event camera ROS driver <a href="https://gitlab.com/inivation/dv/dv-ros">Code</a> provided by Invitation Company with our own driver <a href="https://github.com/arclab-hku/Event_based_VO-VIO-SLAM/tree/main/driver_code/dv-ros-master">Code</a>.
    </p>
  
    <figure style="text-align: center;">
      <img style="width: 60%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/four_event_camera.jpg" alt="Image description">
      <figcaption>Fig. 2. Sync connector pinouts on DAVIS 346</figcaption>
    </figure>
  
    <p>
      There are two key parameter in the driver code of event camera for synchronization, <mark>syncDevices</mark> and <mark> waitForSync </mark>
    </p>
  
    <div class="code-box">
      <code> &lt;rosparam param=&quot;syncDevices&quot;&gt;["series number of your event camera"]&lt;/rosparam&gt; </code>
    </div>
  
    <p>
      A list of other cameras connected with synchronization cable to this camera, If this list is empty, the camera node will not properly synchronize them.
    </p>
  
    <div class="code-box">
      <code>&lt;param name=&quot;waitForSync&quot; value=&quot;true&quot;/&gt;</code>
    </div>
  
    <p>
      This means that it does not publish data until synchronization is complete.
      The launch file of our event camera synchronization can be seen in <a href="https://github.com/arclab-hku/Event_based_VO-VIO-SLAM/tree/main/driver_code/dv-ros-master/dv_ros_visualization/launch">Link</a>.
      We use an event camera as "master" while the other three event cameras is waiting on list.
      Through the series number of the event camera to avoid mis-match.
    </p>
  
    <iframe width="600" height="340" src="//player.bilibili.com/player.html?aid=869034367&bvid=BV1xV4y1B7qF&cid=1141562890&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="display: block; margin: 0 auto;"></iframe>
  
    <p>
      Furthermore, to validate the synchronization effect, we simultaneously started four event cameras, recorded a rosbag, and then outputted the rostopic, as shown in the figure below.
      It can be observed that "/DAVIS346 left/events: 4482", "/DAVIS346 right/events: 4482", "/DVXplorer left/events: 4483", and "/DVXplorer right/events: 4480".
      The data volume from all four event cameras is similar.
      Additionally, from the video demo, it can be seen that when the event cameras are started, there is a waiting period until all of them have finished initializing.
      Moreover, the difference in data volume between "/DAVIS346 left/events" and "/DAVIS346 left/imu" is only 1, and both are recorded at 1000Hz.
    </p>
  
    <figure style="text-align: center;">
      <img style="width: 60%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/number_of_event_after_synchronization.png" alt="Image description">
      <figcaption>Fig. 3. The rosbag information for the four event cameras</figcaption>
    </figure>
  
  
    <h1>3. The Synchronization of Multi Sensors</h1>
  
    <figure style="text-align: center;">
      <img style="width: 60%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/structure_of_syns.png" alt="Image description">
      <figcaption>Fig. 4. The Structure of Our Sensors Synchronization</figcaption>
    </figure>
  
    <h3>3.1. The Synchronization of Two Onboard Computer</h3>
    <p>
      Firstly, we install the PTP.
    </p>
  
    <div class="code-box">
      <code>sudo apt install ptpd</code>
    </div>
  
    <p>
      Choose a machine to serve as the master node and initiate the following on it (where eth0 is the selected network interface for synchronization, please note that it requires the connected switch to support the PTP protocol):
    </p>
  
    <div class="code-box">
      <code>sudo ptpd -M -i eth0</code>
    </div>
  
    <p>
      On the remaining slave nodes, initiate the following:
    </p>
  
    <div class="code-box">
      <code>sudo ptpd -g -i eth0</code>
    </div>
  
    <p>
      If the "-C" parameter is added to both the master and slave, it will run in the foreground and print the output. For example, on the master side:
    </p>
  
    <div class="code-box">
      <code>sudo ptpd -g -i eth0 -C</code>
    </div>
  
    <br>
  
    <figure style="text-align: center;">
      <img style="width: 60%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/ptpd_syns.png" alt="Image description">
      <figcaption>The screenshot of the PTPD</figcaption>
    </figure>
  
    <h3>3.2 The Synchronization of Event Camera and Standard Camera </h3>
    <p>
      After synchronizing the clock cycles between two onboard computers, we set the image publishing frequency of DAVIS346 on onboard computer A to 20Hz, and the publishing frequency of the industrial camera on onboard computer B to 20Hz as well.
      Then, we placed a stopwatch in front of both cameras.
      We observed the stopwatch values for image topics with the same timestamps from both cameras.
      After multiple verifications, we concluded that the time difference between the DAVIS346 on onboard computer A and the industrial camera on onboard computer B, directly capturing images, was within 10ms, which aligns with our expectations.
    </p>
  
    <figure style="text-align: center;">
      <img style="width: 48%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/event_camera_synchronization.jpg" alt="Image description">
      <img style="width: 48%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/image_synchronization.jpg" alt="Image description">
      <figcaption>Fig. 5. Synchronization Testing between the Event Camera (left) and the Standard Camera (right) </figcaption>
    </figure>
  
    <p>
      We further test the synchronization between the event and image data streams in DAVIS346 as following:
    </p>
  
    <figure style="text-align: center;">
      <img style="width: 48%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/event_vs_image_1.png" alt="Image description">
      <img style="width: 48%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/event_vs_image_2.png" alt="Image description">
      <img style="width: 48%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/event_vs_image_3.png" alt="Image description">
      <img style="width: 48%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/event_vs_image_4.png" alt="Image description">
      <img style="width: 48%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/event_vs_image_5.png" alt="Image description">
      <img style="width: 48%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/event_vs_image_6.png" alt="Image description">
      <img style="width: 48%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/event_vs_image_7.png" alt="Image description">
      <img style="width: 48%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/event_vs_image_8.png" alt="Image description">
      <figcaption>Fig. 6. Synchronization Testing between the Event and Image from DAVIS346 </figcaption>
    </figure>
  
    <br>
    <iframe width="600" height="340" src="//player.bilibili.com/player.html?aid=229390210&bvid=BV168411o7BJ&cid=1152522995&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="display: block; margin: 0 auto;"></iframe>
  
    <h3>3.3. The Synchronization of Other Sensors</h3>
    <p>
      Our sensor platform can be seen as following:
    </p>
  
    <figure style="text-align: center;">
      <img style="width: 60%;" src="https://kwanwaipang.github.io/Poster_files/Sensor_Synchronization/four_event_camera_platform.jpg" alt="Image description">
      <figcaption>Fig. 7. Sensor Setup</figcaption>
    </figure>
  
  
    <h4>Tips:</h4>
    <ul>
      <li>
        <p>
          Event cameras are sensitive to infrared light, enabling them to detect the brightness changes caused by LiDAR with the right wavelength.
        </p>
      </li>
    </ul>
  
    <p>
      To alleviate disturbance from the LiDAR on the event camera, we add an infrared filter on the lens surface of the DAVIS346 camera.
      As can be seen from the video, we demonstrate that two event cameras with infrared filter while the other two event cameras without infrared filter.
    </p>
  
    <iframe width="600" height="340"  src="//player.bilibili.com/player.html?aid=399299383&bvid=BV15o4y1u7Up&cid=1147844860&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="display: block; margin: 0 auto;"></iframe>
  
  
  </article>