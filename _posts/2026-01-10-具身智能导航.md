---
layout: post
title: "谈谈具身智能导航"
date:   2026-01-10
tags: [VLN/VLA]
comments: true
author: kwanwaipang
toc: true
excerpt: "本文系统性地梳理了具身智能导航的架构演进，涵盖从传统分块式到端到端大模型、世界模型等前沿工作的深度思考。" # 【指定摘要内容】
---


<!-- * 目录
{:toc} -->

~~~
导航，是机器人实现自主移动与一切智能决策的先决条件。
~~~

# 引言

本人从2016年开始就围绕机器人导航开展相关的研究，从最初的感知-规划-控制三段式架构，到如今融合大模型的端到端导航系统，具身智能导航已从"能走"走向"会思考"。
随着具身智能与大模型技术的深度融合，视觉-语言-导航（VLN）成为具身智能的核心研究方向。
此前也写了一些博客来记录调研与学习的过程。总体来看还是有些散乱，为此写下本博客对整个具身智能导航做系统性介绍。
本博文仅供本人学习记录用～

* [Paper List for VLN](https://github.com/KwanWaiPang/Awesome-VLN)
* [Paper Survey之——Awesome Visual-Language-Navigation (VLN)](https://kwanwaipang.github.io/VLN/)
* [代码阅读笔记之——Odin Navigation Stack](https://kwanwaipang.github.io/Odin-Navigation-Stack/)
* [论文阅读及复现笔记之——《NaVILA: Legged Robot Vision-Language-Action Model for Navigation》](https://kwanwaipang.github.io/NaVILA/)
* [Paper Survey之——基于真实机器人的VLN](https://kwanwaipang.github.io/VLN-with-robotics/)
* [论文阅读笔记之——《Zero-shot Object-Centric Instruction Following: Integrating Foundation Models with Traditional Navigation》](https://kwanwaipang.github.io/nl-slam/)
* [论文阅读笔记之——《Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey》](https://kwanwaipang.github.io/Enbodied-Navigation/)
* [Paper Survey——CLIP on VLN](https://kwanwaipang.github.io/CLIP-on-VLN/)
* [论文阅读笔记之——《Vision-and-language navigation today and tomorrow: A survey in the era of foundation models》](https://kwanwaipang.github.io/VLNsurvery2024/)

# 具身智能导航的核心逻辑是什么？

首先，解读下什么是具身智能？
* 广义智能：是指一个“智能体”具备看、听、说、思考以及行动的综合能力。
* 具身（Embodied）：其本质是“赋予身体”，即智能不再是脱离物理世界的算法，而是与物理实体深度耦合。

因此，具身智能就是为智能体赋予物理形态。这种形态可以是多样化的：人形机器人、无人机、自动驾驶车辆、工业机械臂或轮式底盘。

<div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/WX20260110-122053@2x.png" width="90%" />
<figcaption> 
CVPR 2024 Embodied AI workshop上关于Embodied AI的定义 
</figcaption>
</div>

~~~
具身智能（Embodied Intelligence）是指智能体通过物理身体与环境交互，实现感知、推理与行动的综合能力。它不仅仅是"能看、能听、会说、爱思考"，更是"能动"——通过身体与环境的互动来理解世界。
~~~

基于此，具身智能导航的研究内核便十分清晰：即让一个具有物理本体（不限于机器人，无人机，无人车）的智能体，能够依靠自身‘视觉’理解能力，‘听从’人类语言指令，依靠空间‘推理’，在真实环境中完成‘导航’任务，并且在需要的情况下和人类或其他智能体进行‘交流’。看、说、听、想、动再加个本体，这就是具身智能导航，就是我们本博文讨论的问题。

~~~
对于VLN，智能体需要遵循自然语言指令，在陌生环境中导航到目标位置。这需要智能体具备视觉感知、语言理解、空间推理以及路径规划等能力。
~~~

目前具身智能研究主要聚焦于“上半身”的操作（Manipulation）与“下半身”的移动（Locomotion/Navigation）。尽管两者都遵循“感知-推理-执行”的工作流，但侧重点截然不同：
* 操作（Manipulation）更侧重“执行”，而相对轻“感知与推理”。 这是因为操作任务面对的环境相对局促，目标通常为单一或少数物体，对宏观环境的理解要求较低。其核心难点在于高精度的控制信号、复杂的接触力学以及细粒度的抓取策略。
* 导航（Navigation）则相反，它重“感知与推理”，而相对轻“执行”。 导航面对的是广阔且异构的物理空间，物体种类与位置排布千差万别。这要求智能体具备极强的空间语义理解与逻辑推理能力，从而在复杂环境中拆解任务并定位目标。至于“如何走到目标点”，在机器人领域已属于相对成熟的工程问题。因此，导航任务的核心挑战在于认知与规划。

~~~
navigation其实是一个重‘感知推理’，而轻‘执行’的课题。
~~~

<!-- # 任务分类 -->

# 技术分类

## 1. 传统分块式 （Modular-based）
核心逻辑：采用经典的感知与控制解耦架构，各模块功能明确。代表工作有：
* [Odin-Nav-Stack](https://github.com/ManifoldTechLtd/Odin-Nav-Stack)：SLAM+动态避障/Neupan(基于雷达点输入的端到端导航控制)+语义导航(基于指令分解与YoLo实现物体的检测、定位与相对物体的导航)+场景描述
* [RANGER](https://arxiv.org/pdf/2512.24212):对于RGB输入先通过MASt3R-SLAM估算pose和dense 3D map，然后调用语以模块（CLIP+Grounding DINO）生成语义点云，导航则是采用基于2D栅格（由3D投影而来）的路径规划，进而实现zero-shot目标导航。

## 2. 端到端大模型 (End-to-End Foundation Models)

核心逻辑：模型直接将传感器输入映射到底层控制指令（如离散动作或速度），取消显式的物理坐标接口。强调大规模数据驱动的泛化能力。代表性工作有：
<!-- * Uni-NaVid：支持视频流输入与 Token 合并，直接输出 Action Token。 -->
<!-- * RoboTron-Nav：虽然包含 3D 模块，但核心决策由 Action Head 直接完成。 -->
<!-- * Vi-LAD：通过知识蒸馏将 VLM 的注意力机制迁移至轻量级网络。 -->
<!-- * TrackVLA：专注于动态目标跟踪的视觉-语言-动作模型。 -->
<!-- * OctoNav：旨在通过单一通用模型解决多场景导航任务。 -->
<!-- * NaVid (2024)：基于视频 VLM 进行下一步规划，摆脱了对地图的依赖。 -->


## 3. 双系统架构
核心逻辑：遵循“大脑指路，小脑走路”的哲学。大脑负责高层语义规划，小脑负责底层运动控制，是目前足式机器人长程导航的主流。代表性工作有：
<!-- * NaVILA：面向足式机器人，上层模型输出中层指令，下层 RL 策略驱动电机。 -->
<!-- * Mobility VLA：适用于长距离场景，基于拓扑图预测下一个路点（Waypoint）。 -->
<!-- * LOG-Nav: 具备布局感知能力，输出 3D 路点。 -->
<!-- * OmniNav: 采用快慢系统设计，慢系统负责高精度路点规划。 -->
<!-- * Skill-Nav: 四足机器人专用，将相对路点传递给 RL 运动策略。 -->

## 4. 分层架构
核心逻辑：大脑在视觉空间内规划目标（如“画个圈”），视觉策略网络负责引导机器人追踪该目标。换句话说大脑负责规划，然后通过端到端导航网络实现导航，这部分跟双系统其实是有点像的。代表性工作有：
* [VLA-AN](https://arxiv.org/pdf/2512.15258): 1. 基于3D-GS的高保真数据构建(100K+ 轨迹和 1M+ 多模态样本的混合数据集);2.渐进式三阶段训练策略(海量VQA数据训练学会看图说话/空间推理---导航训练学习输出3D航点和偏航角---强化学习保证长序列任务的决策一致性和鲁棒性)；3. 非生成式动作模块（为了保证安全不直接输出电机控制信号，而是输出局部3D航点）+轻量级的action module（结合深度信息保证安全）+轻量化模型（Flash-Attention、算子融合、KV-Cache 预加载以及 CUDA Graph 调度）。7B模型（ViT+LLM），在Jetson Orin NX（ 100+TOPS）上可实现2～3Hz的控制频率（7B模型：0.11s/token， 2B模型：0.032s/token），单任务成功率高达98%且有实机飞行测试；（注意，此工作的模型是不直接输出电机控制信号的，而是交给action module，action model是负责避障碍然后生成控制指令）
<!-- * PixNav:：LLM 预测目标像素点，辅以纯 RGB 导航网络进行追踪。 -->
<!-- * Magma：引入 Set-of-Mark (SoM) 视觉提示与轨迹预测。 -->
<!-- * TopV-Nav：在俯视图上通过视觉提示（Visual Prompts）进行规划。 -->
<!-- * LM-Nav：通过检索目标图像节点，利用视觉伺服实现图像匹配导航。 -->

## 5. 地图与图解算流派 (Map-Centric & Solver-Based)

核心逻辑：强调显式的空间记忆与几何约束，依赖显式构建的地图与逻辑算力而非纯粹的神经直觉。代表性工作有：
* [FSR-VLN](https://arxiv.org/pdf/2509.13733)：层次化多模态场景图(HMSG)：多模态地图表征（FAST-LIVO2等：几何+语义+显式拓扑关系）实现粗略的room-level定位到精细的目标视角与物体定位；接下来的快速到慢速导航推理（FSR）基于HMGS，应用VLM实现最终目标的选择；基于选择的目标实现路径规划以及全身控制来到达目标（这部分采用传统方案）；Unitree-G1验证157m导航。
<!-- * GC-VLN：免训练架构。将语言指令转化为图约束，利用求解器计算最优路径。 -->
<!-- * LOVON：结合足式机器人与体素地图，构建 3D 语义空间。 -->
<!-- * ApexNav：双地图策略。自适应切换语义分数地图与前沿探索地图。 -->
<!-- * MapNav / BeliefMapNav：构建显式的语义地图或信念图以存储时空记忆。 -->
<!-- * VoxPoser / IMPACT：通过代码生成 3D 价值图或Cost Map。 -->

##  6. 生成式与世界模型流派 (Generative & World Models)
核心逻辑：利用 扩散模型(Diffusion) 或 世界模型(World Model) 预测未来，把导航变成“视觉生成”问题。代表性工作有：
* [NavForesee](https://arxiv.org/pdf/2512.01550)：高德地图，首次将视觉语言模型（VLM）规划与世界模型预测相结合，用于导航任务。
<!-- * NavigateDiff：利用扩散模型生成“下一帧景象”来引导机器人移动。 -->
<!-- * NoMaD：基于扩散策略（Diffusion Policy）实现的无地图探索。 -->
<!-- * WMNav：引入世界模型，通过模拟行动后果及“好奇心机制”进行探索。 -->
<!-- * VISTA / ForesightNav：强调“视觉想象力”，通过生成未见区域的预测图像辅助规划。 -->


# 参考资料
* [VLNVerse“宇宙降临”：吴琦团队交出2025具身导航最终答卷](https://mp.weixin.qq.com/s/vV3xeLbUVtZ77WBT5_CV3w)
* [具身智能导航2025最流行的架构分类](https://mp.weixin.qq.com/s/Mp6eRhHO3VsOutlq2A9aOw)
