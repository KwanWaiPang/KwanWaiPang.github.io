---
layout: post
title: "论文复现之——《AirIO: Learning Inertial Odometry with Enhanced IMU Feature Observability》"
date:   2025-02-13
tags: [SLAM, Deep Learning]
comments: true
author: kwanwaipang
toc: true
---


<!-- * 目录
{:toc} -->


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# 引言

之前博客[Paper Survey之——Deep IMU-Bias Inference](https://kwanwaipang.github.io/Deep-IMU-Bias/#airimu-learning-uncertainty-propagation-for-inertial-odometry)对基于learning的IMU odometry进行了调研及学习，其中复现了[AirIMU: Learning uncertainty propagation for inertial odometry](https://kwanwaipang.github.io/Deep-IMU-Bias/#airimu-learning-uncertainty-propagation-for-inertial-odometry)工作对应IMU的补偿，发现效果非常惊艳（虽然泛化能力还有待提升）
最近AirIMU作者新出了工作《AirIO: Learning Inertial Odometry with Enhanced IMU Feature Observability》并且开源了，为此写下本博客记录学习与复现过程~

~~~
@article{qiu2025airio,
  title={AirIO: Learning Inertial Odometry with Enhanced IMU Feature Observability},
  author={Qiu, Yuheng and Xu, Can and Chen, Yutian and Zhao, Shibo and Geng, Junyi and Scherer, Sebastian},
  journal={arXiv preprint arXiv:2501.15659},
  year={2025}
}
~~~

<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# 论文学习
该工作是围绕着基于无人机的IO（inertial odometry），作者发现将原始IMU数据转换为全局坐标会破坏无人机关键运动学信息的可观察性。因此直击在body frame下进行IMU数据的特征学习（preserving the IMU feature representation in the body frame）。
通过结合他们的前作[AirIMU](https://kwanwaipang.github.io/Deep-IMU-Bias/#airimu-learning-uncertainty-propagation-for-inertial-odometry)（IMU correction model）和EKF，提出AirIO来实现无人机在剧烈飞行状态下的位姿估计，无需额外的sensor或者控制输入（比如thrust commands）.
EKF是用于整合IMU预积分以及所学习的IMU correction model的

从下面效果来看还是比较impressive的，仅仅依靠IMU即可实现稳定的状态估计

<div align="center">
<video playsinline autoplay loop muted src="https://air-io.github.io/video/blackbird_for_web.mov" poster="https://kwanwaipang.github.io/File/Representative_works/loading-icon.gif" alt="sym" width="80%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>

[IMO](https://github.com/uzh-rpg/learned_inertial_model_odometry):[《Learned Inertial Odometry for Autonomous Drone Racing》RAL2023](https://arxiv.org/pdf/2210.15287)

[RONIN](https://github.com/Sachini/ronin):[《Ronin: Robust neural inertial navigation in the wild: Benchmark, evaluations, & new methods》ICRA2020](https://arxiv.org/pdf/1905.12853)

</div>

论文的贡献点如下：
1. 对于learning-based IO而言，bodyframe representation is more expressive and observable(一般的做法则是转换到global world frame或者从原始的IMU数据中去掉重力)
2. AirIO显式编码姿态（pose）信息，并且将其与body-frame IMU data融合来估算body frame的速度以及对应的uncertainty。（这部分就是所谓的AirIO motion network）
3. 整合uncertainty-aware IMU preintegration mode（应该就是前作AirIMU，可以估算IMU预积分及其uncertainty）与learned motion network（上面第二点）到EKF中来实现odometry estimation

而我个人认为更重要的poit是```Our model also demonstrates generalizability to the unseen datasets that are not included in the training sets.```具备较好的泛化能力是很重要的，当然这里所谓的model应该不是指uncertainty-aware IMU preintegration mode或者learned motion network，而是指整个AirIO

<div align="center">
  <table style="border: none; background-color: transparent;">
    <tr>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="../images/微信截图_20250213150331.png" width="100%" />
      </td>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="../images/微信截图_20250213150508.png" width="100%" />
      </td>
    </tr>
  </table>
  <figcaption>
  左图为motion network；右图为full system
  </figcaption>
</div>


## IMU Coordinate Frame
对于IMU实际测量量，加速度部分包括了物体自身运动带来的（usually aligned with object body frame）以及地球的重力，因此在timestamp i下所测量的加速度应该为：
<div align="center">
  <table style="border: none; background-color: transparent;">
    <tr>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="../images/微信截图_20250213150951.png" width="100%" />
      </td>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="../images/微信截图_20250213151202.png" width="100%" />
      </td>
    </tr>
  </table>
  
  Body Coordinate Frame vs. Global Coordinate Frame（$\widehat{R}$<sub>i</sub>一般则是通过IMU预积分或者EKF得到，精度也不够）
</div>
接下来作者通过Principal Component Analysis和t-SNE一顿分析，证明了上述两个表达中，在body frame下的表达更好（过程就跳过了😀）

## AirIO Motion Network & Training
在encoders，用CNN来提取body-frame IMU data (w<sup>B</sup>, a<sup>B</sup>)的特征以及飞机的orientation $\xi \in so(3)$.
然后将feature concatenate到一起，再用GRU layer来提取latent feature；
而在decoder，采用两个线性层来输出body-frame velocity and its uncertainty。整个网络的结构可以表达如下：
<div align="center">
  <img src="../images/微信截图_20250213153525.png" width="60%" />
<figcaption>  
在训练中飞机的orientation有GT pose提供，而测试的时候则有EKF估算的
</figcaption>
</div>

PS:将飞机的orientation转换到so(3) Lie algebra space（更有利于网络的smoother gradient）

## Extended Kalman Filter



<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# 代码复现
* [My AirIO_Comment](https://github.com/KwanWaiPang/AirIO_Comment)








# 参考资料
* [AirIO: Learning Inertial Odometry with Enhanced IMU Feature Observability](https://arxiv.org/pdf/2501.15659)
* [AirIO主页](https://air-io.github.io/)
* [AirIO代码](https://github.com/Air-IO/Air-IO)
* [AirIO作者github的代码](https://github.com/haleqiu/Air-IO)
* [Paper Survey之——Deep IMU-Bias Inference](https://kwanwaipang.github.io/Deep-IMU-Bias/#airimu-learning-uncertainty-propagation-for-inertial-odometry)
* [Paper Survey: Learning-based VO and VIO](https://kwanwaipang.github.io/File/Blogs/Poster/Learning_based_VO.html)