---
layout: post
title: "Paper Surveyä¹‹â€”â€”Awesome Event-based Contrast Maximization"
date:   2025-02-18
tags: [Event-based Vision]
comments: true
author: kwanwaipang
toc: true
---


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# å¼•è¨€

åœ¨Event Cameraé¢†åŸŸåŸºäºContrast Maximization(CMax,æˆ–CM)æ¡†æ¶è¡ç”Ÿäº†å¤§é‡çš„å·¥ä½œï¼ŒåŒ…æ‹¬å…‰æµä¼°è®¡ã€æ·±åº¦ä¼°è®¡ã€è¿åŠ¨ä¼°è®¡ï¼ˆ2Dã€3Dç”šè‡³6Dï¼‰ç­‰ç­‰ã€‚
å…¶å®æœ¬è´¨ä¸Šï¼ŒCMè¿™ä¸ªæ¡†æ¶æ˜¯é€šè¿‡å¯¹äº‹ä»¶è¿›è¡Œè¿åŠ¨è¡¥å¿ï¼Œè€Œåœ¨åšè¿åŠ¨è¡¥å¿çš„è¿‡ç¨‹ä¸­ä¼°ç®—å‡ºäº‹ä»¶çš„ä½ç§»ï¼Œè€Œè¿™ä¸ªä½ç§»ä¹Ÿå°±æ˜¯æ‰€è°“çš„event point trajectoriesï¼Œè€ŒåŸºäºåƒç´ çš„ä½ç§»å°±å¯ä»¥æ¨å¹¿åˆ°ç³»åˆ—è§†è§‰çš„ä»»åŠ¡ã€‚
```CMax aligns point trajectories on the image plane with event data by maximizing the contrast of an image of warped events (IWE)```

æœ¬åšæ–‡å¯¹Event-based Contrast Maximizationè¿›è¡Œè¾ƒä¸ºå…¨é¢çš„surveyï¼Œå¹¶ä¸”å°†å¯¹åº”çš„ç»å…¸è®ºæ–‡éƒ½åšç®€å•çš„ä»‹ç»ï¼ŒåŒæ—¶ä¹Ÿåˆ—å‡ºäº†CMaxç›¸å…³æˆ–è€…ç”¨åˆ°CMaxçš„æ–‡çŒ®ä»¥ä½œmark



<!-- * ç›®å½•
{:toc} -->

<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# åŸºæœ¬åŸç†
Event-based visionçš„å·¥ä½œåˆ†ç±»æœ‰å¾ˆå¤šç§ï¼Œè€Œå…¶ä¸­ï¼ŒæŒ‰ç…§å¤„ç†äº‹ä»¶çš„å½¢å¼å¯ä»¥åˆ†ä¸º`event-by-event`å’Œ`groups of events`ã€‚è€ŒCMaxåˆ™æ˜¯å±äºç¬¬äºŒç§ã€‚
CMax é¦–æ¬¡åœ¨æ–‡çŒ®[ã€ŠA Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth and Optical Flow Estimation (CVPR2018)ã€‹](https://www.ifi.uzh.ch/dam/jcr:a22071c9-b284-43c6-8f71-6433627b2db2/CVPR18_Gallego.pdf)ä¸­æå‡ºï¼š`The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events`
è€Œå…¶ä¸­çš„å¯»æ‰¾point trajectorieså¯ä»¥çœ‹æˆæ˜¯ä¸€ç§éšå¼çš„data associationï¼Œä¹Ÿæ˜¯Event-based visionç§çš„å…³é”®éƒ¨åˆ†,æ¯”å¦‚short characteristic time (optical flow)æˆ–longer estimation time (monocular depth estimation)ï¼›
æ­¤å¤–ï¼ŒCMå¯ä»¥ç”Ÿæˆmotion-corrected event imagesï¼Œæ—¢å¯ä»¥çœ‹æˆæ˜¯å¯¹`groups of events`çš„è¿åŠ¨è¡¥å¿ï¼Œä¹Ÿå¯ä»¥çœ‹æˆæ˜¯ç”±äº‹ä»¶äº§ç”Ÿçš„å›¾åƒçš„æ¢¯åº¦æˆ–edge imageã€‚

ä¸‹å›¾è¾ƒå¥½è§£æäº†CMçš„åŸºæœ¬åŸç†ï¼Œå…¶å®å°±æ˜¯å¯»æ‰¾ pointçš„è½¨è¿¹ï¼Œä½¿å…¶è¾ƒå¥½çš„ä¸eventç›¸alignï¼š

<div align="center">
  <table style="border: none; background-color: transparent;">
    <tr>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/Poster_files/Event_camera/event_in_voxel.gif" width="100%" />
      </td>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219113408.png" width="100%" />
      </td>
    </tr>
  </table>
  <figcaption>
  </figcaption>
</div>

äº‹ä»¶ç›¸æœºæ˜¯motion-activeçš„ï¼Œä¸€èˆ¬æ˜¯ç”±äº®åº¦å˜åŒ–æˆ–ç›¸å¯¹è¿åŠ¨ï¼ˆç›¸æœºä¸åœºæ™¯ï¼‰äº§ç”Ÿäº‹ä»¶çš„ï¼Œå¹¶ä¸”å¤§éƒ¨åˆ†çš„äº‹ä»¶éƒ½æ˜¯ç”±ç›¸å¯¹è¿åŠ¨çš„edgeæ¥äº§ç”Ÿçš„ï¼Œå› æ­¤äº‹ä»¶ç›¸æœºä¸­çš„æ•°æ®å…³è”åº”è¯¥æ˜¯`establishing which events were triggered by the same scene edge`ã€‚
è€Œç§»åŠ¨çš„edgeå®é™…ä¸Šæ˜¯æè¿°äº†åœ¨å¹³é¢ä¸Šçš„point trajectoryï¼Œå› æ­¤å°†æ²¿ç€point trajectoryç”Ÿæˆçš„eventå…³è”èµ·æ¥ä¹Ÿå°±æ˜¯æ•°æ®å…³è”çš„è¿‡ç¨‹ã€‚
å¦‚ä¸Šé¢å³å›¾æ‰€ç¤ºï¼Œpoint trajectoryè¿‘ä¼¼äºç›´çº¿ã€‚

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219120444.png" width="60%" />
<figcaption>
æ•°å­¦æè¿°æ‰€è°“çš„point trajectory  
</figcaption>
</div>

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219152239.png" width="80%" />
<figcaption>  
</figcaption>
</div>

CMç®—æ³•çš„æ¡†æ¶å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç”±ä»¥ä¸‹ä¸‰æ­¥ç»„æˆï¼š
1. Warp the events into an image H, according to the point trajectories defined by the geometric model and candidate parameters Î¸. æ‰€è°“çš„geometric modelå°±æ˜¯åŸºäºoptical flow, depth estimation, motion estimationç­‰é—®é¢˜ï¼Œæ¥æè¿°how points move on the image planeï¼ˆä¹Ÿå°±æ˜¯æ€ä¹ˆå¯¹event pointè¿›è¡ŒæŠ•å½±ï¼‰
2. Compute a score f based on the image of warped eventsï¼ˆä¹Ÿå°±æ˜¯score function,ä¹Ÿå°±æ˜¯ç´¯ç§¯IWEï¼Œç„¶åè®¡ç®—å¯¹æ¯”çš„score functionï¼‰
3. Optimize the score or objective function with respect to the parameters Î¸ of the model.è¿™æ­¥å…¶å®å°±æ˜¯ä¼˜åŒ–çš„è¿‡ç¨‹äº†ï¼Œæ¯”å¦‚é‡‡ç”¨æ¢¯åº¦ä¸‹é™æˆ–Newtonæ–¹æ³•æ¥ä¼°ç®—æœ€å¥½çš„Î¸å€¼ã€‚è€Œè¿™ä¸ªÎ¸å…¶å®å°±æ˜¯æ‰€æ±‚çš„point trajectoryäº†ï¼Œå› ä¸º`x=x+Î¸*t`

æ•°å­¦æè¿°å¦‚ä¸‹ï¼š

<div align="center">
<table style="border: none; background-color: transparent;">
  <tr>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219161647.png" width="100%" />
    </td>
    <td style="width: 40%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219161822.png" width="100%" />
    </td>
    <td style="width: 40%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219161937.png" width="100%" />
    </td>
  </tr>
</table>
<figcaption>
</figcaption>
</div>


å¯¹äºCMç®—æ³•ï¼Œå…¶æœ‰ä¸¤ä¸ªå‰¯äº§å“ï¼š
1. ä¼°ç®—å‡ºpoint trajectoryï¼Œéšå¼å»ºç«‹äº‹ä»¶ä¹‹é—´çš„æ•°æ®å…³è”
2. ä¼°ç®—å‡ºçš„point trajectoryå¯ä»¥ç”¨æ¥å¯¹è¿åŠ¨çš„edgeåšæ ¡æ­£ï¼ˆcorrectï¼‰

è€Œæ­¤å·¥ä½œä½œä¸ºCMç®—æ³•çš„åŸºç¡€å·¥ä½œä¹Ÿç»™å‡ºäº†CMä¸‰å¤§åŸºæœ¬åº”ç”¨ï¼š[Rotational motion estimation](#rotational-velocity-estimation)ï¼ˆè¿˜æœ‰[motion estimation in planar scenes](#motion-estimation-in-planar-scenes)ï¼‰, [Depth estimation](#depth-estimation), å’Œ [Optical Flow estimation](#optical-flow-estimation)

## è®¡ç®—å¤æ‚åº¦
è€ŒCMç®—æ³•çš„æ—¶é—´å¤æ‚åº¦åº”è¯¥æ˜¯`Oï¼ˆnï¼‰`,ä¹Ÿå°±æ˜¯è·Ÿäº‹ä»¶é‡æˆçº¿æ€§å…³ç³»ã€‚å…¶ä¸­warp eventåº”è¯¥æ˜¯è€—æ—¶æœ€å¤§çš„ï¼Œæƒ³æ±‚å¯¹æ¯”åº¦è¿™äº›æ“ä½œå‡ ä¹å¯ä»¥å¿½ç•¥ã€‚æ­¤å¤–ï¼Œä¼˜åŒ–æ–¹æ³•ä¹Ÿæ˜¯å½±å“çš„é‡è¦å› ç´ ã€‚

## loss function
[Focus is all you need: Loss functions for event-based vision (CVPR2019)](https://openaccess.thecvf.com/content_CVPR_2019/papers/Gallego_Focus_Is_All_You_Need_Loss_Functions_for_Event-Based_Vision_CVPR_2019_paper.pdf)ä¸­ï¼Œä»‹ç»äº†åŸºäºCMæ¡†æ¶çš„22ä¸ªç›®æ ‡å‡½æ•°ï¼Œä¸è¿‡è¿™ç¯‡è®ºæ–‡ä¸­æ˜¯é’ˆå¯¹Motion Compensationè¿™ä¸ªä¸»é¢˜çš„ã€‚
æ‰€è°“çš„loss functionå…¶å®å°±æ˜¯[Rotational motion estimation](#rotational-velocity-estimation)ï¼Œ[motion estimation in planar scenes](#motion-estimation-in-planar-scenes), [Depth estimation](#depth-estimation), å’Œ [Optical Flow estimation](#optical-flow-estimation)å‡ ä¸ªsectionä¸­æåˆ°çš„å…¬å¼3ï¼Œå°±æ˜¯ä¸Šé¢CMç®—æ³•æ¡†æ¶å›¾ä¸­æè¿°çš„`measure event alignment`.
æ±‡æ€»å¦‚ä¸‹ï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219161028.png" width="60%" />
<figcaption>  
</figcaption>
</div>
å¯¹åº”çš„å…¬å¼å¦‚ä¸‹ï¼ˆå®åœ¨å¤ªå¤šäº†ï¼Œåªèƒ½æ˜¯çœŸæ­£å¼€å‘çš„æ—¶å€™è¦ç”¨åˆ°å†ç»†è¯»äº†hh~ï¼‰ï¼š
<div align="center">
<table style="border: none; background-color: transparent;">
  <tr>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219162425.png" width="100%" />
    </td>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219162445.png" width="100%" />
    </td>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219162543.png" width="100%" />
    </td>
  </tr>
</table>

<img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219162659.png" width="60%" />

<table style="border: none; background-color: transparent;">
  <tr>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219162837.png" width="100%" />
    </td>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219162845.png" width="100%" />
    </td>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219162857.png" width="100%" />
    </td>
  </tr>
</table>

<table style="border: none; background-color: transparent;">
  <tr>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219162939.png" width="100%" />
    </td>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219162951.png" width="100%" />
    </td>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219163003.png" width="100%" />
    </td>
  </tr>
</table>

<table style="border: none; background-color: transparent;">
  <tr>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219163208.png" width="100%" />
    </td>
    <td style="width: 30%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
      <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219163225.png" width="100%" />
    </td>
  </tr>
</table>

<figcaption>
</figcaption>
</div>

æ­¤å¤–ï¼Œè®ºæ–‡[ã€ŠEvent Cameras, Contrast Maximization and Reward Functions: An Analysis (CVPR2019)ã€‹](https://openaccess.thecvf.com/content_CVPR_2019/papers/Stoffregen_Event_Cameras_Contrast_Maximization_and_Reward_Functions_An_Analysis_CVPR_2019_paper.pdf)ä¹Ÿæ˜¯å¯¹å„ç§loss functionï¼ˆæ­¤å¤„æ¢ç§°å‘¼ä¸ºreward functionï¼‰è¿›è¡Œäº†åˆ†æï¼Œä¸è¿‡å½“ç„¶20å¤šç§é‚£ä¹ˆå¤šäº†ã€‚


## Event Collapse
æ‰€è°“çš„Event Collapseç›´è¯‘å°±æ˜¯â€œäº‹ä»¶å´©æºƒâ€ï¼ˆä¹Ÿæœ‰ç§°ä¸ºover fittingï¼‰ï¼Œè¡¨ç°å‡ºæ¥çš„çº¿æ€§å°±æ˜¯äº‹ä»¶è¢«warpedåˆ°å°‘æ•°çš„pixelåŒºåŸŸ(`events accumulate into too few pixels`)ï¼Œä¹Ÿå°±æ˜¯é€€åŒ–/å¤±çœŸï¼Œåœºæ™¯ä¸­çš„äº‹ä»¶è¢«æŠ•åˆ°ä¸€å—ï¼ˆé™·å…¥æ‰€è°“çš„å±€éƒ¨æœ€å°å€¼äº†ï¼‰ï¼Œè€ŒProf. Gallegoå›¢é˜Ÿçš„ä¸¤ç¯‡è®ºæ–‡[Event Collapse in Contrast Maximization Frameworks (Sensor 2022)](https://web.archive.org/web/20220813065935id_/https://depositonce.tu-berlin.de/bitstream/11303/17328/1/sensors-22-05190-v3.pdf)å’Œ[A Fast Geometric Regularizer to Mitigate Event Collapse in the Contrast Maximization Framework (AIS2023)](https://advanced.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/aisy.202200251)çš„ç ”ç©¶ä¹Ÿè¯æ˜ï¼Œæ·»åŠ æ­£åˆ™åŒ–é¡¹æ˜¯å”¯ä¸€çš„æœ‰æ•ˆï¼Œæ¶ˆé™¤Event Collapseçš„æ–¹æ¡ˆ.

PS:è¯´æ˜¯ä¸¤ç¯‡ï¼Œæˆ‘ä¸ªäººè§‰å¾—æ˜¯ä¸€ç¯‡ï¼Œå› ä¸ºä¸¤ç¯‡è®ºæ–‡çš„ç»“æœå›¾ä¹Ÿå°±æ˜¯æ¢ä¸ªæ’åºè€Œå·²ğŸ˜‚
<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219171050.png" width="80%" />
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219171731.png" width="80%" />
<figcaption>  
</figcaption>
</div>

ä»ä¸Šå›¾å¯ä»¥çœ‹åˆ°ï¼Œç”šè‡³åœ¨å•è‡ªç”±åº¦ï¼ˆç›´çº¿è¿åŠ¨ï¼‰çš„æ—¶å€™ï¼Œä¹Ÿä¼šå‡ºç°event collapseã€‚
å®é™…ä¸Šï¼Œå¦‚æœæ˜¯æµ‹è¿‡CMax-SLAMå°±ä¼šå‘ç°ï¼Œé™¤éå¯¹äº‹ä»¶åˆ‡åˆ†å¾—å¾ˆå¥½ï¼Œä¸ç„¶ä¼šå‡ºç°å¤§é‡çš„event collapseçš„æƒ…å†µ~

é™¤äº†æ­£åˆ™åŒ–ä»¥å¤–ï¼Œè§£å†³event collapseçš„æ–¹å¼æœ‰ï¼š
1. æŠŠå‚æ•°åˆå§‹åŒ–ä¸ºéå¸¸æ¥è¿‘çœŸå€¼ï¼ˆ`initializing the parameters sufficiently close to the desired solution`ï¼‰emmmmè¿™ç‚¹æ€ä¹ˆè¯´å‘¢ï¼Œç®€å•è¯´åº”è¯¥å°±æ˜¯ï¼šç»™çœŸå€¼åŠ ä¸ªé«˜æ–¯noiseç„¶åè®©ç®—æ³•ä¼°ç®—è·å¾—estimated valueå†è·ŸçœŸå€¼å¯¹æ¯”ï¼ˆè¿˜çœŸæœ‰é¡¶ä¼šç”šè‡³TPAMIã€TROçº§åˆ«çš„è¿™æ ·åšçš„ğŸ˜‚ï¼‰
2. é™ä½é—®é¢˜çš„è‡ªç”±åº¦ï¼ˆä½†ä¸ªäººäººä¸ºæ²¡å•¥ç”¨ï¼Œæ¯•ç«Ÿå®é™…ä¸­CMæ˜¯åœ¨1Dä¸‹ä¹Ÿå¯èƒ½å‡ºç°event collapseï¼‰ï¼Œä¹Ÿæœ‰é€šè¿‡global optimalæˆ–è€…é€šè¿‡è®¾ç½® reward functionsçš„upper and lower boundsæ¥å®ç°æ›´å¥½çš„æ•ˆæœçš„
3. é‡‡ç”¨å…¶ä»–ä¼ æ„Ÿå™¨æä¾›æ›´å¤šçº¦æŸï¼ˆæ¯”å¦‚æ·±åº¦å›¾ï¼‰

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219174809.png" width="80%" />
<figcaption>  
</figcaption>
</div>

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ
`If the warp does not enable event collapse (contraction or accumulation of flow vectors cannot happen due to the geometric properties of the warp), as in the case of feature flow (2 DOF) (Figure 3b) or rotational motion flow (3 DOF) (Figure 3c), then the optimization problem is well posed and multiple objective functions can be designed to achieve event alignment`
event collapseæ˜¯ç”±motion hypothesisï¼ˆä¹Ÿå°±æ˜¯wrapçš„æ¨¡å‹ï¼‰æ¥å†³å®šï¼ŒæŸäº›è¿è¡Œï¼ˆæ¯”å¦‚å…‰æµæˆ–è€…rotational motionï¼‰æ˜¯ä¸ä¼šäº§ç”Ÿevent collapseçš„

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220105110.png" width="80%" />
<figcaption>  
</figcaption>
</div>


è€Œæ‰€è®¾è®¡çš„æ­£åˆ™åŒ–å™¨ä¹Ÿåº”è¯¥ç”±motion hypothesisï¼ˆä¹Ÿå°±æ˜¯wrapçš„æ¨¡å‹ï¼‰æ¥ï¼Œå› æ­¤å¯¹äºä¸‹é¢æ„å»ºçš„CMä¼˜åŒ–é—®é¢˜ï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220110211.png" width="25%" />
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220110216.png" width="40%" />
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220110222.png" width="25%" />
<figcaption>  
</figcaption>
</div>

æ‰€è°“çš„æ­£åˆ™åŒ–åˆ™æ˜¯æŠŠä¼˜åŒ–å‡½æ•°æ”¹ä¸ºä»¥ä¸‹çš„å½¢å¼ï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220110405.png" width="50%" />
<figcaption>  
</figcaption>
</div>

è€Œé’ˆå¯¹ä¸åŒçš„è¿è¡Œæ¨¡å‹æ­£åˆ™åŒ–å‡½æ•°Ræ˜¯ä¸ä¸€æ ·çš„ã€‚å…·ä½“çš„æ¨å¯¼åˆ†æè¯·è§åŸæ–‡äº†ã€‚
ä¸è¿‡åœ¨è¯¥ä½œè€…çš„æœ€æ–°çš„å·¥ä½œä¸­[Secrets of Event-based Optical Flow, Depth and Ego-motion Estimation by Contrast Maximization (TPAMI2024)](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10517639)ï¼Œå‰é¢ä¸¤ç¯‡å·¥ä½œçš„æ­£åˆ™åŒ–å™¨å…¨éƒ¨ä¸ç”¨ğŸ˜‚ï¼Œç›´æ¥æ”¹ä¸ºé‡‡ç”¨total variation (TV)

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220113242.png" width="50%" />
<figcaption>  
</figcaption>
</div>


ç±»ä¼¼åœ°ï¼Œæ–‡çŒ®[Density Invariant Contrast Maximization for Neuromorphic Earth Observations (CVPR2023)](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Arja_Density_Invariant_Contrast_Maximization_for_Neuromorphic_Earth_Observations_CVPRW_2023_paper.pdf)ä¹Ÿé’ˆå¯¹1Då’Œ2Dçš„è¿åŠ¨æ¥å¯¹CMæ¡†æ¶äº§ç”Ÿçš„noiseè¿›è¡Œäº†å¤„ç†ï¼Œä½¿å¾—åœ¨æ›´é•¿çš„time windowsä»¥åŠé«˜å™ªå£°ä¸‹çš„CMç®—æ³•æ¢å¤çš„å›¾åƒæ›´å¥½ï¼Œå¹¶åº”ç”¨åˆ°å¤ªç©ºæ•°æ®è§‚æµ‹ä¸­ã€‚


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# CMaxçš„ä¸»è¦åº”ç”¨
ç†è®ºä¸ŠCMæ¡†æ¶å¯ä»¥ç”¨åˆ°æ‰€æœ‰çš„event-based visionçš„topicä¸­ï¼Œç‰¹åˆ«æ˜¯ä»¥`groups event`çš„æ–¹å¼æ¥å¤„ç†eventæ•°æ®çš„ã€‚æœ¬è´¨ä¸ŠCMæ˜¯ä¸€ä¸ª`event processing framework`.

æ­¤å¤„ä¸»è¦åˆ—å‡ºçš„æ˜¯åŸºäºCMaxçš„åŸç†æ¥å®ç°çš„frameworkï¼Œè€Œä¸ä»…ä»…æ˜¯ä½œä¸ºæ•°æ®å¤„ç†çš„å½¢å¼

## Optical Flow Estimation
æ‰€è°“çš„å…‰æµå®é™…ä¸Šå°±è¯´æ¯ä¸ªpixelçš„motion vectorï¼ˆåœ¨å°çš„æ—¶é—´æ®µå†…ï¼‰ã€‚è€Œåœ¨ç†æƒ³çš„æƒ…å†µä¸‹ï¼ˆæ— ç©·å°ï¼‰åœ¨å›¾åƒå¹³é¢ä¸Šçš„ç‚¹çš„è½¨è¿¹åº”è¯¥æ˜¯ä¸€æ¡ç›´çº¿ï¼Œé‚£ä¹ˆå¯ä»¥ç”¨ä¸‹é¢å…¬å¼æ¥è¡¨è¾¾ï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219120444.png" width="60%" />
<figcaption>  
</figcaption>
</div>

è€Œæ•°æ®å…³è”çš„è¿‡ç¨‹ï¼Œå°±æ˜¯æŠŠåœ¨è¿™ä¸ªè½¨è¿¹ï¼ˆç›´çº¿ï¼‰ä¸Šçš„äº‹ä»¶ç»™å…³è”èµ·æ¥ã€‚
å…·ä½“çš„åšæ³•åˆ™æ˜¯ï¼š
é¦–å…ˆå°†eventç´¯ç§¯åœ¨ä¸€èµ·ï¼Œé€šè¿‡proposed trajectorieså°†ä»–ä»¬warpåˆ°å‚è€ƒæ—¶é—´t<sub>ref</sub>ä¸‹:

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219121240.png" width="60%" />
<figcaption>  
</figcaption>
</div>

å…¬å¼2å…¶å®å¯ä»¥çœ‹æˆeventç´¯ç§¯çš„imageï¼ˆan image patch of warped eventsï¼‰ï¼Œæ±‚å®ƒçš„æ–¹å·®å°±å¯ä»¥å¾—åˆ°ä¸‹é¢å…¬å¼

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219121823.png" width="60%" />
<figcaption>  
</figcaption>
</div>

Î¸ï¼Œä¹Ÿå°±æ˜¯å…‰æµé€Ÿåº¦ï¼Œå¯è§†åŒ–ä¸ºheat mapï¼ˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰ï¼Œå¯ä»¥çœ‹åˆ°å®ƒæ˜¯smoothä»¥åŠæœ‰æ˜æ˜¾çš„å³°å€¼çš„ã€‚å¹¶ä¸”åœ¨é’ˆå¯¹ä¸åŒçš„Î¸ï¼Œå¯¹åº”çš„IWEï¼ˆimage of warped eventï¼‰ä¹Ÿå¯è§†åŒ–åˆ°å³å­å›¾ä¸­äº†ã€‚å¯ä»¥çœ‹åˆ°æ›´é«˜çš„æ–¹å·®å¯¹åº”IWEæ›´é«˜å¯¹æ¯”åº¦ï¼ˆæ›´sharpï¼‰ã€‚å› æ­¤ä¼°ç®—å…‰æµçš„é—®é¢˜å¯ä»¥è½¬æ¢ä¸ºé€šè¿‡ä¸Šé¢å…¬å¼3ï¼ˆæœ€å¤§åŒ–æ–¹å·®å‡½æ•°ï¼‰æ¥å¯»æ‰¾Î¸å‚æ•°çš„è¿‡ç¨‹ã€‚

PSï¼šå› ä¸ºè¿™ä¸ªæ±‚æœ€ä¼˜çš„è¿‡ç¨‹ï¼Œå…¶å®ä¹Ÿå°±æ˜¯å¯¹äºIWEè¦æ±‚å¯¹æ¯”åº¦ï¼ˆcontrastï¼‰æœ€å¤§ï¼Œå› æ­¤è¿™ä¸ªæ–¹æ³•å‘½åä¸º`contrast maximization`

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219121945.png" width="60%" />
<figcaption>  
</figcaption>
</div>

å¯¹äºä¸Šé¢å…¬å¼2ä¸­b<sub>k</sub>,ä½œè€…åœ¨é™„åŠ ææ–™<sup>[link](https://www.ifi.uzh.ch/dam/jcr:a22071c9-b284-43c6-8f71-6433627b2db2/CVPR18_Gallego.pdf)</sup>ä¸­åšäº†æ·±å…¥çš„åˆ†æã€‚

æ‰€è°“çš„è€ƒä¸è€ƒè™‘ææ€§æ˜¯æŒ‡åœ¨è®¡ç®—IWEçš„æ—¶å€™çš„å…¬å¼2ï¼Œå¯¹åº”ä¸‹é¢ä¸¤ä¸ªå…¬å¼ï¼š
<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219151227.png" width="60%" />
<figcaption>  
</figcaption>
</div>

æ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤º.é¦–å…ˆä»ä¼˜åŒ–å‡½æ•°æ¥è¯´ï¼Œæ˜¯å¦ä½¿ç”¨ææ€§çš„ä¼˜åŒ–é€Ÿåº¦æ˜¯è¿‘ä¼¼çš„ã€‚ä½†æ˜¯é‡‡ç”¨äº†ææ€§æ˜¾ç„¶ä¸­é—´éƒ¨åˆ†è¦æ˜æ˜¾ä¸€äº›ï¼Œå¯ä»¥ç†è§£ä¸ºæ›´é›†ä¸­ï¼ˆoptimal value is slightly narrower and more pronouncedï¼‰ã€‚
è€Œé€šè¿‡IWEçš„å¯¹æ¯”åˆ†æåˆ™å‘ç°ï¼Œå¦‚æœå¸¦æœ‰ææ€§ï¼Œä¼šä½¿å¾— contrast functionï¼ˆå…¬å¼3ï¼‰ä¸‹é™å¾—æ›´åŠ æ˜æ˜¾ï¼ˆå› ä¸ºå­˜åœ¨æ­£è´Ÿææ€§ç›¸æŠµæ¶ˆï¼‰

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219151430.png" width="80%" />
<figcaption>  
</figcaption>
</div>

## Depth Estimation

æ­¤éƒ¨åˆ†æ˜¯åŸºäº[EMVS](https://www.researchgate.net/profile/Davide-Scaramuzza-3/publication/320914498_EMVS_Event-Based_Multi-View_Stereo-3D_Reconstruction_with_an_Event_Camera_in_Real-Time/links/5a663bff0f7e9b6b8fde4241/EMVS-Event-Based-Multi-View-Stereo-3D-Reconstruction-with-an-Event-Camera-in-Real-Time.pdf)çš„æ¡†æ¶çš„.
é¦–å…ˆç›¸æœºçš„poseã€intrinsicéƒ½æ˜¯æ˜¯å·²çŸ¥çš„ã€‚

å…¶æ¬¡ï¼Œå…¶ geometric model ä¸ºï¼šä¸€ä¸ªå›¾åƒç‚¹çš„trajectoryå¯ä»¥é€šè¿‡å°†ä¸€ä¸ª3Dç‚¹ï¼ˆåŒæ—¶æœ‰å·²çŸ¥çš„6 DoFç›¸æœºposeå’Œç›¸å¯¹äºå‚è€ƒè§†è§’ä¸‹3Dç‚¹çš„æ·±åº¦ï¼‰è¿›è¡ŒæŠ•å½±è·å¾—çš„ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æ­¤æ—¶å‚æ•°Î¸ä¸ºæ·±åº¦,å‡è®¾ä¸€ä¸ªå°çš„åŒºåŸŸï¼ˆpatchï¼‰å†…æ‰€æœ‰çš„ç‚¹éƒ½å…·æœ‰ç›¸åŒçš„æ·±åº¦ã€‚
<div align="center">
  <table style="border: none; background-color: transparent;">
    <tr>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219142622.png" width="100%" />
      </td>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219143658.png" width="100%" />
      </td>
    </tr>
  </table>
  <figcaption>
  å·¦å›¾ä¸ºEvent-based space-sweep  
  </figcaption>
</div>

åŸºäºCMæ¡†æ¶ä¸‹çš„æ·±åº¦ä¼°è®¡æ­¥éª¤å¦‚ä¸‹ï¼š
1. Transfer the events (triggered at the image plane of the moving event camera) onto the reference view using the candidate depth parameter. 
å¯¹äºä¸€ä¸ªäº‹ä»¶ç‚¹ e<sub>k</sub> é‡‡ç”¨ warp (W) functionè½¬æ¢åˆ°e<sup>'</sup><sub>k</sub>=(x<sup>'</sup><sub>k</sub>,t<sub>ref</sub>,p<sub>k</sub>)ä¸‹:

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219143824.png" width="60%" />
<figcaption>
</figcaption>
</div>

ç„¶åï¼Œç±»ä¼¼äºä¸Šé¢å…¬å¼2ï¼ˆä¹Ÿå°±æ˜¯ä¸‹å›¾å…¬å¼ï¼‰ï¼Œé€šè¿‡è®¡ç®—æ²¿ç€candidate point trajectoryäº‹ä»¶çš„æ•°é‡

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219144013.png" width="60%" />
<figcaption>
</figcaption>
</div>

2. é€šè¿‡æµ‹é‡ä¸Šé¢è·å¾—çš„å›¾ç‰‡çš„å¯¹æ¯”åº¦ï¼ˆæˆ–è€…è¯´æ–¹å·®ï¼‰æ¥æµ‹è¯•eventä»¥åŠdepth value Î¸çš„åŒ¹é…å¯¹ï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219144203.png" width="60%" />
<figcaption>
</figcaption>
</div>

3. æœ€å¤§åŒ–å¯¹æ¯”åº¦æ¥è·å–æ·±åº¦å€¼Î¸

ä¸‹å›¾é€šè¿‡å¯è§†åŒ–ä¸¤ä¸ªpatchçš„ä¼˜åŒ–è¿‡ç¨‹æ¥çœ‹CMç®—æ³•çš„æ·±åº¦ä¼°è®¡æ•ˆæœï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219144537.png" width="80%" />
<figcaption>
</figcaption>
</div>


## Rotational Velocity Estimation 
è¿™å…¶å®æ˜¯è§’é€Ÿåº¦çš„ä¼°è®¡,æœ€ç»å…¸çš„åº”è¯¥æ˜¯è¿™ç¯‡å·¥ä½œ[ã€ŠAccurate angular velocity estimation with an event camera (RAL2017)ã€‹](https://www.zora.uzh.ch/id/eprint/138896/1/RAL16_Gallego.pdf)

é¦–å…ˆï¼Œæ­¤ä»»åŠ¡æ˜¯é’ˆå¯¹åœ¨é™æ€ç¯å¢ƒä¸‹ç›¸æœºä»…æœ‰rotational motionçš„ï¼ŒåŒæ—¶ç›¸æœºçš„intrinsicä¹Ÿæ˜¯å·²çŸ¥ä¸”å»å¤±çœŸ~

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219145133.png" width="60%" />
<figcaption>
</figcaption>
</div>

1. å…¶ geometric modelå¦‚ä¸‹ï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219145522.png" width="60%" />
<figcaption>
</figcaption>
</div>

2. é€šè¿‡ä¸‹é¢æ„æ€æ„å»º image of warped events

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219144013.png" width="60%" />
<figcaption>
</figcaption>
</div>

3. æ›´å‰é¢ä¸¤ä¸ªå­ä»»åŠ¡ä¸€æ ·ï¼Œé‡‡ç”¨ä¸‹å¼æ¥æ„å»ºä¼˜åŒ–æ–¹ç¨‹

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219144203.png" width="60%" />
<figcaption>
</figcaption>
</div>

ä¼°ç®—çš„è§’é€Ÿåº¦çš„ç²¾åº¦è¿˜æ˜¯æ¯”è¾ƒé«˜çš„~

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219150052.png" width="60%" />
<figcaption>
</figcaption>
</div>


## Motion Estimation in Planar Scenes 
è¿™æ˜¯åœ¨å¹³é¢ä¸‹çš„rotation å’Œtranslationè¿åŠ¨ä¼°è®¡ï¼Œå±äº3 DoFå§~

åœ¨æ­¤åœºæ™¯ï¼Œå›¾åƒç‚¹çš„å˜æ¢å¦‚ä¸‹ï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219150439.png" width="60%" />
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219150728.png" width="60%" />
<figcaption>
</figcaption>
</div>

åŒæ ·åœ°ï¼Œé‡‡ç”¨ä¸Šé¢çš„å…¬å¼2å’Œ3æ¥æ„å»ºIWEä»¥åŠæ±‚æœ€ä¼˜åŒ–

<div align="center">
  <table style="border: none; background-color: transparent;">
    <tr>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219144013.png" width="100%" />
      </td>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219144203.png" width="100%" />
      </td>
    </tr>
  </table>
  <figcaption>
  </figcaption>
</div>

æ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219150909.png" width="60%" />
<figcaption>
</figcaption>
</div>

å¹¶ä¸”ä¸‹å›¾ä¹Ÿå±•ç¤ºäº†é‡‡ç”¨æˆ–è€…ä¸é‡‡ç”¨CMæ¡†æ¶ä¸‹çš„VIOæ•ˆæœã€‚å¯ä»¥çœ‹åˆ°é‡‡ç”¨CMç®—æ³•æ¢å¤çš„å¹³é¢ç‚¹æ˜¯è¦å¹³æ•´ä¸€äº›~

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250219151034.png" width="60%" />
<figcaption>
</figcaption>
</div>



<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
## SLAM or 6DoF Pose Tracking
æ­¤å¤„çš„åŸºäºSLAMçš„åº”ç”¨æ˜¯æŒ‡full-SLAMæˆ–6DoF Pose Trackingï¼Œå› ä¸ºå¤§éƒ¨åˆ†çš„CMaxä¸­æ‰€è°“çš„motion estimationéƒ½æ˜¯æŒ‡ rotational æˆ–è€…fronto-parallel motion estimationï¼Œè¿™å…¶å®åº”ç”¨åœºæ™¯éå¸¸å±€é™çš„ï¼Œæ¯”å¦‚å·¥ä½œ[ã€ŠCMax-SLAM: Event-based Rotational-Motion Bundle Adjustment and SLAM System using Contrast Maximization (TRO2024)ã€‹](https://arxiv.org/pdf/2403.08119)</sup>

<!-- [ã€ŠVisual Odometry with an Event Camera Using Continuous Ray Warping and Volumetric Contrast Maximization (Sensor2022)ã€‹](https://arxiv.org/pdf/2107.03011)å®ç°äº†contrast maximization in 3D -->

æ–‡çŒ®<sup>[ref](https://ieeexplore.ieee.org/abstract/document/10855459)</sup>åˆ™æ˜¯æ˜¯é¦–æ¬¡å®ç°äº†å°†CM frameworkç”¨åˆ°EVIOé—®é¢˜ä¸­ï¼›è€Œæ›´æ—©çš„æ–‡çŒ®<sup>[ref](https://ieeexplore.ieee.org/abstract/document/10275026)</sup>åˆ™æ˜¯é¦–æ¬¡å°†CMæ¡†æ¶æ‹“å±•åˆ°EVOï¼ˆevent+image odometryï¼‰é—®é¢˜ä¸­ï¼Œè®ºæ–‡ä¸­ä¹Ÿå®£ç§°é¦–æ¬¡æ‹“å±•åˆ°6 DoF motionã€‚

æœ¬è´¨ä¸Šè¿™ä¸¤ä¸ªèƒ½åŸºäºCMå®ç°6DoF Pose Trackingçš„åŸºæœ¬åŸå› éƒ½æ˜¯ä»…ä»…ç”¨CMæ¥ä½œä¸ºè¿åŠ¨è¡¥å¿ï¼Œå¹¶ä¸æ˜¯ç›´æ¥é‡‡ç”¨CMçš„åŸç†æ¥è®¡ç®—poseï¼Œå—é™äºå±€éƒ¨æœ€ä¼˜ä»¥åŠå®¹æ˜“é€€åŒ–ï¼ŒåŸºäºCMåŸç†çš„motion estimationä¸€èˆ¬éƒ½æ˜¯é™åˆ¶åœ¨rotational æˆ–è€…fronto-parallel motion estimation.


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
## Learning-based Framework
å…¶ä¸­CMax frameworkä¹Ÿè¢«å¹¿æ³›åº”ç”¨äºdeep learningä¸­ï¼Œç‰¹åˆ«åœ°ï¼Œæ˜¯ç”¨æ¥æ„å»ºSelf-Supervised Learning lossï¼š
<!-- * [Reducing the sim-to-real gap for event camerasï¼ˆECCV2020ï¼‰](https://arxiv.org/pdf/2003.09078) è¿™ä¸ªä¸å±äº -->

* [Unsupervised event-based learning of optical flow, depth, and egomotion (CVPR2019)](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Unsupervised_Event-Based_Learning_of_Optical_Flow_Depth_and_Egomotion_CVPR_2019_paper.pdf)
<!-- * [Unsupervised learning of a hierarchical spiking neural network for optical flow estimation: From events to global motion perception (TPAMI2019)](https://arxiv.org/pdf/1807.10936) -->
* [Back to event basics: Self-supervised learning of image reconstruction for event cameras via photometric constancy (CVPR2021)](https://openaccess.thecvf.com/content/CVPR2021/papers/Paredes-Valles_Back_to_Event_Basics_Self-Supervised_Learning_of_Image_Reconstruction_for_CVPR_2021_paper.pdf)
* [Self-supervised learning of event-based optical flow with spiking neural networks (NIPS2021)](https://proceedings.neurips.cc/paper_files/paper/2021/file/39d4b545fb02556829aab1db805021c3-Paper.pdf)
* [Taming contrast maximization for learning sequential, low-latency, event-based optical flow (CVPR2023)](https://openaccess.thecvf.com/content/ICCV2023/papers/Paredes-Valles_Taming_Contrast_Maximization_for_Learning_Sequential_Low-latency_Event-based_Optical_Flow_ICCV_2023_paper.pdf)
* [Fully neuromorphic vision and control for autonomous drone flight (SRO2024)](https://www.science.org/doi/epdf/10.1126/scirobotics.adi0591)
* [Motion-prior Contrast Maximization for Dense Continuous-Time Motion Estimation (ECCV2024)](https://arxiv.org/pdf/2407.10802)

å¯¹äºå…‰æµä¼°ç®—çš„ç½‘ç»œï¼Œå¯ä»¥é€šè¿‡CMæ¡†æ¶æ¥å®ç°Unsupervisedæˆ–è€…Self-supervised learning(é¦–æ¬¡åº”è¯¥æ˜¯åœ¨è®ºæ–‡ã€Š [Unsupervised event-based learning of optical flow, depth, and egomotion (CVPR2019)](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Unsupervised_Event-Based_Learning_of_Optical_Flow_Depth_and_Egomotion_CVPR_2019_paper.pdf)ã€‹ä¸­æå‡ºçš„)ã€‚åŸç†å¦‚ä¸‹ï¼š

å¯¹äºä¼°ç®—å‡ºæ¥çš„å…‰æµï¼šu(x, y), v(x, y)å¯ä»¥æ„å»ºwarp functionå¦‚ä¸‹ï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220121922.png" width="60%" />
<figcaption>
</figcaption>
</div>

å¦‚æœç½‘ç»œä¼°ç®—çš„å…‰æµæ˜¯æ­£ç¡®çš„ï¼Œé‚£ä¹ˆå°±å¯ä»¥å°†è¿™æ®µäº‹ä»¶å†…çš„äº‹ä»¶éƒ½è¿›è¡ŒæŠ•å½±ï¼Œè·å¾—çš„IWEçš„å¯¹æ¯”åº¦æ˜¯æœ€å¤§çš„ã€‚å½“ç„¶å¦‚æœå•çº¯ç”¨å¯¹æ¯”åº¦æœ€å¤§åŒ–è¿™ä¸ªloss functionï¼Œç½‘ç»œå¾ˆå®¹æ˜“overfit(åº”è¯¥ä¹Ÿå°±æ˜¯å‡ºç°ç±»ä¼¼äºEvent Collapse)çš„æƒ…å†µã€‚
`the network would easily overfit to this loss, by predicting flow values that push all events within each region of the image to a line. `
é‚£ä¹ˆä½œè€…é€šè¿‡åˆ†ç¦»äº‹ä»¶çš„ææ€§ï¼Œç”Ÿæˆan image of the average timestamp at each pixel for each polarityï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220122850.png" width="60%" />
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220122941.png" width="60%" />
<figcaption>
</figcaption>
</div>

è€Œé‡‡ç”¨çš„æœ€ç»ˆçš„losså®é™…ä¸Šå°±æ˜¯ä¸¤ç§å›¾ç‰‡çš„å¹³æ–¹å’Œ:

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220123059.png" width="60%" />
<figcaption>
</figcaption>
</div>

å†è¿›ä¸€æ­¥çš„ï¼Œä½¿ç”¨è¿™ä¸ªå‚è€ƒæ—¶é—´t'ä¼šå­˜åœ¨ä¸€å®šçš„å°ºåº¦é—®é¢˜(å¯¹äºç¦»å‚è€ƒäº‹ä»¶ä¸ä¸€æ ·çš„äº‹ä»¶ï¼Œæ¢¯åº¦ç›¸å·®å¾ˆå¤§)ï¼Œé‚£ä¹ˆå°±è¿›ä¸€æ­¥çš„æŠŠlossè½¬æ¢ä¸ºbackwards å’Œ forwardsä¹Ÿå°±æ˜¯ tâ€² = t<sub>1</sub> å’Œ tâ€² = t<sub>N</sub>ï¼ˆå¯ä»¥ç†è§£ä¸ºç”¨å¤´ä¸ä¸ºåˆ†åˆ«ä½œä¸ºå‚è€ƒæ—¶é—´æ¥warpï¼‰:

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220123439.png" width="60%" />
<figcaption>
</figcaption>
</div>

åŒæ—¶ï¼Œåœ¨ä¸Šé¢çš„contrast lossçš„åŸºç¡€ä¸Šï¼Œé¢å¤–å¼•å…¥äº†ä¸€ä¸ªCharbonnier smoothness prior (local smoothness regularization):

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220123734.png" width="60%" />
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220124359.png" width="60%" />
<figcaption>
</figcaption>
</div>

å…¶å®è®ºæ–‡ã€Š[EV-FlowNet: Self-supervised optical flow estimation for event-based cameras (RSS2018)](https://arxiv.org/pdf/1802.06898)ã€‹[Github](https://github.com/daniilidis-group/EV-FlowNet)ä¹Ÿæ˜¯ç”¨äº†ç±»ä¼¼çš„æ€è·¯ï¼Œåªä¸è¿‡æ˜¯åŸºäºphotometric lossè€Œå·²

<div align="center">
  <table style="border: none; background-color: transparent;">
    <tr>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220132113.png" width="100%" />
      </td>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220132124.png" width="100%" />
      </td>
    </tr>
  </table>
  <figcaption>
  </figcaption>
</div>

è®ºæ–‡ã€Š[Unsupervised event-based learning of optical flow, depth, and egomotion (CVPR2019)](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Unsupervised_Event-Based_Learning_of_Optical_Flow_Depth_and_Egomotion_CVPR_2019_paper.pdf)ã€‹åº”è¯¥æ˜¯CMç”¨åˆ°self-supervised learningé‡Œé¢æœ€ç»å…¸çš„è®ºæ–‡ï¼Œä¸è¿‡å¯æƒœçš„æ˜¯ä½œè€…å¹¶æ²¡æœ‰å¼€æºä»£ç ï¼Œä½†æ˜¯ç½‘ä¸Šæ‰¾åˆ°äº†ä¸€ä¸ªéå®˜æ–¹å®ç°[Github](https://github.com/mingyip/Motion_Compensated_FlowNet)åç»­å¯ä»¥æµ‹è¯•çœ‹çœ‹~

æ¥ä¸‹æ¥è®ºæ–‡ã€Š[Back to event basics: Self-supervised learning of image reconstruction for event cameras via photometric constancy (CVPR2021)](https://openaccess.thecvf.com/content/CVPR2021/papers/Paredes-Valles_Back_to_Event_Basics_Self-Supervised_Learning_of_Image_Reconstruction_for_CVPR_2021_paper.pdf)ã€‹å®é™…ä¸Šå°±æ˜¯æ²¿ç”¨äº†CM for SSLçš„æ€è·¯ï¼Œåªæ˜¯æ”¹ä¸ºSSLä¼°ç®—çš„å…‰æµæ¥ç›‘ç£image reconstructionï¼Œå¹¶ä¸”è½»é‡åŒ–äº†è¿™ä¸ªå…‰æµç½‘ç»œ

<div align="center">
  <img src="../images/å¾®ä¿¡æˆªå›¾_20250221165806.png" width="60%" />
<figcaption>
</figcaption>
</div>

è€Œåœ¨è®ºæ–‡ã€Š[Self-supervised learning of event-based optical flow with spiking neural networks (NIPS2021)](https://proceedings.neurips.cc/paper_files/paper/2021/file/39d4b545fb02556829aab1db805021c3-Paper.pdf)ã€‹ä¸­ä½œè€…åˆ™æ˜¯æŠŠCNNæ”¹ä¸ºSNNï¼ˆSpiking neurons networkï¼‰ã€‚
æ­¤å¤–ï¼Œä½œè€…ä¹Ÿæåˆ°ï¼Œæ¯æ¬¡è¾“å…¥çš„eventéœ€è¦è¶³å¤Ÿçš„å¤šæ‰å¯ä»¥ä¿è¯è¿™ä¸ªlossæ˜¯æœ‰æ•ˆçš„,ä½†æ˜¯å¦‚æœæ˜¯é‡‡ç”¨é«˜é¢‘çš„å¤„ç†ä»¥æœŸè·å¾—é«˜é¢‘çš„å…‰æµï¼ˆfine discretization of the event streamï¼‰ï¼Œå°±ä¸é€‚ç”¨äº†~

å› æ­¤ï¼Œä½œè€…å¯¹CM lossæå‡ºæ”¹è¿›`improves its convexity`ä¹Ÿå°±æ˜¯é¢å¤–é™¤ä»¥pixelæ•°ï¼š

<div align="center">
  <img src="../images/å¾®ä¿¡æˆªå›¾_20250221173602.png" width="60%" />
<figcaption>
</figcaption>
</div>

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç¡®å®é€šè¿‡æ·»åŠ äº†scaling factorï¼Œloss functionæ˜¾å¾—æ›´â€œå‡¸â€äº†~

<div align="center">
  <img src="../images/å¾®ä¿¡æˆªå›¾_20250221173749.png" width="60%" />
<figcaption>
</figcaption>
</div>

æ­¤å¤–ä½œè€…ä¹Ÿæåˆ°ï¼ŒCMç®—æ³•æ˜¯éœ€è¦æœ‰è¶³å¤Ÿçš„linear bluræ‰æ˜¯æœ‰æ•ˆçš„ï¼Œè€Œæ­¤å¤„é‡‡ç”¨SNNçš„ç»“æ„ï¼Œæ¯æ¬¡è¾“å…¥éƒ½æ˜¯`small number of event`,å› æ­¤ä½œè€…æå‡ºç›¸åº”çš„è§£å†³æ–¹æ¡ˆï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220135703.png" width="60%" />
<figcaption>
PS: è¿™ç‚¹è¿˜ä¸æ˜¯å¤ªç†è§£ï¼Œåç»­çœ‹çœ‹ä»£ç ï¼šhttps://github.com/tudelft/event_flow
</figcaption>
</div>

å¦‚ä¸‹å›¾ï¼ˆå·¦ä¸‹ï¼‰æ‰€ç¤ºã€‚è¿™äº›ç”¨CMæ¥åšself-supervised learningçš„æ–¹æ³•éƒ½æ˜¯éœ€è¦å‡è®¾`events move linearly within the time window of the loss`.å› æ­¤åœ¨æ–‡çŒ®ã€Š[Taming contrast maximization for learning sequential, low-latency, event-based optical flow (CVPR2023)](https://openaccess.thecvf.com/content/ICCV2023/papers/Paredes-Valles_Taming_Contrast_Maximization_for_Learning_Sequential_Low-latency_Event-based_Optical_Flow_ICCV_2023_paper.pdf)ã€‹ä¸­ï¼Œé¦–å…ˆé€šè¿‡ä½¿ç”¨å¾ªç¯æ¨¡å‹æ¥å¤„ç†å°åˆ†åŒºï¼ˆsmall partitionsï¼‰çš„äº‹ä»¶æµï¼Œè€Œä¸æ˜¯å¤„ç†å¤§é‡è¾“å…¥äº‹ä»¶(ä¹Ÿå°±æ˜¯æ²¿ç€ä»–ä»¬ä¸Šä¸€ç¯‡è®ºæ–‡ã€Š[Self-supervised learning of event-based optical flow with spiking neural networks (NIPS2021)](https://proceedings.neurips.cc/paper_files/paper/2021/file/39d4b545fb02556829aab1db805021c3-Paper.pdf)ã€‹æ¥åš)ã€‚è¿™æ ·å¯ä»¥å¾ˆå¥½åˆ©ç”¨event cameraçš„high temporal resolutionã€‚è¿™ä¹Ÿå°±æ˜¯è®ºæ–‡ä¸­æåˆ°çš„`sequential processing`

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220144810.png" width="80%" />
<figcaption>
</figcaption>
</div>

å¯¹äº`sequential processing`ä½œè€…é€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220175806.png" width="60%" />
<figcaption>
</figcaption>
</div>

åŒæ—¶å†é€šè¿‡`iterative event warping`è¿›è€Œå¯ä»¥å®ç°å¤šæ®µæ—¶é—´çš„event warpæ¥é¿å…å‡è®¾äº‹ä»¶æ˜¯çº¿æ€§motionçš„~

<div align="center">
  <table style="border: none; background-color: transparent;">
    <tr>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220180553.png" width="100%" />
      </td>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220180601.png" width="100%" />
      </td>
    </tr>
  </table>
  <figcaption>
  </figcaption>
</div>

å…¶æ¬¡é€šè¿‡multiple temporal scalesï¼Œä¹Ÿå°±æ˜¯å¤šç§æ—¶é—´å°ºå¯¸æ¥ç´¯ç§¯äº‹ä»¶ï¼Œæ¥å¤„ç†CM frameworkã€‚

<div align="center">
  <table style="border: none; background-color: transparent;">
    <tr>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220181125.png" width="100%" />
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220181444.png" width="100%" />
      </td>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220181139.png" width="100%" />
      </td>
    </tr>
  </table>
  <figcaption>
  </figcaption>
</div>

è€Œä¼ ç»Ÿçš„å…¶ä»–CMæ–¹æ³•åˆ™æ˜¯å¦‚ä¸‹ï¼š

<div align="center">
  <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220181228.png" width="60%" />
<figcaption>
</figcaption>
</div>


è®ºæ–‡ä¹Ÿé€šè¿‡å¤§é‡çš„å®éªŒè¯æ˜äº†è¿™ç§multi-timescale CM frameworkè¶…è¶Šæ‰€æœ‰åŸºäºCMçš„frameworkï¼Œä»…ä»…ä¸å¦‚ç”¨GTæ•°æ®è®­ç»ƒçš„çº¯supervised learningæ–¹æ³•
<div align="center">
  <table style="border: none; background-color: transparent;">
    <tr>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220145754.png" width="100%" />
      </td>
      <td style="width: 50%; border: none; padding: 0.01; background-color: transparent; vertical-align: middle;">
        <img src="https://kwanwaipang.github.io/ubuntu_md_blog/images/å¾®ä¿¡æˆªå›¾_20250220145754.png" width="100%" />
      </td>
    </tr>
  </table>
  <figcaption>
supervised learning (SL) methods trained with ground truth, 
self-supervised learning (SSL) methods trained with grayscale images (SSLF) or events (SSLE), 
and model-based approaches (MB, non-learning method).

  </figcaption>
</div>


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# Paper Resource
æ­¤å¤„åˆ—å‡ºCMaxç›¸å…³æˆ–è€…ç”¨åˆ°CMaxçš„æ–‡çŒ®ã€‚

* A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth and Optical Flow Estimation (CVPR2018)
  * [paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gallego_A_Unifying_Contrast_CVPR_2018_paper.pdf)
  * [supplementary material](https://www.ifi.uzh.ch/dam/jcr:a22071c9-b284-43c6-8f71-6433627b2db2/CVPR18_Gallego.pdf)

* Accurate angular velocity estimation with an event camera (RAL2017)
  * [paper](https://www.zora.uzh.ch/id/eprint/138896/1/RAL16_Gallego.pdf)

* Focus is all you need: Loss functions for event-based vision (CVPR2019)
  * [paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Gallego_Focus_Is_All_You_Need_Loss_Functions_for_Event-Based_Vision_CVPR_2019_paper.pdf)

* Event Cameras, Contrast Maximization and Reward Functions: An Analysis (CVPR2019)
  * [paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Stoffregen_Event_Cameras_Contrast_Maximization_and_Reward_Functions_An_Analysis_CVPR_2019_paper.pdf)
  * [Github](https://github.com/TimoStoff/events_contrast_maximization): A python library for contrast maximization and voxel creation using events.

* Unsupervised event-based learning of optical flow, depth, and egomotion (CVPR2019)
  * [paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Unsupervised_Event-Based_Learning_of_Optical_Flow_Depth_and_Egomotion_CVPR_2019_paper.pdf)
  * [éå®˜æ–¹çš„githubå®ç°](https://github.com/mingyip/Motion_Compensated_FlowNet)

* Globally optimal contrast maximisation for event-based motion estimation (CVPR2020)
  * [paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Globally_Optimal_Contrast_Maximisation_for_Event-Based_Motion_Estimation_CVPR_2020_paper.pdf)

* Real-Time Rotational Motion Estimation With Contrast Maximization Over Globally Aligned Events (RAL2021)
  * [paper](https://ieeexplore.ieee.org/abstract/document/9454404)

<!-- * Reducing the sim-to-real gap for event camerasï¼ˆECCV2020ï¼‰
  * [paper](https://arxiv.org/pdf/2003.09078)
  * [github](https://github.com/TimoStoff/event_cnn_minimal) -->

* Globally-optimal event camera motion estimation (ECCV2020)
  * [paper](https://arxiv.org/pdf/2203.03914)

* Globally-optimal contrast maximisation for event cameras (TPAMI2021)
  * [paper](https://arxiv.org/pdf/2206.05127)
  * æ­¤ç¯‡æ˜¯ä¸Šä¸€ç¯‡çš„æœŸåˆŠç‰ˆæœ¬

<!-- * Unsupervised learning of a hierarchical spiking neural network for optical flow estimation: From events to global motion perception (TPAMI2019)
  * [paper](https://arxiv.org/pdf/1807.10936)
  * [github](https://github.com/tudelft/cuSNN) -->

* Self-supervised learning of event-based optical flow with spiking neural networks (NIPS2021)
  * [paper](https://proceedings.neurips.cc/paper_files/paper/2021/file/39d4b545fb02556829aab1db805021c3-Paper.pdf)
  * [Supplementary Material](https://arxiv.org/pdf/2106.01862)
  * [github](https://github.com/tudelft/event_flow)

* Back to event basics: Self-supervised learning of image reconstruction for event cameras via photometric constancy (CVPR2021)
  * [paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Paredes-Valles_Back_to_Event_Basics_Self-Supervised_Learning_of_Image_Reconstruction_for_CVPR_2021_paper.pdf)
  * [github](https://github.com/tudelft/ssl_e2vid)

* Visual Odometry with an Event Camera Using Continuous Ray Warping and Volumetric Contrast Maximization (Sensor2022)
  * [paper](https://arxiv.org/pdf/2107.03011)

* Contrast maximization-based feature tracking for visual odometry with an event camera (Processes2022)

* Recursive Contrast Maximization for Event-Based High-Frequency Motion Estimation (IEEE Access2022)

* Event Collapse in Contrast Maximization Frameworks (Sensor 2022)
  * [paper](https://web.archive.org/web/20220813065935id_/https://depositonce.tu-berlin.de/bitstream/11303/17328/1/sensors-22-05190-v3.pdf)

* A Fast Geometric Regularizer to Mitigate Event Collapse in the Contrast Maximization Framework (AIS2023)
  * [paper](https://advanced.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/aisy.202200251)
  * [github](https://github.com/tub-rip/event_collapse)
  * è¿™ç¯‡åº”è¯¥æ˜¯ä¸Šä¸€ç¯‡çš„æ‹“å±•ç‰ˆ

* Taming contrast maximization for learning sequential, low-latency, event-based optical flow (CVPR2023)
  * [paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Paredes-Valles_Taming_Contrast_Maximization_for_Learning_Sequential_Low-latency_Event-based_Optical_Flow_ICCV_2023_paper.pdf)
  * [Supplementary Material](https://openaccess.thecvf.com/content/ICCV2023/supplemental/Paredes-Valles_Taming_Contrast_Maximization_ICCV_2023_supplemental.pdf)
  * [github](https://github.com/tudelft/taming_event_flow)

* Density Invariant Contrast Maximization for Neuromorphic Earth Observations (CVPR2023)
  * [paper](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Arja_Density_Invariant_Contrast_Maximization_for_Neuromorphic_Earth_Observations_CVPRW_2023_paper.pdf)
  * [github](https://github.com/neuromorphicsystems/event_warping)

* MC-VEO: A visual-event odometry with accurate 6-DoF motion compensation (TIV2023)
  * [paper](https://ieeexplore.ieee.org/abstract/document/10275026)
  * [github](https://cslinzhang.github.io/MC-VEO/MC-VEO.html)

* CMax-SLAM: Event-based Rotational-Motion Bundle Adjustment and SLAM System using Contrast Maximization (TRO2024)
  * [paper](https://arxiv.org/pdf/2403.08119)
  * [github](https://github.com/tub-rip/cmax_slam)

* Secrets of Event-based Optical Flow (ECCV2022)
  * [paper](https://arxiv.org/pdf/2207.10022)

* Secrets of Event-based Optical Flow, Depth and Ego-motion Estimation by Contrast Maximization (TPAMI2024)
  * [paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10517639)
  * [github](https://github.com/tub-rip/event_based_optical_flow)
  * è¿™ç¯‡æ˜¯ä¸Šä¸€ç¯‡çš„æœŸåˆŠç‰ˆæœ¬

* Motion-prior Contrast Maximization for Dense Continuous-Time Motion Estimation (ECCV2024)
  * [paper](https://arxiv.org/pdf/2407.10802)
  * [github](https://github.com/tub-rip/MotionPriorCMax)

* Secrets of Edge-Informed Contrast Maximization for Event-Based Vision
  * [paper](https://arxiv.org/pdf/2409.14611)

* EROAM: Event-based Camera Rotational Odometry and Mapping in Real-time
  * [paper](https://arxiv.org/pdf/2411.11004)
  * [github](https://wlxing1901.github.io/eroam/)

* Fully neuromorphic vision and control for autonomous drone flight (SRO2024)
  * [paper](https://www.science.org/doi/epdf/10.1126/scirobotics.adi0591)
  * [github](https://github.com/tudelft/event_planar)

* Event-Frame-Inertial Odometry Using Point and Line Features Based on Coarse-to-Fine Motion Compensation (RAL2025)
  * [paper](https://ieeexplore.ieee.org/abstract/document/10855459)
  * [github](https://github.com/choibottle/C2F-EFIO)




<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# å¤ç°æˆ–æµ‹è¯•CMç›¸å…³å·¥ä½œ

* [CMax-SLAM-comment](https://github.com/KwanWaiPang/CMax-SLAM-comment)


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
<!-- # å‚è€ƒèµ„æ–™ -->