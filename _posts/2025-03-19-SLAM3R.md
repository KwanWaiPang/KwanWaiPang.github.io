---
layout: post
title: "论文学习及实验笔记之——《SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos》"
date:   2025-03-19
tags: [Deep Learning,SLAM]
comments: true
author: kwanwaipang
toc: true
---


<!-- * 目录
{:toc} -->


<!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
# 引言
之前博客对Transformer以及基于Transformer的SLAM进行了解读。而最近北大开源了SLAM3R，一种实时（20+ FPS）、可稠密重建的单目SLAM系统，且也是基于Transformer架构的

为此，写下本博文记录阅读及测试过程，本博文仅供本人学习记录用~

相关的资料：
* [paper](https://arxiv.org/pdf/2412.09401)
* [Github](https://github.com/PKU-VCL-3DV/SLAM3R)
* 本博客采用的代码及注释（如有）均在[Github](https://github.com/KwanWaiPang/SLAM3R)
* 博客：[Paper Survey之——Awesome Transformer-based SLAM](https://kwanwaipang.github.io/Transformer_SLAM/)
* [Awesome Transformer-based SLAM](https://github.com/KwanWaiPang/Awesome-Transformer-based-SLAM)
* 博客：[What is Transformer? Form NLP to CV](https://kwanwaipang.github.io/Transformer/)


# 理论解读

对于输入的视频流，首先先通过滑窗的机制将其转换成有overlapping的片段，而由于采用了DUSt3R的架构，因此可以直接从每个窗口的RGB图像获取3D point map。
SLAM3R分为两个层次架构：
1. Images-to-Points (I2P) network ：通过滑动窗口来处理来自输入视频流的短片段，然后提取局部的3D几何信息
2. Local-to-World (L2W) network：将这些获得的3D局部信息逐步登记，以获取全局一致性的3D场景
而I2P应该也就是DUSt3R，只是只会处理关键帧的信号，并且可以处理multiple view；
而L2W网络则是增量数处理来自于DUSt3R的信息最终获取全局三维环境。

I2P和L2W两个网络都是基于DUSt3R的架构（做了小量的更改），训练的时候都是用DUSt3R的权重来初始化的

PS：个人感觉，SLAM3R本质上跟MASt3R-SLAM有点像，只不过是用MASt3R与DUSt3R的区别，且MASt3R-SLAM更接近于传统SLAM的思想，以two-view 3D point matching为架构搭建一个新的SLAM体系，但是SLAM3R则是把DUSt3R改成SLAM的输入输出流~

至于架构的理念感觉跟Spann3R是很像的，虽然在introduction提到了Spann3R缺点是会存在累积误差，但是作者自己也在Limitation提到SLAM3R在大场景也存在累积误差。

系统框架如下图所示。I2P将输入的窗口中的数据转换为3D pointmap，而系统也会从每个窗口中选择关键帧（窗口中的中间图片为关键帧）来作为当前point reconstruction的reference coordinate.而全局的初始坐标则是用第一个窗口

<div align="center">
  <img src="../images/微信截图_20250319131933.png" width="80%" />
<figcaption>  
</figcaption>
</div>

I2P相比起DUSt3R的改进应该是：
1. 采用multi-branch ViT，这样可以适用于多视角的输入（在这点上，MASt3R-SLAM应该仍然是两个视角输入的处理）
2. decoder部分，DUSt3R用的是标准的cross attention，此处用的是`multi-view cross-attention`
<div align="center">
  <img src="../images/微信截图_20250319134011.png" width="60%" />
<figcaption>  
</figcaption>
</div>
看上去似乎是将多个视角下得到的token都进行cross-attention，网络结构都是不变的，只是引入更多的数据流一起运算。

至于训练就跟DUSt3R一样，用GT scene point于估算的point map求loss：
<div align="center">
  <img src="../images/微信截图_20250319134255.png" width="60%" />
<figcaption>  
</figcaption>
</div>

对于I2W部分此处略过，感觉跟I2P差不多，只是改变一些DUSt3R的数据流，毕竟实验部分训练也提到尽量把网络架构的参数跟DUSt3R保持一致，可以用其预训练权重的~


## 论文的实验效果

首先看三维重建的效果，其中`SLAM3R-NoConf`是直接用所有帧预测出的全部的pointmap来进行重建的结果，而SLAM3R则是原系统（有根据confidence map filtering处理的）

<div align="center">
  <img src="../images/微信截图_20250319121240.png" width="80%" />
  <img src="../images/微信截图_20250319121346.png" width="80%" />
  <img src="../images/微信截图_20250319121637.png" width="80%" />
<figcaption>  
</figcaption>
</div>

其他的实验就属于消融实验来分析各个模块的性能了~

而在附加材料中有精度对比的实验
<div align="center">
  <img src="../images/微信截图_20250319122102.png" width="80%" />
<figcaption>  
</figcaption>
</div>

看似效果是没有超越DROID-SLAM的，也没有跟ORB-SLAM，DPVO或DPV-SLAM对比，应该从定位上来看是稍逊于MASt3R-SLAM的，MASt3R-SLAM在经典的精度评估数据集TUM上超越传统的方法

# 实验测试

