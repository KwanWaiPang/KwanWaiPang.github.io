<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">

  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  .paper-pdf{
        width:100%;
        height:80vh;
        display: flex; /* ËÆ©ÂÆπÂô®Ê∞¥Âπ≥Â±Ö‰∏≠ */
        margin: auto;
    }
    .map-container {
        display: flex;
        justify-content: center;
        align-items: center;
    }
  </style>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Kwan Wai-Pang</title>
<meta name="author" content="Kwan Wai-Pang " />
<meta name="description" content="Academic Website of Kwan Wai-Pang" />
<meta name="keywords" content="Event-based Vision, SLAM, Robotics" />

<!-- OpenGraph Áî®‰∫éÊõ¥Êñ∞È°µÈù¢-->
<meta property="og:site_name" content="Academic Website" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Kwan Wai-Pang | Academic Website " />
<meta property="og:description" content="Welcome üòä" />
<meta property="og:image" content="https://kwanwaipang.github.io/Poster_files/hku_logo.jpg" />

<meta property="og:locale" content="en" />

<meta name="google-site-verification" content="Jtxa1xy6N3_2RjVVFrZgXjPZ0AHklxJXQ1eQ6QXNWr8" />


<!-- Bootstrap & MDB -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Barriecito&family=Poppins:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/PASTIE.css" media="none" id="highlight_theme_light" />

<!-- <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'> -->
<script type="text/javascript" src="./js/js/hidebib.js"></script>

 <!-- Start : Google Analytics Code -->
 <!-- <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'G-R1QX9D95NS', 'auto');
    ga('send', 'pageview');
    </script> -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-R1QX9D95NS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-R1QX9D95NS');
  </script>
  <!-- End : Google Analytics Code -->

  <!-- Scramble Script by Jeff Donahue -->
  <script src="./js/js/scramble.js"></script>
</head>

<!-- Styles -->
<!-- ‰∏ãÈù¢‰∏∫ÁΩëÈ°µÁöÑÁï•Áº©Âõæ -->
<link rel="shortcut icon" href="https://kwanwaipang.github.io/Poster_files/hku_logo.jpg"/>

<!-- <link rel="stylesheet" href="https://kwanwaipang.github.io/File/Blogs/assets/css/main.css"> -->
<link rel="stylesheet" href="https://kwanwaipang.github.io/File/Blogs/assets/css/fonts.css">
<link rel="stylesheet" href="/assets/css/fonts.css">

</head>
 
<body>


<!-- ‰∏ãÈù¢ÊèíÂÖ•ÂÜÖÂÆπ -->
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <p align="center">
        <pageheading>Kwan Wai-Pang</pageheading><br>
    </p>

    <tr>
        <td width="30%" valign="top"><img src="https://kwanwaipang.github.io/Poster_files/Image/Guan_Weipeng.jpg" width="100%" style="border-radius:15px"> 
        <p align=center>
          | <a href="mailto:wpguan@connect.hku.hk">Email</a> |
          <a href="https://scholar.google.com/citations?user=fUU5Cv0AAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a> |
          <br>
          |<a href="https://github.com/KwanWaiPang" target="_blank">Github</a> | 
          <a href="https://orcid.org/0000-0003-4518-7852"  target="_blank" >ORCID</a> | 
          <a href="./home/">Blog</a> |
          <br>
          | <a href="./File/Representative_works/Granted_patents.html" target="_blank">Granted Patents</a> |
        </p>
        </td>
  
        <td width="70%" valign="top" align="justify">
        <!-- <p style="font-size: 16px;"> -->
            <!-- Kwan Wai-Pang is currently pursuing PhD degree in Robotics at the University of Hong Kong.
            Previously, he obtained his Bachelor degree in Electronic Science & Technology, as well as Master degree in Control Theory and Control Engineering from the South China University of Technology.
            He has worked with several reputable organizations, including: Samsung Electronics, Huawei Technologies, The Chinese Academy of Sciences, The Chinese University of Hong Kong, The Hong Kong University of Science and Technology, etc.
            He has also served as technical consultant for multiple companies, such as TCL.
            Moreover, he has published over 60 research articles in prestigious international journals and conferences, as well as holds more than 50 authorized patents.
            His research interests primarily focus on robotics, event-based VO/VIO/SLAM, visible light positioning, etc. -->
        </p>
            Kwan Wai-Pang is currently pursuing PhD degree in Robotics at the University of Hong Kong, supervised by <a href="https://scholar.google.com/citations?user=ts7ItWgAAAAJ&hl=en&oi=ao" target="_blank">Prof. Lu Peng</a> and co-supervised by <a href="https://meweb.hku.hk/jlam/" target="_blank">Prof. James Lam</a>. 
            He obtained his 
            Master degree in Control Theory and Control Engineering, supervised by <a href="https://yanzhao.scut.edu.cn/open/ExpertInfo.aspx?zjbh=msHo3DkUNNTex1RJnGJMVA==" target="_blank">Prof. Wu Yuxiang</a>,
            as well as
            Bachelor degree in Electronic Science & Technology, supervised by <a href="https://scholar.google.com/citations?user=cTG8Na8AAAAJ&hl=en&oi=ao" target="_blank">Prof. Wen Shangsheng</a>, 
            both from the South China University of Technology.
            Previously, he worked as a research assistant at the Chinese Academy of Sciences (<a href="http://mmlab.siat.ac.cn/" target="_blank">MMLab@SIAT</a>), collaborating with <a href="https://scholar.google.com/citations?hl=en&user=OSDCB0UAAAAJ" target="_blank">Prof. Dong Chao</a> and <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en" target="_blank">Prof. Qiao Yu</a>, focusing on deep learning and computer vision. 
            He also had a short but highly rewarding experience working with <a href="https://scholar.google.com/citations?user=0gxC-bcAAAAJ&hl=en&oi=ao" target="_blank">Prof. Lian-Kuan Chen </a> at the Chinese University of Hong Kong.
            In addition, he has served as consultant or intern for several reputable companies, such as Samsung Electronics, Huawei Technologies, TCL, etc.
            Moreover, he has published over 60 research articles in prestigious international journals and conferences, e.g. Advanced Intelligent Systems, IEEE-TIV, IEEE-TASE, IEEE/OSA-JLT, IEEE-TIM, IEEE-RAL, IEEE-SJ, ICRA, IROS, OFC, CLEO, etc.
            He also holds more than 50 authorized patents.
            His research interests primarily focus on SLAM, event-based vision, visible light positioning, etc.
        </p>
        </td>
      </tr>
    </table>
  
    <hr/>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tr><td><sectionheading>&nbsp;&nbsp;Selected Publications</sectionheading></td></tr>
  </table>

  
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  
<!-- ‰∏ãÈù¢ÊèíÂÖ•ÂÜÖÂÆπËÆ∫ÊñáÁöÑÂÜÖÂÆπ -->

<!-- DEIO: Deep Event Inertial Odometry -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
    <video playsinline autoplay loop muted src="https://kwanwaipang.github.io/DEIO/static/video/HKU_agg_small_flip.mp4" poster="./File/Representative_works/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>

  <td width="60%" valign="top">
      <p><a href=" " target="_blank">
      <heading>DEIO: Deep Event Inertial Odometry</heading></a><br>
      <strong>First Author</strong><br>
      </p>

      <div class="paper">
      <a href="https://kwanwaipang.github.io/DEIO/" target="_blank">Website</a> |
      <a href="https://github.com/arclab-hku/DEIO" target="_blank">Code</a> |
      <a href="https://arxiv.org/pdf/2411.03928" target="_blank">Paper Link</a> 
      </div>
  </td>
</tr>

<!-- LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian Splatting -->
<tr>
  <td width="40%" valign="top" align="center">
  <video playsinline autoplay loop muted src="https://kwanwaipang.github.io/LVI-GS/static/video/hku_logo.mp4" poster="./File/Representative_works/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
</td>

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/LVI-GSÔºöTightly Coupled LiDAR‚ÄìVisual‚ÄìInertial SLAM Using 3-D Gaussian Splatting.pdf" target="_blank">
      <heading>LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian Splatting</heading></a><br>
      <strong>Co-First Author</strong><br>
      2025 IEEE Transactions on Instrumentation and Measurement, JCR: Q1, IF: 5.6<br>
      </p>

      <div class="paper">
      <a href="https://kwanwaipang.github.io/LVI-GS/" target="_blank">Website</a> |
      <a href="https://ieeexplore.ieee.org/document/10926911" target="_blank">Paper Link</a>
      </div>
  </td>
</tr>

<!-- ! EVI-SAM: Robust, Real-time, Tightly-coupled Event-Visual-Inertial State Estimation and 3D Dense Mapping -->
<tr>
    <td width="40%" valign="top" align="center">
    <video playsinline autoplay loop muted src="https://kwanwaipang.github.io/SLAM_DEMO/evi_sam.mp4" poster="./File/Representative_works/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </td>

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/EVI-SAMÔºöRobust, Real-time, Tightly-coupled Event-Visual-Inertial State Estimation and 3D Dense Mapping.pdf" target="_blank">
      <heading>EVI-SAM: Robust, Real-time, Tightly-coupled Event-Visual-Inertial State Estimation and 3D Dense Mapping</heading></a><br>
      <strong>First Author</strong><br>
      2024, Advanced Intelligent Systems, JCR: Q1, IF: 7.4;<br>
      </p>

      <div class="paper">
      <a href="https://kwanwaipang.github.io/EVI-SAM/" target="_blank">Website</a> |
      <a href="https://www.bilibili.com/video/BV1za4y1d76c/?spm_id_from=333.999.0.0" target="_blank">Video</a> |
      <a href="https://github.com/arclab-hku/Event_based_VO-VIO-SLAM/blob/main/EVI-SAM/data_srcipt.md" target="_blank">Mapping DataSet</a> |
      <a href="https://onlinelibrary.wiley.com/doi/10.1002/aisy.202400243" target="_blank">Paper Link</a>
      </div>
  </td>
</tr>

<!-- ! PL-EVIOÔºöRobust Monocular Event-based Visual Inertial Odometry with Point and Line Features -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
    <video playsinline autoplay loop muted src="https://kwanwaipang.github.io/SLAM_DEMO/PL-EVIO.mp4" poster="./File/Representative_works/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/PL-EVIOÔºöRobust Monocular Event-based Visual Inertial Odometry with Point and Line Features.pdf" target="_blank">
      <heading>PL-EVIOÔºöRobust Monocular Event-based Visual Inertial Odometry with Point and Line Features</heading></a><br>
      <strong>First Author</strong><br>
      2023, IEEE Transactions on Automation Science and Engineering, JCR: Q1, IF: 5.6<br>
      </p>

      <div class="paper">
      <a href="./File/Blogs/Poster/ICRA2024_presentation.html" target="_blank">ICRA2024 Presentation</a>
      <br>
      <a href="https://kwanwaipang.github.io/PL-EVIO/" target="_blank">Website</a> |
      <a href="https://www.bilibili.com/video/BV12t4y1L7eK/?spm_id_from=333.999.0.0" target="_blank">Video</a> |
      <a href="https://ieeexplore.ieee.org/abstract/document/10287884" target="_blank">Paper Link</a>
      </div>
  </td>
</tr>

<!-- ! ECMD: An Event-Centric Multisensory Driving Dataset for SLAM -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
  <video poster="https://github.com/arclab-hku/Event_based_VO-VIO-SLAM/raw/main/ECMD/sensor_video.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/ECMDÔºöAn Event-Centric Multisensory Driving Dataset for SLAM.pdf" target="_blank">
      <heading>ECMD: An Event-Centric Multisensory Driving Dataset for SLAM</heading></a><br>
      <strong>Co-First Author</strong><br>
      2023, IEEE Transactions on Intelligent Vehicles, JCR: Q1, IF: 8.2<br>
      </p>

      <div class="paper">
      <a href="https://arclab-hku.github.io/ecmd/" target="_blank">Website</a> |
      <a href="https://www.bilibili.com/video/BV1au4y1V72x/?spm_id_from=333.999.0.0" target="_blank">Video</a> |
      <a href="https://ieeexplore.ieee.org/abstract/document/10342726" target="_blank">Paper Link</a> 
      </div>
  </td>
</tr>

<!-- ! ESVIOÔºöEvent-based Stereo Visual Inertial Odometry -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
  <video playsinline autoplay loop muted src="https://kwanwaipang.github.io/SLAM_DEMO/ESVIO.mp4" poster="./File/Representative_works/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/ESVIOÔºöEvent-based Stereo Visual Inertial Odometry.pdf" target="_blank">
      <heading>ESVIOÔºöEvent-based Stereo Visual Inertial Odometry</heading></a><br>
      <strong>Co-First Author</strong><br>
      2023, IEEE Robotics and Automation Letters, JCR: Q2, IF: 5.2<br>
      </p>

      <div class="paper">
      <a href="./File/Blogs/Poster/IROS2023_presentation.html" target="_blank">IROS2023 Presentation</a> 
      <br>
      <a href="https://kwanwaipang.github.io/ESVIO/" target="_blank">Website</a> |
      <a href="https://github.com/arclab-hku/ESVIO" target="_blank">Code</a> |
      <a href="https://www.bilibili.com/video/BV1ve4y1M7v4/?spm_id_from=333.999.0.0" target="_blank">Video</a> |
      <a href="https://ieeexplore.ieee.org/abstract/document/10107754" target="_blank">Paper Link</a>
      </div>
  </td>
</tr>

<!-- ! Monocular Event Visual Inertial Odometry based on Event-corner using Sliding Windows Graph-based Optimization -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
  <video poster="https://github.com/arclab-hku/Event_based_VO-VIO-SLAM/raw/main/IROS2022/cover.jpg" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/Monocular Event Visual Inertial Odometry Based on Event-Corner Using Sliding Windows Graph-Based Optimization.pdf" target="_blank">
      <heading>Monocular Event Visual Inertial Odometry based on Event-corner using Sliding Windows Graph-based Optimization</heading></a><br>
      <strong>First Author</strong><br>
      2022, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)<br>
      </p>

      <div class="paper">
      <a href="https://kwanwaipang.github.io/Mono-EIO/" target="_blank">Website</a> |
      <a href="./File/Blogs/Poster/IROS2022_presentation.html" target="_blank">IROS2022 Presentation</a> |
      <a href="https://ieeexplore.ieee.org/abstract/document/9981970" target="_blank">Paper Link</a>
      </div>
  </td>
</tr>

<!-- Robot Localization and Navigation Using Visible Light Positioning and SLAM Fusion -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
  <div style="position: relative; width: 100%; padding-bottom: 56.25%;">
    <!-- Âä†ËΩΩ‰∏≠ÁöÑÂä®Âõæ -->
    <img id="loadingImage2" src="./File/Representative_works/loading-icon.gif" alt="Loading..." 
         style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; border-radius: 15px;">
    
    <!-- iframe Áî®‰∫éÂµåÂÖ• Bilibili ËßÜÈ¢ë -->
    <iframe id="videoIframe2" 
            src="//player.bilibili.com/player.html?isOutside=true&aid=714632834&bvid=BV1JX4y137Q7&cid=311016735&p=1" 
            title="Bilibili video player" 
            frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
            allowfullscreen 
            style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border-radius: 15px; display: none;">
    </iframe>
</div>
  </a></td>  

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/Robot Localization and Navigation Using Visible Light Positioning and SLAM Fusion.pdf" target="_blank">
      <heading>Robot Localization and Navigation Using Visible Light Positioning and SLAM Fusion</heading></a><br>
      <strong>First Author</strong><br>
      2021 IEEE/OSA Journal of Lightwave Technology, JCR: Q1, IF: 4.7<br>
      </p>

      <div class="paper">
      <a href="https://cmte.ieee.org/photonics-asia/2022/04/22/ieee-photonics-journal-club-april-6th-2022/" target="_blank">IEEE Photonics Journal Club session</a>
      <br>
      <a href="https://www.bilibili.com/video/BV1Nv4y137Xn/?spm_id_from=333.999.0.0" target="_blank">CLEO2022 Presentation</a> |
      <a href="https://www.bilibili.com/video/BV1JX4y137Q7/?spm_id_from=333.999.0.0&vd_source=a88e426798937812a8ffc1a9be5a3cb7" target="_blank">Video</a> |
      <!-- <a href="https://www.bilibili.com/video/BV15t4y1t7yS/?t=4.941058&spm_id_from=333.1350.jump_directly&vd_source=a88e426798937812a8ffc1a9be5a3cb7" target="_blank">IEEE Photonics Journal Club session</a> | -->
      <a href="https://ieeexplore.ieee.org/abstract/document/9541016" target="_blank">Paper Link</a> 
      </div>
  </td>
</tr>

<!-- Robust Robotic Localization Using Visible Light Positioning and Inertial Fusion -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
    <video poster="https://kwanwaipang.github.io/VLC_Demo/ÂæÆ‰ø°Êà™Âõæ_20240808123230.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>  

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/Robust Robotic Localization Using Visible Light Positioning and Inertial Fusion.pdf" target="_blank">
      <heading>Robust Robotic Localization Using Visible Light Positioning and Inertial Fusion</heading></a><br>
      <strong>First Author</strong><br>
      2021 IEEE Sensors Journal, JCR: Q1, IF: 4.3<br>
      </p>

      <div class="paper">
      <a href="https://kwanwaipang.github.io/VLC_Demo/ESI Hightly Cited Paper 1.png" target="_blank">ESI Hightly Cited Paper</a> |
      <a href="https://www.bilibili.com/video/BV1ZT4y1F75x/?spm_id_from=333.999.0.0" target="_blank">Video</a> |
      <a href="https://ieeexplore.ieee.org/abstract/document/9330552" target="_blank">Paper Link</a>
      </div>
  </td>
</tr>

<!-- High accuracy, 6-DoF simultaneous localization and calibration using visible light positioning -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
    <video playsinline autoplay loop muted src="https://kwanwaipang.github.io/VLC_Demo/Demo_video/double-led-vlp.mp4" poster="./File/Representative_works/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>  

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/High accuracy, 6-DoF simultaneous localization and calibration using visible light positioning.pdf" target="_blank">
      <heading>High accuracy, 6-DoF simultaneous localization and calibration using visible light positioning</heading></a><br>
      <strong>Corresponding Author</strong><br>
      2022 IEEE/OSA Journal of Lightwave Technology, JCR: Q1, IF: 4.7<br>
      </p>

      <div class="paper">
      <a href="https://www.bilibili.com/video/BV1wb4y1E7Kd/?spm_id_from=333.999.0.0" target="_blank">Video</a> |
      <a href="https://ieeexplore.ieee.org/document/9857575" target="_blank">Paper Link</a>
      </div>
  </td>
</tr>

<!-- CGA-VLP: High Accuracy Visible Light Positioning Algorithm Using Single Square LED with Geomagnetic Angle Correction -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
    <video playsinline autoplay loop muted src="https://kwanwaipang.github.io/VLC_Demo/Demo_video/ÂçïÁÅØVLP+PDR.mp4" poster="./File/Representative_works/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>  

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/CGA-VLPÔºöHigh Accuracy Visible Light Positioning Algorithm Using Single Square LED with Geomagnetic Angle Correction.pdf" target="_blank">
      <heading>CGA-VLP: High Accuracy Visible Light Positioning Algorithm Using Single Square LED with Geomagnetic Angle Correction</heading></a>
      <br>
      2022 Photonics, JCR: Q2, IF: 2.1<br>
      </p>

      <div class="paper">
      <a href="https://www.bilibili.com/video/BV1NP411G7Ae/?spm_id_from=333.999.0.0" target="_blank">Video</a> |
      <a href="https://www.mdpi.com/2304-6732/9/9/653" target="_blank">Paper Link</a>
      </div>
  </td>
</tr>


<!-- VWR-SLAM Tightly-coupled SLAM System Based on Visible Light Positioning Landmark, Wheel Odometer and RGB-D Camera -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
  <video poster="https://kwanwaipang.github.io/VLC_Demo/VWR-SLAM.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>  

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/VWR-SLAM Tightly-coupled SLAM System Based on Visible Light Positioning Landmark, Wheel Odometer and RGB-D Camera.pdf" target="_blank">
      <heading>VWR-SLAM Tightly-coupled SLAM System Based on Visible Light Positioning Landmark, Wheel Odometer and RGB-D Camera</heading></a><br>
      <strong>Corresponding Author</strong><br>
      2022 IEEE Transactions on Instrumentation and Measurement, JCR: Q1, IF: 5.6<br>
      </p>

      <div class="paper">
      <a href="https://www.bilibili.com/video/BV1dT4y1i7hD/?spm_id_from=333.999.0.0" target="_blank">Video</a> |
      <a href="https://ieeexplore.ieee.org/document/10003174" target="_blank">Paper Link</a>
      </div>
  </td>
</tr>

<!-- Multirobot Cooperative Localization Based on Visible Light Positioning and Odometer -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
    <video playsinline autoplay loop muted src="https://kwanwaipang.github.io/VLC_Demo/Demo_video/multi_robot_vlp.mp4" poster="./File/Representative_works/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>  

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/Multirobot Cooperative Localization Based on Visible Light Positioning and Odometer.pdf" target="_blank">
      <heading>Multirobot Cooperative Localization Based on Visible Light Positioning and Odometer</heading></a><br>
      <strong>Co-First and Corresponding Author</strong><br>
      2021 IEEE Transactions on Instrumentation and Measurement, JCR: Q1, IF: 5.6<br>
      </p>

      <div class="paper">
      <a href="https://www.bilibili.com/video/BV1b54y1j7w4/?spm_id_from=333.999.0.0" target="_blank">Video</a> |
      <a href="https://ieeexplore.ieee.org/document/9447720" target="_blank">Paper Link</a>
      </div>
  </td>
</tr>

  <!-- High-Accuracy Robot Indoor Localization Scheme Based on Robot Operating System Using Visible Light Positioning -->
  <tr>
    <td width="40%" valign="top" align="center"><a href=" ">
    <video playsinline autoplay loop muted src="https://kwanwaipang.github.io/VLC_Demo/Demo_video/vlp_ros.mp4" poster="./File/Representative_works/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>  

    <td width="60%" valign="top">
        <p><a href="https://kwanwaipang.github.io/Poster_files/papers/High-Accuracy Robot Indoor Localization Scheme Based on Robot Operating System Using Visible Light Positioning.pdf" target="_blank">
        <heading>High-Accuracy Robot Indoor Localization Scheme Based on Robot Operating System Using Visible Light Positioning</heading></a><br>
        <strong>First Author</strong><br>
        2020 IEEE Photonics Journal, JCR: Q2, IF: 2.4<br>
        </p>

        <div class="paper">
        <a href="https://ieeephotonics.org/announcements/ieee-photonics-journal-top-15-downloads-of-2024/" target="_blank">IEEE Photonics Journal: Top 15 Downloads of 2024</a> 
        <br>
        <a href="https://player.youku.com/embed/XNjM5NTk0NDA5Mg==" target="_blank">OFC2021 Presentation</a> |
        <a href="https://www.bilibili.com/video/BV1jE411P7xx/?spm_id_from=333.999.0.0" target="_blank">Video</a> |
        <a href="https://ieeexplore.ieee.org/abstract/document/9040589" target="_blank">Paper Link</a>
        </div>
    </td>
</tr>

<!-- Performance enhancement scheme for RSE-based underwater optical camera communication using de-bubble algorithm and binary fringe correction -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
    <video poster="https://kwanwaipang.github.io/VLC_Demo/UOCC.jpg" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>  

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/Performance enhancement scheme for RSE-based underwater optical camera communication using de-bubble algorithm and binary fringe correction.pdf" target="_blank">
      <heading>Performance enhancement scheme for RSE-based underwater optical camera communication using de-bubble algorithm and binary fringe correction</heading></a><br>
      <strong>Corresponding Author</strong><br>
      2021 Electronics, JCR: Q2, IF: 2.6<br>
      </p>

      <div class="paper">
      <a href="https://www.bilibili.com/video/BV1m44y167yd/?spm_id_from=333.999.0.0" target="_blank">OFC2021 Presentation</a> |
      <a href="https://www.mdpi.com/2079-9292/10/8/950" target="_blank">Paper Link</a> 
      </div>
  </td>
</tr>

<!-- The LED-ID Detection and Recognition Method Based on Visible Light Positioning Using Proximity Method -->
<tr>
  <td width="40%" valign="top" align="center"><a href=" ">
    <video playsinline autoplay loop muted src="https://kwanwaipang.github.io/VLC_Demo/Demo_video/‰∏¥ËøëÊ≥ïVLPÂÆö‰Ωç.mp4" poster="./File/Representative_works/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
  </a></td>  

  <td width="60%" valign="top">
      <p><a href="https://kwanwaipang.github.io/Poster_files/papers/The LED-ID Detection and Recognition Method Based on Visible Light Positioning Using Proximity Method.pdf" target="_blank">
      <heading>The LED-ID Detection and Recognition Method Based on Visible Light Positioning Using Proximity Method</heading></a><br>
      <strong>Corresponding Author</strong><br>
      2018 IEEE Photonics Journal, JCR: Q2, IF: 2.4<br>
      </p>

      <div class="paper">
      <a href="https://kwanwaipang.github.io/VLC_Demo/ESI Hightly Cited Paper 2.png" target="_blank">ESI Hightly Cited Paper</a> |
      <a href="https://ieeexplore.ieee.org/document/8302893" target="_blank">Paper Link</a> 
      </div>
  </td>
</tr>


  </table>
  <hr/>

  <!-- ‰∏éÂ≠¶ÊúØÁõ∏ÂÖ≥ÁöÑ‰∏Ä‰∫õËµÑËÆØ -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tr>
      <td>
        <sectionheading>&nbsp;&nbsp;Academic Service</sectionheading>
      </td>
    </tr>
    <tr>
      <td style="padding-left: 40px"> <!-- Êï¥‰ΩìÂ∑¶‰æßÁïôÁôΩ -->
        <heading>Journal Reviewer:</heading>
        <ul style="margin: 0; padding-left: 20px"> <!-- ‰∫åÁ∫ßÁº©Ëøõ -->
          <li>IEEE Transactions on Robotics (T-RO)</li>
          <li>IEEE Transactions on Wireless Communications (TWC)</li>
          <li>Transactions on Intelligent Transportation Systems (T-ITS)</li>
          <li>IEEE Transactions on Multimedia</li>
          <li>IEEE Transactions on Industrial Informatics (TII)</li>
          <li>IEEE Transactions on Intelligent Vehicles (TIV)</li>
          <li>IEEE Transactions on Automation Science and Engineering (TASE)</li>  
          <li>IEEE Transactions on Cybernetics</li>
          <li>IEEE Transactions on Circuits and Systems II: Express Briefs</li>                            
          <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
          <li>IEEE Transactions on Biometrics, Behavior, and Identity Science</li>
          <li>IEEE Transactions on Computational Imaging</li>
          <li>IEEE Transactions on Instrumentation and Measurement (TIM)</li>
          <li>IEEE Internet of Things Journal</li>
          <li>Journal of Lightwave Technology (JLT)</li>
          <li>IEEE Robotics and Automation Letters (RAL)</li>
          
        </ul>
      </td>
    </tr>
    <tr>
      <td style="padding-left: 40px"> <!-- Êï¥‰ΩìÂ∑¶‰æßÁïôÁôΩ -->
        <heading>Conference Reviewer:</heading>
        <ul style="margin: 0; padding-left: 20px"> <!-- ‰∫åÁ∫ßÁº©Ëøõ -->
          <li>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</li>
          <li>IEEE International Conference on Robotics and Automation (ICRA)</li>
        </ul>
      </td>
    </tr>
  </table>
  <hr/>
  
  <footer class="nofixed-bottom">    
    <div class="map-container">
        <body>
        <a href='https://clustrmaps.com/site/1bupy'  title='Visit tracker'><img class="first-img" src='//clustrmaps.com/map_v2.png?cl=ffffff&w=880&t=tt&d=pagJUnWnL_Z1rl0JVfBnYnM2UHbrfs-1Y3zuFwZJfCs&co=2d78ad&ct=ffffff'/></a>
        </body>
    </div>
  </footer> 


  </td></tr>
</table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('material_review_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ieee_iot_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acm_turc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('aog_mcts_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pragmatics_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('collab_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('rma_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('energyloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('navloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('maniploco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('parkour_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('AeCoM_abs');
</script>

<!-- Âà†ÊéâPDFÁöÑÂ∑•ÂÖ∑Ê†è -->
<script>
  // Ëé∑ÂèñÊâÄÊúâÁöÑiframeÂÖÉÁ¥†
  var iframes = document.querySelectorAll('.paper-pdf');

  // ÈÅçÂéÜÊâÄÊúâÁöÑiframeÂÖÉÁ¥†
  iframes.forEach(function(iframe) {
      // Ëé∑ÂèñiframeÁöÑsrcÂ±ûÊÄß
      var src = iframe.getAttribute('src');

      // Â¶ÇÊûúsrcÂ±ûÊÄßÂ≠òÂú®
      if (src) {
          // ÁªôsrcÊ∑ªÂä†ÂèÇÊï∞
          src += '#toolbar=0&navpanes=0&scrollbar=0';

          // Êõ¥Êñ∞iframeÁöÑsrcÂ±ûÊÄß
          iframe.setAttribute('src', src);
      }
  });
</script>

<script>
  // ÂΩì iframe Âä†ËΩΩÂÆåÊàêÂêéÊâßË°å
  document.getElementById('videoIframe1').onload = function() {
      // ÈöêËóèÂä†ËΩΩ‰∏≠ÁöÑÂä®Âõæ
      document.getElementById('loadingImage1').style.display = 'none';
      // ÊòæÁ§∫ iframe
      document.getElementById('videoIframe1').style.display = 'block';
  };
</script>

<script>
  // ÂΩì iframe Âä†ËΩΩÂÆåÊàêÂêéÊâßË°å
  document.getElementById('videoIframe2').onload = function() {
      // ÈöêËóèÂä†ËΩΩ‰∏≠ÁöÑÂä®Âõæ
      document.getElementById('loadingImage2').style.display = 'none';
      // ÊòæÁ§∫ iframe
      document.getElementById('videoIframe2').style.display = 'block';
  };
</script>

<!-- ‰∏ãÈù¢‰∏∫ÁΩëÈ°µÁöÑÁï•Áº©Âõæ -->
<link rel="shortcut icon" href="https://kwanwaipang.github.io/Poster_files/hku_logo.jpg"/>


</body>

</html>