<!DOCTYPE html>
<html lang="en">

<style>
    p { /* ÊñáÂ≠óÂØπÈΩê */
        text-align: justify;
        text-align-last: left;
        text-justify: inter-word;
    }
    </style>

<!-- Head -->
<head>    <!-- Metadata, OpenGraph and Schema.org -->


  <!-- Standard metadata -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Kwan Wai-Pang's Blog</title>
  <meta name="author" content="Kwan Wai-Pang " />
  <meta name="description" content="Personal Blog of Kwan Wai-Pang" />
  <meta name="keywords" content="Event-based Vision, SLAM, Robotics" />

  <!-- OpenGraph -->
  <meta property="og:site_name" content="My Technology Blog" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="My Blog  | Home" />
  <meta property="og:description" content="My Blog.
" />

  <meta property="og:locale" content="en" />

  <!-- Twitter card -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Home" />
  <meta name="twitter:description" content="Personal Blog of Kwan Wai-Pang" />

  <!-- Bootstrap & MDB -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Barriecito&family=Poppins:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700">

  <!-- Code Syntax Highlighting -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/PASTIE.css" media="none" id="highlight_theme_light" />

  <!-- Styles -->

  <link rel="stylesheet" href="https://kwanwaipang.github.io/File/Blogs/assets/css/main.css">
  <link rel="stylesheet" href="https://kwanwaipang.github.io/File/Blogs/assets/css/fonts.css">
  <link rel="stylesheet" href="/assets/css/fonts.css">

  <!-- Dark Mode -->


</head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">
                <li class="nav-item ">
                  <a class="nav-link" href="../My_Blog.html">Homepage</a>
                </li>

            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- home.html -->
      <div class="post">
        <header class="post-header">
          <h1 align="center" class="post-title">
           <span style="font-weight: 600;">Paper Survey: Learning-based VO</span>
          </h1>
          <p><br></p>
        </header>

<article>
<!-- ÊèíÂÖ•ÂçöÂÆ¢ÂÜÖÂÆπ-->
<h1>Abstract</h1>
<p>
This blog is about the paper survey and analysis for learning-based VO, VIO and SLAM.
This blog is based on the paper reading and my personal understanding, which is only for self-record rather than any commercial purposes.

<br>
As shown in following figure, a classic VO pipeline, which typically consists of camera calibration, feature detection, feature matching (or tracking), outlier rejection (e.g., RANSAC), motion estimation, scale estimation and local optimization (Bundle Adjustment), has been developed and broadly considered as a golden rule to follow. 
However, the traditional VO pipeline is not robust enough to handle the challenging scenarios, such as dynamic environments, illumination changes, textureless scenes, etc.
<figure style="text-align: center;">
    <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂÖ∏ÂûãÁöÑVOÊ°ÜÊû∂.png" alt="Image description">
    <figcaption>
    The framework of the traditional VO system.
    </figcaption>
</figure>
</p>

<p><br></p>
<h3><a href="https://arxiv.org/pdf/1709.08429" target="_blank">Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks</a></h3>
<p>
  ICRA2017
<br>
Motivations:
<ul>
  <li> Most of existing VO algorithms need to be carefully designed and specifically fine-tuned to work well in different environments. </li>
  <li> Some prior knowledge is also required to recover an <font color="#FF0000">absolute scale</font>  for monocular VO. </li>
  <li> <p>
    Most of the deep learning architectures are designed to tackle recognition and classification problems (extract high-level appearance information).
    While learning the appearance representation limited the VO to function only in trained environments and seriously hinders the generalization ability.
  </p></li>
  <li><p>
    <font color="#FF0000">VO heavily rely on geometric features rather than the appearance ones.</font>
    Because a trained CNN model serves as an appearance ‚Äúmap‚Äù of the scene, it needs to be re-trained or at least fine-tuned for a new environment.
    This seriously hampers the technique for widespread usage, which is also one of the biggest difficulties when applying deep learning for VO.
    Therefore, some works need to estimate the optical flow or depth map to get the geometric features.
    This leads to a two-stage pipeline and also requires the pre-processed optical flow or depth map as input, which is not end-to-end.
  </p></li>
  <li> <p>
  VO algorithm ideally should model motion dynamics by examining the changes and connections on a <strong>sequence of images</strong> rather than processing a single image. This means we need sequential learning, which the CNNs are inadequate to.
  But Recurrent Convolution Neural Networks (RCNNs) can be used to model the <font color="#FF0000">sequential dynamics and relations</font>.
</p></li>
</ul>

Contributions:
<ul>
  <li> <font color="#FF0000">The first</font> end-to-end framework using deep Neural Networks. </li>
  <li> infers poses directly from a sequence of raw RGB images (or videos) </li>
  <li> Key-point: be generalized to new environments by using the geometric feature representation learnt by CNN </li>
  <li>  <p>
    Based on the RCNNs, it not only automatically learns <font color="#FF0000">effective feature representation</font> for the VO problem through CNN, but also implicitly <font color="#FF0000">models sequential dynamics and relations </font> using deep Recurrent Neural Networks.
    </p> </li>  
</ul>

<figure style="text-align: center;">
<div style="margin-bottom: 10px;">
  <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240704191626.png" alt="Image description">
</div>
  <figcaption> <p>
    The framework of the proposed method. 
    RCNN takes a sequence of RGB images (video) as input and learns features by CNN for RNN based sequential modelling to estimate poses.
  </p>
  <a href="https://epiception.github.io/assets/Projects/deep_event_vo.pdf" target="_blank">DeepEvent-VO</a> use the same framework.
</figcaption>
</figure>

Methodology:
<ul>
  <li><p> It is composed of CNN for geometric feature extraction and RNN for sequential learning (modelling the dynamics and relations among a sequence of CNN features). </p></li>
</ul>
<details>
ü§î <a href="https://blog.csdn.net/gwplovekimi/article/details/83474593" target="_blank">what is RNN?</a>
</details>
<figure style="text-align: center;">
  <div style="margin-bottom: 10px;">
    <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240704191824.png" alt="Image description">
  </div>
    <figcaption> 
      Architecture of the RCNN based monocular VO system
  </figcaption>
  </figure>

  <ul>
    <li> <p>It seems that the <mark>geometric feature extraction</mark> is just a manual explanation, since the network doesn't have any supervise label for this <mark>geometric feature</mark>, how to ensure the CNN network learn the <mark>geometric feature</mark>.</p></li>
    <li> <p>Cost Function</p></li>
  </ul>
  <p>
    The input is the RGB image while the target is the pose of the camera.
    The pose is represented by the 6-DOF transformation matrix. 
    The cost function is the Mean Square Error (MSE) between the predicted pose (positions and orientations) and the ground truth pose.
  </p>

  <figure style="text-align: center;">
    <div style="margin-bottom: 10px;">
      <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240704213130.png" alt="Image description">
    </div>
      <figcaption> 
        Note: using quaternion degrades the orientation estimate, therefore Euler angles should be better.
    </figcaption>
  </figure>

Experiments:
<ul>
  <li> only done on the KITTI dataset. </li>
  <li> worse than the traditional methods. </li>
</ul>

<!-- Marks:
<ul>
  <li> <a href="https://blog.csdn.net/gwplovekimi/article/details/83474593" target="_blank">what is RNN</a></li>
  <li> <a href="https://blog.csdn.net/gwplovekimi/article/details/84647354" target="_blank">what is Normalization</a></li>
  <li> <a href="https://blog.csdn.net/gwplovekimi/article/details/84539021" target="_blank">different kinds of Normalization</a></li>
</ul> -->

</p>

<p><br></p>
<h3><a href="https://arxiv.org/pdf/1709.06841" target="_blank">UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning</a></h3>
ICRA2018
<br>
<!-- Motivations:
<ul>
  <li><p>

  </p></li>  
</ul> -->

Contributions:
<ul>
  <li>
    estimate the 6-DoF pose of a monocular camera and the depth of its view (both tracking and mapping).
  </li>  
  <li>
    unsupervised scheme: train with unlabeled datasets and can be applied to localization scenarios (through the spatial and temporal geometric constraint). 
  </li> 
  <li>
    absolute scale recovery for both pose and dense depth: train UnDeepVO by using stereo image pairs to recover the scale but test it by using consecutive monocular images.
  </li> 
</ul>

Some key-points:
<ul>
  <li>
    <font color="#FF0000">stereo image training without the need of labeled datasets</font>.
  </li>  
  <li><p>
    Since rotation (represented by Euler angles) has high nonlinearity, it is usually difficult to train compared with translation.
    For supervised training, a popular solution is to give a bigger weight to the rotational loss as a way of normalization.
  </p></li>
</ul>

Methodology:
<ul>
  <li><p>
    The pose estimator is a CNN architecture.
    <br>
    In order to better train the rotation with unsupervised learning, the authors decouple the translation and the rotation with two separate groups of fully-connected layers after the last convolution layer. 
    This enables the network to introduce a weight normalizing the rotation and the translation predictions for better performance.
  </p></li>  
  <li><p>
    The depth estimator is an encoder-decoder architecture.
    <br>
    Instead of produce the disparity images, it directly predicts the depth maps.
  </p></li>  
</ul>

<figure style="text-align: center;">
  <div style="margin-bottom: 10px;">
    <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240704215256.png" alt="Image description">
  </div>
    <figcaption> <p>
      After training with unlabeled stereo images, UnDeepVO can simultaneously perform visual odometry and depth estimation with monocular images. The estimated 6-DoF poses and depth maps are both scaled without the need for scale postprocessing.
    </p></figcaption>
</figure>

Loss are built on geometric constraints.
  <p>
  Its total loss includes spatial image losses and temporal image losses.
  <font color="#FF0000">The spatial image losses drive the network to recover scaled depth maps by using stereo image pairs, 
  while the temporal image losses are designed to minimize the errors on camera motion by using two consecutive monocular images.
  </font>
  <br>
  As shown in following Figure, utilizing both spatial and temporal geometric consistencies of a stereo image sequence to formulate the loss function. 
  The red points in one image all have the corresponding ones in another. 
  <br>
  <mark>Spatial geometric consistency</mark> represents the geometric projective constraint between the corresponding points in left-right image pairs, while <mark>temporal geometric consistency</mark> represents the geometric projective constraint between the corresponding points in two consecutive monocular images (ü§îsimilar to the re-projection or BA?)
  <br>
</p>

<figure style="text-align: center;">
  <div style="margin-bottom: 10px;">
    <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240704222218.png" alt="Image description">
  </div>
    <figcaption> <p>
      Training scheme of UnDeepVO. The pose and depth
estimators take stereo images as inputs to estimate 6-DoF
poses and depth maps, respectively. The total loss including
spatial losses and temporal losses can then be calculated
based on raw RGB images, estimated depth maps and poses.
    </p></figcaption>
</figure>

<p>
<mark>Spatial Image Losses</mark>, including the left-right photometric consistency loss, disparity consistency loss and pose consistency loss.
</p>
(i) Photometric Consistency Loss:
<ul>
<li>Assume p<SUB>L</SUB>(u<SUB>L</SUB>, v<SUB>L</SUB>) is a pixel in the left image and p<SUB>R</SUB>(u<SUB>R</SUB>, v<SUB>R</SUB>) is its corresponding pixel in the right image. 
Then, we have the spatial constraint u<SUB>L</SUB> = u<SUB>R</SUB> and v<SUB>l</SUB> = v<SUB>R</SUB> +D<SUB>P</SUB>.</li>
<li>using the following figure, we can calculate the horizontal distance between the stereo images using the depth value of the network</li>
<details>
üòÄ <a href="https://blog.csdn.net/qq_40918859/article/details/123984329" target="_blank">principle of stereo depth calculation</a>
</details>
<figure style="text-align: center;">
  <div style="margin-bottom: 10px;">
    <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240704224924.png" alt="Image description">
  </div>
    <figcaption>
     calculating the horizontal distance D<SUB>P</SUB> (or disparity) between the corresponding points in the left and right images.
    </figcaption>
</figure>
<li>using the previous horizontal distance map D<SUB>P</SUB>, we can synthesize one image from the other through ‚Äúspatial transformer‚Äù. And then the combination of an L1 norm and an SSIM term is used to construct the left-right photometric consistency loss: </li>
<figure style="text-align: center;">
  <div style="margin-bottom: 10px;">
    <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240704225536.png" alt="Image description">
  </div>
    <figcaption>
    </figcaption>
</figure>
</ul>

(ii) Disparity Consistency Loss:
<ul>
  <li><p>
    Through the horizontal distance map D<SUB>P</SUB>, we can calculate the disparity map.
  </p></li>  
  <figure style="text-align: center;">
    <div style="margin-bottom: 10px;">
      <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240705095420.png" alt="Image description">
    </div>
      <figcaption><p>
        The depth map is generated from the network.
        The horizontal distance map D<SUB>P</SUB> is calculated from the depth map.
        Through the horizontal distance map D<SUB>P</SUB>, we can calculate the disparity map.
        AKA. the disparity map can be obtained from the depth map of the network.
        While the disparity map of left and right images should be consistent.
        (I think the description in the original paper is not clear enough, this is my understanding)
      </p></figcaption>
  </figure>
</ul>

(iii) Pose Consistency Loss:
<ul>
  <li><p>
    both left and right images are taken from the same camera, the transformations between the left and right images should be basically identical.
  </p></li>  
  <figure style="text-align: center;">
    <div style="margin-bottom: 10px;">
      <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240705100606.png" alt="Image description">
    </div>
      <figcaption><p>
      </p></figcaption>
  </figure>
</ul>

<p>
  <mark>Temporal Image Losses</mark>, including  photometric consistency loss and 3D geometric registration loss.
</p>
(i) Photometric Consistency Loss:
<ul>
  <li><p>
    Align the k and k+1 frames based on the predicted relative pose, camera internal parameters, and predicted depth map.
    For two consecutive frames, the photometric should be very similar.
  </p></li>  
  <figure style="text-align: center;">
    <div style="margin-bottom: 10px;">
      <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240705101612.png" alt="Image description">
    </div>
      <figcaption><p>
      </p></figcaption>
  </figure>
</ul>

(ii) 3D Geometric Registration Loss:
<ul>
  <li><p>
    similar to the ICP, the point cloud of the k frame can be transformed to the k+1 frame through the estimated transform.
    Then minimize the loss between the transformed point cloud and the estimated point cloud of the k+1 frame in the network.
  </p></li>  
  <figure style="text-align: center;">
    <div style="margin-bottom: 10px;">
      <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240705102108.png" alt="Image description">
    </div>
      <figcaption><p>
      </p></figcaption>
  </figure>
</ul>
<p>
üôãIn my opinion, all the above-mentioned loss is very similar to the concept of the re-projection error in the traditional VO pipeline.
Or, in other words, it is very similar to the Bundle Adjustment.
The created loss should be minimized (ideally case is zero) to obtain the optimal pose and depth map.
</p>
Experiments:
<ul>
  <li><p>
    better than ORB-SLAM2 in KITTI dataset.
  </p></li>  
</ul>

<p><br></p>
<h3><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Recurrent_Neural_Network_for_Un-Supervised_Learning_of_Monocular_Video_Visual_CVPR_2019_paper.pdf" target="_blank">Recurrent neural network for (un-) supervised learning of monocular video visual odometry and depth</a></h3>
CVPR2019
<br>
Contributions:
<ul>
  <li><p>
    convolutional Long Short-Term Memory (ConvLSTM) units: to carry temporal information from previous views into the current frame‚Äôs depth and visual odometry estimation.
    Since it utilizes multiple views, the image reprojection constraint between <strong>multiple views</strong> can be incorporated into the loss (rather than only two consecutive frames), therefore it has performance improvement.
    <br>
    More details about the multi-view re-projectioin constraint can be found in the paper.
  </p></li>  
  <li><p>
    forward-backward flow-consistency constraint (idea is from the GeoNet): provides additional supervision to image areas and improves the robustness and generalization ability of the model.
    <br>
    It works as self-supervision and regularization.
    Through some strategies, we can get the dense flow field from k to k+1, while the dense flow field in k+1 can be estimated from the network. Therefore, minimize these two flow fields can provide additional supervision.
  </p></li>  
</ul>

ü§îThe GeoNetÔºö
<details>
<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper.pdf" target="_blank">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</a>
<br>
CVPR2018
<br>
Contributions:
<ul>
  <li><p>
    Jointly learning three components: depth, optical flow, and camera pose (VO), using divide-and-conquer strategy.
    <br>
    Most of the natural scenes are comprised of rigid static surfaces. 
    Their projected 2D image motion between video frames can be fully determined by the depth structure and camera motion. 
    Meanwhile, dynamic objects commonly exist in such scenes and usually possess the characteristics of large displacement and disarrangement
  </p></li>  
  <li>geometric consistency loss through forward-backward (or left-right) consistency check. </li>
  <li><p>Concept of dynamic VO: 
    The 2D projection image of a static background between image frames is entirely determined by depth structure and camera motion, and can be simulated using optical flow to capture camera motion. 
    While the motion of a dynamic target is determined by both camera motion and its own motion</p></li>
</ul>
<figure style="text-align: center;">
  <div style="margin-bottom: 10px;">
    <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240705111854.png" alt="Image description">
  </div>
    <figcaption><p>
    </p></figcaption>
</figure>
</details>
<br>
Methodology:
<figure style="text-align: center;">
  <div style="margin-bottom: 10px;">
    <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240705104043.png" alt="Image description">
  </div>
    <figcaption><p>
      Training scheme of the proposed method.
      <br>
      Note that: with depth as input target, the network can estimate the depth at absolute scale, otherwise, the depth is only at relative scale.
    </p></figcaption>
</figure>

<figure style="text-align: center;">
  <div style="margin-bottom: 10px;">
    <img style="width: 100%;" src="../assets/img/learning_based_VO/ÂæÆ‰ø°Êà™Âõæ_20240705104842.png" alt="Image description">
  </div>
    <figcaption><p>
      Network architecture of the proposed method.
      Very similar to the UnDeepVO, but with the ConvLSTM.
    </p></figcaption>
</figure>



<p><br></p>
<h3><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/7ac484b0f1a1719ad5be9aa8c8455fbb-Paper-Conference.pdf" target="_blank">Deep patch visual odometry</a></h3>
NIPS2024
<br>
<ul>
  <li><p>
    <a href="https://blog.csdn.net/gwplovekimi/article/details/139436796?spm=1001.2014.3001.5501" target="_blank">The evaluation and setup of the DPVO</a>
  </p></li>  
  <li><p>
    <a href="https://github.com/KwanWaiPang/DPVO_comment" target="_blank">Self Comment of DPVO</a>
  </p></li>
</ul>



üòäThere are some event-based works developed based on the DPVO:
<details>
  <li><a href="https://arxiv.org/pdf/2312.09800" target="_blank">Deep event visual odometry</a></li>
  <br>
  3DV2024
  <br>
  <a href="https://github.com/KwanWaiPang/DEVO_comment" target="_blank">Comment and evaluation of the DEVO</a>
  <p>
    Package the event data into voxel-based representation, and design a patch selector for the event data.
    While the other part is same as the DPVO.
  </p>
  <figure style="text-align: center;">
    <div style="margin-bottom: 10px;">
      <img style="width: 100%;" src="../assets/img/learning_based_VO/DEVO.png" alt="Image description">
    </div>
      <figcaption><p>
      </p></figcaption>
  </figure>

  <li><a href="https://rpg.ifi.uzh.ch/docs/Arxiv23_Pellerito.pdf" target="_blank">End-to-end Learned Visual Odometry with Events and Frames</a></li>
</details>










<!-- Motivations:
<ul>
  <li><p>

  </p></li>  
</ul> -->



<p><br></p>
<h1>Reference</h1>
<ul>
  <!-- <li></li> -->
  <li> <a href="https://blog.csdn.net/gwplovekimi/article/details/84647354" target="_blank">what is Normalization</a></li>
  <li> <a href="https://blog.csdn.net/gwplovekimi/article/details/84539021" target="_blank">different kinds of Normalization</a></li>
  <li> <a href="https://blog.csdn.net/qin_liang/article/details/132868855" target="_blank">Learned-Based VO Ê¢≥ÁêÜÔºàUSTC Paper ReadingÔºâ</a> </li>
  <li> <a href="https://blog.csdn.net/qin_liang/article/details/132708994" target="_blank">Ê∑±Â∫¶Â≠¶‰π†Áõ∏ÂÖ≥VOÊ¢≥ÁêÜ</a> </li>
</ul>


</article>
</div>
</div>

<!-- Footer -->    
<footer class="nofixed-bottom">
<div class="container mt-0" style="width:100%;text-align:center;">
  Please feel free to contact me through <a href="https://kwanwaipang.github.io/" target="_blank">my personal website</a>
</div>
</footer> 

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/ecmd/assets/js/masonry.js" type="text/javascript"></script>

  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/ecmd/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/ecmd/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

</body>
</html>

